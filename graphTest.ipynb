{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Recommenders contributors.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "\n",
    "class ImplicitCF(object):\n",
    "    \"\"\"Data processing class for GCN models which use implicit feedback.\n",
    "\n",
    "    Initialize train and test set, create normalized adjacency matrix and sample data for training epochs.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train,\n",
    "        test=None,\n",
    "        adj_dir=None,\n",
    "        col_user='userID',\n",
    "        col_item='itemID',\n",
    "        col_rating='rating',\n",
    "        col_prediction='prediction',\n",
    "        seed=None,\n",
    "    ):\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "            adj_dir (str): Directory to save / load adjacency matrices. If it is None, adjacency\n",
    "                matrices will be created and will not be saved.\n",
    "            train (pandas.DataFrame): Training data with at least columns (col_user, col_item, col_rating).\n",
    "            test (pandas.DataFrame): Test data with at least columns (col_user, col_item, col_rating).\n",
    "                test can be None, if so, we only process the training data.\n",
    "            col_user (str): User column name.\n",
    "            col_item (str): Item column name.\n",
    "            col_rating (str): Rating column name.\n",
    "            seed (int): Seed.\n",
    "\n",
    "        \"\"\"\n",
    "        self.user_idx = None\n",
    "        self.item_idx = None\n",
    "        self.adj_dir = adj_dir\n",
    "        self.col_user = col_user\n",
    "        self.col_item = col_item\n",
    "        self.col_rating = col_rating\n",
    "        self.col_prediction = col_prediction\n",
    "        self.train, self.test = self._data_processing(train, test)\n",
    "        self._init_train_data()\n",
    "\n",
    "        random.seed(seed)\n",
    "\n",
    "    def _data_processing(self, train, test):\n",
    "        \"\"\"Process the dataset to reindex userID and itemID and only keep records with ratings greater than 0.\n",
    "\n",
    "        Args:\n",
    "            train (pandas.DataFrame): Training data with at least columns (col_user, col_item, col_rating).\n",
    "            test (pandas.DataFrame): Test data with at least columns (col_user, col_item, col_rating).\n",
    "                test can be None, if so, we only process the training data.\n",
    "\n",
    "        Returns:\n",
    "            list: train and test pandas.DataFrame Dataset, which have been reindexed and filtered.\n",
    "\n",
    "        \"\"\"\n",
    "        df = (\n",
    "            train\n",
    "            if test is None\n",
    "            else pd.concat([train, test], axis=0, ignore_index=True)\n",
    "        )\n",
    "\n",
    "        if self.user_idx is None:\n",
    "            user_idx = df[[self.col_user]].drop_duplicates().reindex()\n",
    "            user_idx[self.col_user + \"_idx\"] = np.arange(len(user_idx))\n",
    "            self.n_users = len(user_idx)\n",
    "            self.n_users_in_train = train[self.col_user].nunique()\n",
    "            self.user_idx = user_idx\n",
    "\n",
    "            self.user2id = dict(\n",
    "                zip(user_idx[self.col_user], user_idx[self.col_user + \"_idx\"])\n",
    "            )\n",
    "            self.id2user = dict(\n",
    "                zip(user_idx[self.col_user + \"_idx\"], user_idx[self.col_user])\n",
    "            )\n",
    "\n",
    "        if self.item_idx is None:\n",
    "            item_idx = df[[self.col_item]].drop_duplicates()\n",
    "            item_idx[self.col_item + \"_idx\"] = np.arange(len(item_idx))\n",
    "            self.n_items = len(item_idx)\n",
    "            self.item_idx = item_idx\n",
    "\n",
    "            self.item2id = dict(\n",
    "                zip(item_idx[self.col_item], item_idx[self.col_item + \"_idx\"])\n",
    "            )\n",
    "            self.id2item = dict(\n",
    "                zip(item_idx[self.col_item + \"_idx\"], item_idx[self.col_item])\n",
    "            )\n",
    "\n",
    "        return self._reindex(train), self._reindex(test)\n",
    "\n",
    "    def _reindex(self, df):\n",
    "        \"\"\"Process the dataset to reindex userID and itemID and only keep records with ratings greater than 0.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): dataframe with at least columns (col_user, col_item, col_rating).\n",
    "\n",
    "        Returns:\n",
    "            list: train and test pandas.DataFrame Dataset, which have been reindexed and filtered.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if df is None:\n",
    "            return None\n",
    "\n",
    "        df = pd.merge(df, self.user_idx, on=self.col_user, how=\"left\")\n",
    "        df = pd.merge(df, self.item_idx, on=self.col_item, how=\"left\")\n",
    "\n",
    "        df = df[df[self.col_rating] > 0]\n",
    "\n",
    "        df_reindex = df[\n",
    "            [self.col_user + \"_idx\", self.col_item + \"_idx\", self.col_rating]\n",
    "        ]\n",
    "        df_reindex.columns = [self.col_user, self.col_item, self.col_rating]\n",
    "        print(df_reindex)\n",
    "        return df_reindex\n",
    "\n",
    "    def _init_train_data(self):\n",
    "        \"\"\"Record items interated with each user in a dataframe self.interact_status, and create adjacency\n",
    "        matrix self.R.\n",
    "\n",
    "        \"\"\"\n",
    "        self.interact_status = (\n",
    "            self.train.groupby(self.col_user)[self.col_item]\n",
    "            .apply(set)\n",
    "            .reset_index()\n",
    "            .rename(columns={self.col_item: self.col_item + \"_interacted\"})\n",
    "        )\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "        self.R[self.train[self.col_user], self.train[self.col_item]] = 1.0\n",
    "\n",
    "    def get_norm_adj_mat(self):\n",
    "        \"\"\"Load normalized adjacency matrix if it exists, otherwise create (and save) it.\n",
    "\n",
    "        Returns:\n",
    "            scipy.sparse.csr_matrix: Normalized adjacency matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.adj_dir is None:\n",
    "                raise FileNotFoundError\n",
    "            norm_adj_mat = sp.load_npz(self.adj_dir + \"/norm_adj_mat.npz\")\n",
    "            print(\"Already load norm adj matrix.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            norm_adj_mat = self.create_norm_adj_mat()\n",
    "            if self.adj_dir is not None:\n",
    "                sp.save_npz(self.adj_dir + \"/norm_adj_mat.npz\", norm_adj_mat)\n",
    "        return norm_adj_mat\n",
    "\n",
    "    def create_norm_adj_mat(self):\n",
    "        \"\"\"Create normalized adjacency matrix.\n",
    "\n",
    "        Returns:\n",
    "            scipy.sparse.csr_matrix: Normalized adjacency matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        adj_mat = sp.dok_matrix(\n",
    "            (self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32\n",
    "        )\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        R = self.R.tolil()\n",
    "\n",
    "        adj_mat[: self.n_users, self.n_users :] = R\n",
    "        adj_mat[self.n_users :, : self.n_users] = R.T\n",
    "        adj_mat = adj_mat.todok()\n",
    "        print(\"Already create adjacency matrix.\")\n",
    "\n",
    "        rowsum = np.array(adj_mat.sum(1))\n",
    "        d_inv = np.power(rowsum + 1e-9, -0.5).flatten()\n",
    "        d_inv[np.isinf(d_inv)] = 0.0\n",
    "        d_mat_inv = sp.diags(d_inv)\n",
    "        norm_adj_mat = d_mat_inv.dot(adj_mat)\n",
    "        norm_adj_mat = norm_adj_mat.dot(d_mat_inv)\n",
    "        print(\"Already normalize adjacency matrix.\")\n",
    "\n",
    "        return norm_adj_mat.tocsr()\n",
    "\n",
    "    def train_loader(self, batch_size):\n",
    "        \"\"\"Sample train data every batch. One positive item and one negative item sampled for each user.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Batch size of users.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray, numpy.ndarray, numpy.ndarray:\n",
    "            - Sampled users.\n",
    "            - Sampled positive items.\n",
    "            - Sampled negative items.\n",
    "        \"\"\"\n",
    "\n",
    "        def sample_neg(x):\n",
    "            if len(x) >= self.n_items:\n",
    "                raise ValueError(\"A user has voted in every item. Can't find a negative sample.\")\n",
    "            while True:\n",
    "                neg_id = random.randint(0, self.n_items - 1)\n",
    "                if neg_id not in x:\n",
    "                    return neg_id\n",
    "\n",
    "        indices = range(self.n_users_in_train)\n",
    "        if self.n_users < batch_size:\n",
    "            users = [random.choice(indices) for _ in range(batch_size)]\n",
    "        else:\n",
    "            users = random.sample(indices, batch_size)\n",
    "\n",
    "        interact = self.interact_status.iloc[users]\n",
    "        pos_items = interact[self.col_item + \"_interacted\"].apply(\n",
    "            lambda x: random.choice(list(x))\n",
    "        )\n",
    "        neg_items = interact[self.col_item + \"_interacted\"].apply(\n",
    "            lambda x: sample_neg(x)\n",
    "        )\n",
    "\n",
    "        return np.array(users), np.array(pos_items), np.array(neg_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import splitter\n",
    "import movielens\n",
    "import graphLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinmike/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "split = splitter.random_split(movielens.load_pandas_df(\"100K\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       userID  itemID  rating\n",
      "0           0       0     3.0\n",
      "1           1       1     4.0\n",
      "2           2       2     4.0\n",
      "3           3       3     4.0\n",
      "4           4       4     3.0\n",
      "...       ...     ...     ...\n",
      "79994     166      24     3.0\n",
      "79995     751     212     4.0\n",
      "79996     121     161     3.0\n",
      "79997     139     532     5.0\n",
      "79998     142     156     4.0\n",
      "\n",
      "[79999 rows x 3 columns]\n",
      "       userID  itemID  rating\n",
      "0         296     281     5.0\n",
      "1         627     352     3.0\n",
      "2         255     294     5.0\n",
      "3         106     451     5.0\n",
      "4         725     492     1.0\n",
      "...       ...     ...     ...\n",
      "19995     222      73     3.0\n",
      "19996     300     132     3.0\n",
      "19997       2     856     5.0\n",
      "19998      75     179     5.0\n",
      "19999     510      66     2.0\n",
      "\n",
      "[20000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "data = ImplicitCF(train=split[0], test=split[1])\n",
    "# print(data.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index (943) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mgraphLoader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImplicitCF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(data.test)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/LLM4Rec-Dataloader/graphLoader.py:46\u001b[0m, in \u001b[0;36mImplicitCF.__init__\u001b[0;34m(self, train, test, col_user, col_item, col_rating, col_prediction, seed)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_prediction \u001b[38;5;241m=\u001b[39m col_prediction\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex_user_item(train, test)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_train_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(seed)\n",
      "File \u001b[0;32m~/Documents/GitHub/LLM4Rec-Dataloader/graphLoader.py:98\u001b[0m, in \u001b[0;36mImplicitCF._init_train_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minteract_status \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_user)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_item]\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mset\u001b[39m)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_item: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_item \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_interacted\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mR \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mdok_matrix((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_users, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_items), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mR\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol_user\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol_item\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mR)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/sparse/_dok.py:237\u001b[0m, in \u001b[0;36m_dok_base.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value):\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 237\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    240\u001b[0m         key \u001b[38;5;241m=\u001b[39m key[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/sparse/_index.py:102\u001b[0m, in \u001b[0;36mIndexMixin.__setitem__\u001b[0;34m(self, key, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, x):\n\u001b[0;32m--> 102\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row, INT_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, INT_TYPES):\n\u001b[1;32m    105\u001b[0m         x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/sparse/_index.py:186\u001b[0m, in \u001b[0;36mIndexMixin._validate_indices\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    184\u001b[0m     row \u001b[38;5;241m=\u001b[39m _validate_bool_idx(bool_row, M, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m--> 186\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_asindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isintlike(col):\n\u001b[1;32m    189\u001b[0m     col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(col)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/sparse/_index.py:220\u001b[0m, in \u001b[0;36mIndexMixin._asindices\u001b[0;34m(self, idx, length)\u001b[0m\n\u001b[1;32m    218\u001b[0m max_indx \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_indx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m length:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) out of range\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m max_indx)\n\u001b[1;32m    222\u001b[0m min_indx \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmin()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_indx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: index (943) out of range"
     ]
    }
   ],
   "source": [
    "data = graphLoader.ImplicitCF(train=split[0], test=split[1])\n",
    "# print(data.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already create adjacency matrix.\n",
      "Already normalize adjacency matrix.\n",
      "  (0, 943)\t0.0314970389008522\n",
      "  (0, 987)\t0.012123215943574905\n",
      "  (0, 1009)\t0.013263952918350697\n",
      "  (0, 1035)\t0.017323972657322884\n",
      "  (0, 1052)\t0.01200038380920887\n",
      "  (0, 1084)\t0.010631080716848373\n",
      "  (0, 1089)\t0.01918824017047882\n",
      "  (0, 1094)\t0.012543190270662308\n",
      "  (0, 1154)\t0.01277023646980524\n",
      "  (0, 1155)\t0.01585903763771057\n",
      "  (0, 1161)\t0.0144942756742239\n",
      "  (0, 1200)\t0.0144942756742239\n",
      "  (0, 1202)\t0.016835875809192657\n",
      "  (0, 1223)\t0.013856849633157253\n",
      "  (0, 1229)\t0.017039954662322998\n",
      "  (0, 1277)\t0.015915175899863243\n",
      "  (0, 1291)\t0.010531567968428135\n",
      "  (0, 1397)\t0.018101215362548828\n",
      "  (0, 1498)\t0.029880713671445847\n",
      "  (0, 1499)\t0.016386838629841805\n",
      "  (0, 1503)\t0.012828949838876724\n",
      "  (0, 1560)\t0.015803487971425056\n",
      "  (0, 1576)\t0.0306569654494524\n",
      "  (0, 1699)\t0.024397501721978188\n",
      "  (0, 1879)\t0.029160592705011368\n",
      "  :\t:\n",
      "  (2571, 118)\t0.04019339382648468\n",
      "  (2572, 118)\t0.04019339382648468\n",
      "  (2573, 300)\t0.06468462198972702\n",
      "  (2574, 403)\t0.07715167850255966\n",
      "  (2575, 165)\t0.05360562726855278\n",
      "  (2576, 689)\t0.14586499333381653\n",
      "  (2577, 415)\t0.08944272249937057\n",
      "  (2578, 183)\t0.07198157161474228\n",
      "  (2579, 118)\t0.04019339382648468\n",
      "  (2580, 129)\t0.052777983248233795\n",
      "  (2581, 165)\t0.05360562726855278\n",
      "  (2582, 142)\t0.04419417306780815\n",
      "  (2583, 346)\t0.18898223340511322\n",
      "  (2584, 118)\t0.04019339382648468\n",
      "  (2585, 188)\t0.04902903363108635\n",
      "  (2586, 606)\t0.11322770267724991\n",
      "  (2587, 128)\t0.0653720423579216\n",
      "  (2588, 338)\t0.05842062458395958\n",
      "  (2589, 316)\t0.14433756470680237\n",
      "  (2590, 67)\t0.051164451986551285\n",
      "  (2591, 340)\t0.11704114824533463\n",
      "  (2592, 144)\t0.042409446090459824\n",
      "  (2593, 129)\t0.052777983248233795\n",
      "  (2594, 118)\t0.04019339382648468\n",
      "  (2595, 316)\t0.14433756470680237\n"
     ]
    }
   ],
   "source": [
    "print(data.get_norm_adj_mat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([764,  52]), array([868, 304]), array([615, 956]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_loader(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
