{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AdditiveAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A general additive attention module.\n",
    "    Originally for NAML.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 query_vector_dim,\n",
    "                 candidate_vector_dim,\n",
    "                 writer=None,\n",
    "                 tag=None,\n",
    "                 names=None):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.linear = nn.Linear(candidate_vector_dim, query_vector_dim)\n",
    "        self.attention_query_vector = nn.Parameter(\n",
    "            torch.empty(query_vector_dim).uniform_(-0.1, 0.1))\n",
    "        # For tensorboard\n",
    "        self.writer = writer\n",
    "        self.tag = tag\n",
    "        self.names = names\n",
    "        self.local_step = 1\n",
    "\n",
    "    def forward(self, candidate_vector):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            candidate_vector: batch_size, candidate_size, candidate_vector_dim\n",
    "        Returns:\n",
    "            (shape) batch_size, candidate_vector_dim\n",
    "        \"\"\"\n",
    "        # batch_size, candidate_size, query_vector_dim\n",
    "        temp = torch.tanh(self.linear(candidate_vector))\n",
    "        # batch_size, candidate_size\n",
    "        candidate_weights = F.softmax(torch.matmul(\n",
    "            temp, self.attention_query_vector),\n",
    "                                      dim=1)\n",
    "        if self.writer is not None:\n",
    "            assert candidate_weights.size(1) == len(self.names)\n",
    "            if self.local_step % 10 == 0:\n",
    "                self.writer.add_scalars(\n",
    "                    self.tag, {\n",
    "                        x: y\n",
    "                        for x, y in zip(self.names,\n",
    "                                        candidate_weights.mean(dim=0))\n",
    "                    }, self.local_step)\n",
    "            self.local_step += 1\n",
    "        # batch_size, candidate_vector_dim\n",
    "        target = torch.bmm(candidate_weights.unsqueeze(dim=1),\n",
    "                           candidate_vector).squeeze(dim=1)\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TODO read\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask=None):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k)\n",
    "        scores = torch.exp(scores)\n",
    "        if attn_mask is not None:\n",
    "            scores = scores * attn_mask\n",
    "        attn = scores / (torch.sum(scores, dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_attention_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        assert d_model % num_attention_heads == 0\n",
    "        self.d_k = d_model // num_attention_heads\n",
    "        self.d_v = d_model // num_attention_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "\n",
    "    def forward(self, Q, K=None, V=None, length=None):\n",
    "        if K is None:\n",
    "            K = Q\n",
    "        if V is None:\n",
    "            V = Q\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.num_attention_heads,\n",
    "                               self.d_k).transpose(1, 2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.num_attention_heads,\n",
    "                               self.d_k).transpose(1, 2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.num_attention_heads,\n",
    "                               self.d_v).transpose(1, 2)\n",
    "\n",
    "        if length is not None:\n",
    "            maxlen = Q.size(1)\n",
    "            attn_mask = torch.arange(maxlen).to(device).expand(\n",
    "                batch_size, maxlen) < length.to(device).view(-1, 1)\n",
    "            attn_mask = attn_mask.unsqueeze(1).expand(batch_size, maxlen,\n",
    "                                                      maxlen)\n",
    "            attn_mask = attn_mask.unsqueeze(1).repeat(1,\n",
    "                                                      self.num_attention_heads,\n",
    "                                                      1, 1)\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        context, attn = ScaledDotProductAttention(self.d_k)(q_s, k_s, v_s,\n",
    "                                                            attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.num_attention_heads * self.d_v)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class NewsEncoder(torch.nn.Module):\n",
    "    def __init__(self, config, pretrained_word_embedding):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.config = config\n",
    "        if pretrained_word_embedding is None:\n",
    "            self.word_embedding = nn.Embedding(config.num_words,\n",
    "                                               config.word_embedding_dim,\n",
    "                                               padding_idx=0)\n",
    "        else:\n",
    "            self.word_embedding = nn.Embedding.from_pretrained(\n",
    "                pretrained_word_embedding, freeze=False, padding_idx=0)\n",
    "\n",
    "        self.multihead_self_attention = MultiHeadSelfAttention(\n",
    "            config.word_embedding_dim, config.num_attention_heads)\n",
    "        self.additive_attention = AdditiveAttention(config.query_vector_dim,\n",
    "                                                    config.word_embedding_dim)\n",
    "\n",
    "    def forward(self, news):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            news:\n",
    "                {\n",
    "                    \"title\": batch_size * num_words_title\n",
    "                }\n",
    "        Returns:\n",
    "            (shape) batch_size, word_embedding_dim\n",
    "        \"\"\"\n",
    "        # batch_size, num_words_title, word_embedding_dim\n",
    "        news_vector = F.dropout(self.word_embedding(news[\"title\"].to(device)),\n",
    "                                p=self.config.dropout_probability,\n",
    "                                training=self.training)\n",
    "        # batch_size, num_words_title, word_embedding_dim\n",
    "        multihead_news_vector = self.multihead_self_attention(news_vector)\n",
    "        multihead_news_vector = F.dropout(multihead_news_vector,\n",
    "                                          p=self.config.dropout_probability,\n",
    "                                          training=self.training)\n",
    "        # batch_size, word_embedding_dim\n",
    "        final_news_vector = self.additive_attention(multihead_news_vector)\n",
    "        return final_news_vector\n",
    "class UserEncoder(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(UserEncoder, self).__init__()\n",
    "        self.config = config\n",
    "        self.multihead_self_attention = MultiHeadSelfAttention(\n",
    "            config.word_embedding_dim, config.num_attention_heads)\n",
    "        self.additive_attention = AdditiveAttention(config.query_vector_dim,\n",
    "                                                    config.word_embedding_dim)\n",
    "\n",
    "    def forward(self, user_vector):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_vector: batch_size, num_clicked_news_a_user, word_embedding_dim\n",
    "        Returns:\n",
    "            (shape) batch_size, word_embedding_dim\n",
    "        \"\"\"\n",
    "        # batch_size, num_clicked_news_a_user, word_embedding_dim\n",
    "        multihead_user_vector = self.multihead_self_attention(user_vector)\n",
    "        # batch_size, word_embedding_dim\n",
    "        final_user_vector = self.additive_attention(multihead_user_vector)\n",
    "        return final_user_vector\n",
    "\n",
    "\n",
    "class NRMS(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    NRMS network.\n",
    "    Input 1 + K candidate news and a list of user clicked news, produce the click probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, pretrained_word_embedding=None):\n",
    "        super(NRMS, self).__init__()\n",
    "        self.config = config\n",
    "        self.news_encoder = NewsEncoder(config, pretrained_word_embedding)\n",
    "        self.user_encoder = UserEncoder(config)\n",
    "        self.click_predictor = DotProductClickPredictor()\n",
    "\n",
    "    def forward(self, candidate_news, clicked_news):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            candidate_news:\n",
    "                [\n",
    "                    {\n",
    "                        \"title\": batch_size * num_words_title\n",
    "                    } * (1 + K)\n",
    "                ]\n",
    "            clicked_news:\n",
    "                [\n",
    "                    {\n",
    "                        \"title\":batch_size * num_words_title\n",
    "                    } * num_clicked_news_a_user\n",
    "                ]\n",
    "        Returns:\n",
    "          click_probability: batch_size, 1 + K\n",
    "        \"\"\"\n",
    "        # batch_size, 1 + K, word_embedding_dim\n",
    "        candidate_news_vector = torch.stack(\n",
    "            [self.news_encoder(x) for x in candidate_news], dim=1)\n",
    "        # batch_size, num_clicked_news_a_user, word_embedding_dim\n",
    "        clicked_news_vector = torch.stack(\n",
    "            [self.news_encoder(x) for x in clicked_news], dim=1)\n",
    "        # batch_size, word_embedding_dim\n",
    "        user_vector = self.user_encoder(clicked_news_vector)\n",
    "        # batch_size, 1 + K\n",
    "        click_probability = self.click_predictor(candidate_news_vector,\n",
    "                                                 user_vector)\n",
    "        return click_probability\n",
    "\n",
    "    def get_news_vector(self, news):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            news:\n",
    "                {\n",
    "                    \"title\": batch_size * num_words_title\n",
    "                },\n",
    "        Returns:\n",
    "            (shape) batch_size, word_embedding_dim\n",
    "        \"\"\"\n",
    "        # batch_size, word_embedding_dim\n",
    "        return self.news_encoder(news)\n",
    "\n",
    "    def get_user_vector(self, clicked_news_vector):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clicked_news_vector: batch_size, num_clicked_news_a_user, word_embedding_dim\n",
    "        Returns:\n",
    "            (shape) batch_size, word_embedding_dim\n",
    "        \"\"\"\n",
    "        # batch_size, word_embedding_dim\n",
    "        return self.user_encoder(clicked_news_vector)\n",
    "\n",
    "    def get_prediction(self, news_vector, user_vector):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            news_vector: candidate_size, word_embedding_dim\n",
    "            user_vector: word_embedding_dim\n",
    "        Returns:\n",
    "            click_probability: candidate_size\n",
    "        \"\"\"\n",
    "        # candidate_size\n",
    "        return self.click_predictor(\n",
    "            news_vector.unsqueeze(dim=0),\n",
    "            user_vector.unsqueeze(dim=0)).squeeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_name = os.environ['MODEL_NAME'] if 'MODEL_NAME' in os.environ else 'NRMS'\n",
    "# Currently included model\n",
    "assert model_name in [\n",
    "    'NRMS', 'NAML', 'LSTUR', 'DKN', 'HiFiArk', 'TANR', 'Exp1'\n",
    "]\n",
    "\n",
    "\n",
    "class BaseConfig():\n",
    "    \"\"\"\n",
    "    General configurations appiled to all models\n",
    "    \"\"\"\n",
    "    num_epochs = 2\n",
    "    num_batches_show_loss = 100  # Number of batchs to show loss\n",
    "    # Number of batchs to check metrics on validation dataset\n",
    "    num_batches_validate = 1000\n",
    "    batch_size = 128\n",
    "    learning_rate = 0.0001\n",
    "    num_workers = 4  # Number of workers for data loading\n",
    "    num_clicked_news_a_user = 50  # Number of sampled click history for each user\n",
    "    num_words_title = 20\n",
    "    num_words_abstract = 50\n",
    "    word_freq_threshold = 1\n",
    "    entity_freq_threshold = 2\n",
    "    entity_confidence_threshold = 0.5\n",
    "    negative_sampling_ratio = 2  # K\n",
    "    dropout_probability = 0.2\n",
    "    # Modify the following by the output of `src/dataprocess.py`\n",
    "    num_words = 1 + 70975\n",
    "    num_categories = 1 + 274\n",
    "    num_entities = 1 + 12957\n",
    "    num_users = 1 + 50000\n",
    "    word_embedding_dim = 300\n",
    "    category_embedding_dim = 100\n",
    "    # Modify the following only if you use another dataset\n",
    "    entity_embedding_dim = 100\n",
    "    # For additive attention\n",
    "    query_vector_dim = 200\n",
    "\n",
    "\n",
    "class NRMSConfig(BaseConfig):\n",
    "    dataset_attributes = {\"news\": ['title'], \"record\": []}\n",
    "    # For multi-head self-attention\n",
    "    num_attention_heads = 15\n",
    "\n",
    "\n",
    "class NAMLConfig(BaseConfig):\n",
    "    dataset_attributes = {\n",
    "        \"news\": ['category', 'subcategory', 'title', 'abstract'],\n",
    "        \"record\": []\n",
    "    }\n",
    "    # For CNN\n",
    "    num_filters = 300\n",
    "    window_size = 3\n",
    "\n",
    "\n",
    "class LSTURConfig(BaseConfig):\n",
    "    dataset_attributes = {\n",
    "        \"news\": ['category', 'subcategory', 'title'],\n",
    "        \"record\": ['user', 'clicked_news_length']\n",
    "    }\n",
    "    # For CNN\n",
    "    num_filters = 300\n",
    "    window_size = 3\n",
    "    long_short_term_method = 'ini'\n",
    "    # See paper for more detail\n",
    "    assert long_short_term_method in ['ini', 'con']\n",
    "    masking_probability = 0.5\n",
    "\n",
    "\n",
    "class DKNConfig(BaseConfig):\n",
    "    dataset_attributes = {\"news\": ['title', 'title_entities'], \"record\": []}\n",
    "    # For CNN\n",
    "    num_filters = 50\n",
    "    window_sizes = [2, 3, 4]\n",
    "    # TODO: currently context is not available\n",
    "    use_context = False\n",
    "\n",
    "\n",
    "class HiFiArkConfig(BaseConfig):\n",
    "    dataset_attributes = {\"news\": ['title'], \"record\": []}\n",
    "    # For CNN\n",
    "    num_filters = 300\n",
    "    window_size = 3\n",
    "    num_pooling_heads = 5\n",
    "    regularizer_loss_weight = 0.1\n",
    "\n",
    "\n",
    "class TANRConfig(BaseConfig):\n",
    "    dataset_attributes = {\"news\": ['category', 'title'], \"record\": []}\n",
    "    # For CNN\n",
    "    num_filters = 300\n",
    "    window_size = 3\n",
    "    topic_classification_loss_weight = 0.1\n",
    "\n",
    "\n",
    "class Exp1Config(BaseConfig):\n",
    "    dataset_attributes = {\n",
    "        # TODO ['category', 'subcategory', 'title', 'abstract'],\n",
    "        \"news\": ['category', 'subcategory', 'title'],\n",
    "        \"record\": []\n",
    "    }\n",
    "    # For multi-head self-attention\n",
    "    num_attention_heads = 15\n",
    "    ensemble_factor = 1  # Not use ensemble since it's too expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not included!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1324\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import csv\n",
    "import importlib\n",
    "\n",
    "try:\n",
    "    config = getattr(importlib.import_module('config'), f\"{model_name}Config\")\n",
    "except AttributeError:\n",
    "    print(f\"{model_name} not included!\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "def parse_behaviors(source, target, user2int_path):\n",
    "    \"\"\"\n",
    "    Parse behaviors file in training set.\n",
    "    Args:\n",
    "        source: source behaviors file\n",
    "        target: target behaviors file\n",
    "        user2int_path: path for saving user2int file\n",
    "    \"\"\"\n",
    "    print(f\"Parse {source}\")\n",
    "\n",
    "    behaviors = pd.read_table(\n",
    "        source,\n",
    "        header=None,\n",
    "        names=['impression_id', 'user', 'time', 'clicked_news', 'impressions'])\n",
    "    behaviors.clicked_news.fillna(' ', inplace=True)\n",
    "    behaviors.impressions = behaviors.impressions.str.split()\n",
    "\n",
    "    user2int = {}\n",
    "    for row in behaviors.itertuples(index=False):\n",
    "        if row.user not in user2int:\n",
    "            user2int[row.user] = len(user2int) + 1\n",
    "\n",
    "    pd.DataFrame(user2int.items(), columns=['user',\n",
    "                                            'int']).to_csv(user2int_path,\n",
    "                                                           sep='\\t',\n",
    "                                                           index=False)\n",
    "    print(\n",
    "        f'Please modify `num_users` in `src/config.py` into 1 + {len(user2int)}'\n",
    "    )\n",
    "\n",
    "    for row in behaviors.itertuples():\n",
    "        behaviors.at[row.Index, 'user'] = user2int[row.user]\n",
    "\n",
    "    for row in tqdm(behaviors.itertuples(), desc=\"Balancing data\"):\n",
    "        positive = iter([x for x in row.impressions if x.endswith('1')])\n",
    "        negative = [x for x in row.impressions if x.endswith('0')]\n",
    "        random.shuffle(negative)\n",
    "        negative = iter(negative)\n",
    "        pairs = []\n",
    "        try:\n",
    "            while True:\n",
    "                pair = [next(positive)]\n",
    "                for _ in range(config.negative_sampling_ratio):\n",
    "                    pair.append(next(negative))\n",
    "                pairs.append(pair)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "        behaviors.at[row.Index, 'impressions'] = pairs\n",
    "\n",
    "    behaviors = behaviors.explode('impressions').dropna(\n",
    "        subset=[\"impressions\"]).reset_index(drop=True)\n",
    "    behaviors[['candidate_news', 'clicked']] = pd.DataFrame(\n",
    "        behaviors.impressions.map(\n",
    "            lambda x: (' '.join([e.split('-')[0] for e in x]), ' '.join(\n",
    "                [e.split('-')[1] for e in x]))).tolist())\n",
    "    behaviors.to_csv(\n",
    "        target,\n",
    "        sep='\\t',\n",
    "        index=False,\n",
    "        columns=['user', 'clicked_news', 'candidate_news', 'clicked'])\n",
    "\n",
    "\n",
    "def parse_news(source, target, category2int_path, word2int_path,\n",
    "               entity2int_path, mode):\n",
    "    \"\"\"\n",
    "    Parse news for training set and test set\n",
    "    Args:\n",
    "        source: source news file\n",
    "        target: target news file\n",
    "        if mode == 'train':\n",
    "            category2int_path, word2int_path, entity2int_path: Path to save\n",
    "        elif mode == 'test':\n",
    "            category2int_path, word2int_path, entity2int_path: Path to load from\n",
    "    \"\"\"\n",
    "    print(f\"Parse {source}\")\n",
    "    news = pd.read_table(source,\n",
    "                         header=None,\n",
    "                         usecols=[0, 1, 2, 3, 4, 6, 7],\n",
    "                         quoting=csv.QUOTE_NONE,\n",
    "                         names=[\n",
    "                             'id', 'category', 'subcategory', 'title',\n",
    "                             'abstract', 'title_entities', 'abstract_entities'\n",
    "                         ])  # TODO try to avoid csv.QUOTE_NONE\n",
    "    news.title_entities.fillna('[]', inplace=True)\n",
    "    news.abstract_entities.fillna('[]', inplace=True)\n",
    "    news.fillna(' ', inplace=True)\n",
    "\n",
    "    def parse_row(row):\n",
    "        new_row = [\n",
    "            row.id,\n",
    "            category2int[row.category] if row.category in category2int else 0,\n",
    "            category2int[row.subcategory]\n",
    "            if row.subcategory in category2int else 0,\n",
    "            [0] * config.num_words_title, [0] * config.num_words_abstract,\n",
    "            [0] * config.num_words_title, [0] * config.num_words_abstract\n",
    "        ]\n",
    "\n",
    "        # Calculate local entity map (map lower single word to entity)\n",
    "        local_entity_map = {}\n",
    "        for e in json.loads(row.title_entities):\n",
    "            if e['Confidence'] > config.entity_confidence_threshold and e[\n",
    "                    'WikidataId'] in entity2int:\n",
    "                for x in ' '.join(e['SurfaceForms']).lower().split():\n",
    "                    local_entity_map[x] = entity2int[e['WikidataId']]\n",
    "        for e in json.loads(row.abstract_entities):\n",
    "            if e['Confidence'] > config.entity_confidence_threshold and e[\n",
    "                    'WikidataId'] in entity2int:\n",
    "                for x in ' '.join(e['SurfaceForms']).lower().split():\n",
    "                    local_entity_map[x] = entity2int[e['WikidataId']]\n",
    "\n",
    "        try:\n",
    "            for i, w in enumerate(word_tokenize(row.title.lower())):\n",
    "                if w in word2int:\n",
    "                    new_row[3][i] = word2int[w]\n",
    "                    if w in local_entity_map:\n",
    "                        new_row[5][i] = local_entity_map[w]\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            for i, w in enumerate(word_tokenize(row.abstract.lower())):\n",
    "                if w in word2int:\n",
    "                    new_row[4][i] = word2int[w]\n",
    "                    if w in local_entity_map:\n",
    "                        new_row[6][i] = local_entity_map[w]\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return pd.Series(new_row,\n",
    "                         index=[\n",
    "                             'id', 'category', 'subcategory', 'title',\n",
    "                             'abstract', 'title_entities', 'abstract_entities'\n",
    "                         ])\n",
    "\n",
    "    if mode == 'train':\n",
    "        category2int = {}\n",
    "        word2int = {}\n",
    "        word2freq = {}\n",
    "        entity2int = {}\n",
    "        entity2freq = {}\n",
    "\n",
    "        for row in news.itertuples(index=False):\n",
    "            if row.category not in category2int:\n",
    "                category2int[row.category] = len(category2int) + 1\n",
    "            if row.subcategory not in category2int:\n",
    "                category2int[row.subcategory] = len(category2int) + 1\n",
    "\n",
    "            for w in word_tokenize(row.title.lower()):\n",
    "                if w not in word2freq:\n",
    "                    word2freq[w] = 1\n",
    "                else:\n",
    "                    word2freq[w] += 1\n",
    "            for w in word_tokenize(row.abstract.lower()):\n",
    "                if w not in word2freq:\n",
    "                    word2freq[w] = 1\n",
    "                else:\n",
    "                    word2freq[w] += 1\n",
    "\n",
    "            for e in json.loads(row.title_entities):\n",
    "                times = len(e['OccurrenceOffsets']) * e['Confidence']\n",
    "                if times > 0:\n",
    "                    if e['WikidataId'] not in entity2freq:\n",
    "                        entity2freq[e['WikidataId']] = times\n",
    "                    else:\n",
    "                        entity2freq[e['WikidataId']] += times\n",
    "\n",
    "            for e in json.loads(row.abstract_entities):\n",
    "                times = len(e['OccurrenceOffsets']) * e['Confidence']\n",
    "                if times > 0:\n",
    "                    if e['WikidataId'] not in entity2freq:\n",
    "                        entity2freq[e['WikidataId']] = times\n",
    "                    else:\n",
    "                        entity2freq[e['WikidataId']] += times\n",
    "\n",
    "        for k, v in word2freq.items():\n",
    "            if v >= config.word_freq_threshold:\n",
    "                word2int[k] = len(word2int) + 1\n",
    "\n",
    "        for k, v in entity2freq.items():\n",
    "            if v >= config.entity_freq_threshold:\n",
    "                entity2int[k] = len(entity2int) + 1\n",
    "\n",
    "        parsed_news = news.swifter.apply(parse_row, axis=1)\n",
    "        parsed_news.to_csv(target, sep='\\t', index=False)\n",
    "\n",
    "        pd.DataFrame(category2int.items(),\n",
    "                     columns=['category', 'int']).to_csv(category2int_path,\n",
    "                                                         sep='\\t',\n",
    "                                                         index=False)\n",
    "        print(\n",
    "            f'Please modify `num_categories` in `src/config.py` into 1 + {len(category2int)}'\n",
    "        )\n",
    "\n",
    "        pd.DataFrame(word2int.items(), columns=['word',\n",
    "                                                'int']).to_csv(word2int_path,\n",
    "                                                               sep='\\t',\n",
    "                                                               index=False)\n",
    "        print(\n",
    "            f'Please modify `num_words` in `src/config.py` into 1 + {len(word2int)}'\n",
    "        )\n",
    "\n",
    "        pd.DataFrame(entity2int.items(),\n",
    "                     columns=['entity', 'int']).to_csv(entity2int_path,\n",
    "                                                       sep='\\t',\n",
    "                                                       index=False)\n",
    "        print(\n",
    "            f'Please modify `num_entities` in `src/config.py` into 1 + {len(entity2int)}'\n",
    "        )\n",
    "\n",
    "    elif mode == 'test':\n",
    "        category2int = dict(pd.read_table(category2int_path).values.tolist())\n",
    "        # na_filter=False is needed since nan is also a valid word\n",
    "        word2int = dict(\n",
    "            pd.read_table(word2int_path, na_filter=False).values.tolist())\n",
    "        entity2int = dict(pd.read_table(entity2int_path).values.tolist())\n",
    "\n",
    "        parsed_news = news.swifter.apply(parse_row, axis=1)\n",
    "        parsed_news.to_csv(target, sep='\\t', index=False)\n",
    "\n",
    "    else:\n",
    "        print('Wrong mode!')\n",
    "\n",
    "\n",
    "def generate_word_embedding(source, target, word2int_path):\n",
    "    \"\"\"\n",
    "    Generate from pretrained word embedding file\n",
    "    If a word not in embedding file, initial its embedding by N(0, 1)\n",
    "    Args:\n",
    "        source: path of pretrained word embedding file, e.g. glove.840B.300d.txt\n",
    "        target: path for saving word embedding. Will be saved in numpy format\n",
    "        word2int_path: vocabulary file when words in it will be searched in pretrained embedding file\n",
    "    \"\"\"\n",
    "    # na_filter=False is needed since nan is also a valid word\n",
    "    # word, int\n",
    "    word2int = pd.read_table(word2int_path, na_filter=False, index_col='word')\n",
    "    source_embedding = pd.read_table(source,\n",
    "                                     index_col=0,\n",
    "                                     sep=' ',\n",
    "                                     header=None,\n",
    "                                     quoting=csv.QUOTE_NONE,\n",
    "                                     names=range(config.word_embedding_dim))\n",
    "    # word, vector\n",
    "    source_embedding.index.rename('word', inplace=True)\n",
    "    # word, int, vector\n",
    "    merged = word2int.merge(source_embedding,\n",
    "                            how='inner',\n",
    "                            left_index=True,\n",
    "                            right_index=True)\n",
    "    merged.set_index('int', inplace=True)\n",
    "\n",
    "    missed_index = np.setdiff1d(np.arange(len(word2int) + 1),\n",
    "                                merged.index.values)\n",
    "    missed_embedding = pd.DataFrame(data=np.random.normal(\n",
    "        size=(len(missed_index), config.word_embedding_dim)))\n",
    "    missed_embedding['int'] = missed_index\n",
    "    missed_embedding.set_index('int', inplace=True)\n",
    "\n",
    "    final_embedding = pd.concat([merged, missed_embedding]).sort_index()\n",
    "    np.save(target, final_embedding.values)\n",
    "\n",
    "    print(\n",
    "        f'Rate of word missed in pretrained embedding: {(len(missed_index)-1)/len(word2int):.4f}'\n",
    "    )\n",
    "\n",
    "\n",
    "def transform_entity_embedding(source, target, entity2int_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: path of embedding file\n",
    "        target: path of transformed embedding file in numpy format\n",
    "        entity2int_path\n",
    "    \"\"\"\n",
    "    entity_embedding = pd.read_table(source, header=None)\n",
    "    entity_embedding['vector'] = entity_embedding.iloc[:,\n",
    "                                                       1:101].values.tolist()\n",
    "    entity_embedding = entity_embedding[[0, 'vector'\n",
    "                                         ]].rename(columns={0: \"entity\"})\n",
    "\n",
    "    entity2int = pd.read_table(entity2int_path)\n",
    "    merged_df = pd.merge(entity_embedding, entity2int,\n",
    "                         on='entity').sort_values('int')\n",
    "    entity_embedding_transformed = np.random.normal(\n",
    "        size=(len(entity2int) + 1, config.entity_embedding_dim))\n",
    "    for row in merged_df.itertuples(index=False):\n",
    "        entity_embedding_transformed[row.int] = row.vector\n",
    "    np.save(target, entity_embedding_transformed)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_dir = './data/train'\n",
    "    val_dir = './data/val'\n",
    "    test_dir = './data/test'\n",
    "\n",
    "    print('Process data for training')\n",
    "\n",
    "    print('Parse behaviors')\n",
    "    parse_behaviors(path.join(train_dir, 'behaviors.tsv'),\n",
    "                    path.join(train_dir, 'behaviors_parsed.tsv'),\n",
    "                    path.join(train_dir, 'user2int.tsv'))\n",
    "\n",
    "    print('Parse news')\n",
    "    parse_news(path.join(train_dir, 'news.tsv'),\n",
    "               path.join(train_dir, 'news_parsed.tsv'),\n",
    "               path.join(train_dir, 'category2int.tsv'),\n",
    "               path.join(train_dir, 'word2int.tsv'),\n",
    "               path.join(train_dir, 'entity2int.tsv'),\n",
    "               mode='train')\n",
    "\n",
    "    print('Generate word embedding')\n",
    "    generate_word_embedding(\n",
    "        f'./data/glove/glove.840B.{config.word_embedding_dim}d.txt',\n",
    "        path.join(train_dir, 'pretrained_word_embedding.npy'),\n",
    "        path.join(train_dir, 'word2int.tsv'))\n",
    "\n",
    "    print('Transform entity embeddings')\n",
    "    transform_entity_embedding(\n",
    "        path.join(train_dir, 'entity_embedding.vec'),\n",
    "        path.join(train_dir, 'pretrained_entity_embedding.npy'),\n",
    "        path.join(train_dir, 'entity2int.tsv'))\n",
    "\n",
    "    print('\\nProcess data for validation')\n",
    "\n",
    "    print('Parse news')\n",
    "    parse_news(path.join(val_dir, 'news.tsv'),\n",
    "               path.join(val_dir, 'news_parsed.tsv'),\n",
    "               path.join(train_dir, 'category2int.tsv'),\n",
    "               path.join(train_dir, 'word2int.tsv'),\n",
    "               path.join(train_dir, 'entity2int.tsv'),\n",
    "               mode='test')\n",
    "\n",
    "    print('\\nProcess data for test')\n",
    "\n",
    "    print('Parse news')\n",
    "    parse_news(path.join(test_dir, 'news.tsv'),\n",
    "               path.join(test_dir, 'news_parsed.tsv'),\n",
    "               path.join(train_dir, 'category2int.tsv'),\n",
    "               path.join(train_dir, 'word2int.tsv'),\n",
    "               path.join(train_dir, 'entity2int.tsv'),\n",
    "               mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pexpect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpip install swifter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/ipykernel/zmqshell.py:657\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/IPython/utils/_process_posix.py:125\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msystem\u001b[39m(\u001b[38;5;28mself\u001b[39m, cmd):\n\u001b[1;32m    114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute a command in a subshell.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    int : child's exitstatus\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpexpect\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# Get likely encoding for the output.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     enc \u001b[38;5;241m=\u001b[39m DEFAULT_ENCODING\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pexpect'"
     ]
    }
   ],
   "source": [
    "!pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
