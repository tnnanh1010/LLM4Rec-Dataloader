{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jb2bDxXnL4Ot",
        "outputId": "a5cf2e33-b982-4137-ce69-b654c3071002"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E7i79ErILbl8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "import logging\n",
        "import os\n",
        "import torch.optim as optim\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WxUXYhf2MjaM"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "@dataclass\n",
        "class Args:\n",
        "    nGPU: int = 1\n",
        "    seed: int = 0\n",
        "    prepare: bool = True\n",
        "    mode: str = \"train\"\n",
        "    train_data_dir: str = \"/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/MINDsmall_train\"\n",
        "    test_data_dir: str = \"/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/MINDsmall_dev\"\n",
        "    train_abstract_dir: str = '/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/genAbs0.json'\n",
        "    # \"/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/train_gen_abs.json\"\n",
        "    test_abstract_dir: str = '/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/genAbs0.json'\n",
        "    # \"/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/Dev_gen_abs.json\"\n",
        "    model_dir: str = '/content/model'\n",
        "    batch_size: int = 32\n",
        "    npratio: int = 4\n",
        "    enable_gpu: bool = False\n",
        "    filter_num: int = 3\n",
        "    log_steps: int = 100\n",
        "    epochs: int = 5\n",
        "    lr: float = 0.0003\n",
        "    num_words_title: int = 20\n",
        "    num_words_abstract: int = 50\n",
        "    user_log_length: int = 50\n",
        "    word_embedding_dim: int = 300\n",
        "    glove_embedding_path: str = '/home/vinmike/Downloads/glove.840B.300d.txt'\n",
        "    freeze_embedding: bool = False\n",
        "    news_dim: int = 400\n",
        "    news_query_vector_dim: int = 200\n",
        "    user_query_vector_dim: int = 200\n",
        "    num_attention_heads: int = 15\n",
        "    user_log_mask: bool = False\n",
        "    drop_rate: float = 0.2\n",
        "    save_steps: int = 10000\n",
        "    start_epoch: int = 0\n",
        "    load_ckpt_name: Optional[str] = None\n",
        "    use_category: bool = True\n",
        "    use_subcategory: bool = True\n",
        "    use_abstract: bool = True\n",
        "    use_custom_abstract: bool = True\n",
        "    category_emb_dim: int = 100\n",
        "\n",
        "def parse_args():\n",
        "  return Args()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_SLKbAZpKFK"
      },
      "source": [
        "**Dataset.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pqMrlk2u-Hiv"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import IterableDataset, Dataset\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "class DatasetTrain(IterableDataset):\n",
        "    def __init__(self, filename, news_index, news_combined, args):\n",
        "        super(DatasetTrain).__init__()\n",
        "        self.filename = filename\n",
        "        self.news_index = news_index\n",
        "        self.news_combined = news_combined\n",
        "        self.args = args\n",
        "\n",
        "    def trans_to_nindex(self, nids):\n",
        "        return [self.news_index[i] if i in self.news_index else 0 for i in nids]\n",
        "\n",
        "    def pad_to_fix_len(self, x, fix_length, padding_front=True, padding_value=0):\n",
        "        if padding_front:\n",
        "            pad_x = [padding_value] * (fix_length - len(x)) + x[-fix_length:]\n",
        "            mask = [0] * (fix_length - len(x)) + [1] * min(fix_length, len(x))\n",
        "        else:\n",
        "            pad_x = x[-fix_length:] + [padding_value] * (fix_length - len(x))\n",
        "            mask = [1] * min(fix_length, len(x)) + [0] * (fix_length - len(x))\n",
        "        return pad_x, np.array(mask, dtype='float32')\n",
        "\n",
        "    def line_mapper(self, line):\n",
        "        line = line.strip().split('\\t')\n",
        "        click_docs = line[3].split()\n",
        "        sess_pos = line[4].split()\n",
        "        sess_neg = line[5].split()\n",
        "\n",
        "        click_docs, log_mask = self.pad_to_fix_len(self.trans_to_nindex(click_docs), self.args.user_log_length)\n",
        "        user_feature = self.news_combined[click_docs]\n",
        "\n",
        "        pos = self.trans_to_nindex(sess_pos)\n",
        "        neg = self.trans_to_nindex(sess_neg)\n",
        "\n",
        "        label = random.randint(0, self.args.npratio)\n",
        "        sample_news = neg[:label] + pos + neg[label:]\n",
        "        sample_news, log_mask = self.pad_to_fix_len(sample_news, self.args.user_log_length)\n",
        "\n",
        "        news_feature = self.news_combined[sample_news]\n",
        "        \n",
        "        return user_feature, news_feature, label\n",
        "\n",
        "    def __iter__(self):\n",
        "        file_iter = open(self.filename)\n",
        "        return map(self.line_mapper, file_iter)\n",
        "\n",
        "\n",
        "\n",
        "class DatasetTest(DatasetTrain):\n",
        "    def __init__(self, filename, news_index, news_scoring, args):\n",
        "        super(DatasetTrain).__init__()\n",
        "        self.filename = filename\n",
        "        self.news_index = news_index\n",
        "        self.news_scoring = news_scoring\n",
        "        self.args = args\n",
        "\n",
        "    def line_mapper(self, line):\n",
        "        line = line.strip().split('\\t')\n",
        "        click_docs = line[3].split()\n",
        "        click_docs, log_mask = self.pad_to_fix_len(self.trans_to_nindex(click_docs), self.args.user_log_length)\n",
        "        user_feature = self.news_scoring[click_docs]\n",
        "\n",
        "        candidate_news = self.trans_to_nindex([i.split('-')[0] for i in line[4].split()])\n",
        "        labels = np.array([int(i.split('-')[1]) for i in line[4].split()])\n",
        "        news_feature = self.news_scoring[candidate_news]\n",
        "\n",
        "        return user_feature, log_mask, news_feature, labels\n",
        "\n",
        "    def __iter__(self):\n",
        "        file_iter = open(self.filename)\n",
        "        return map(self.line_mapper, file_iter)\n",
        "\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1IjisEE-LHb"
      },
      "source": [
        "**Metric.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ppd8qdG1-Nr3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def dcg_score(y_true, y_score, k=10):\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "    gains = 2**y_true - 1\n",
        "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
        "    return np.sum(gains / discounts)\n",
        "\n",
        "\n",
        "def ndcg_score(y_true, y_score, k=10):\n",
        "    best = dcg_score(y_true, y_true, k)\n",
        "    actual = dcg_score(y_true, y_score, k)\n",
        "    return actual / best\n",
        "\n",
        "\n",
        "def mrr_score(y_true, y_score):\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order)\n",
        "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
        "    return np.sum(rr_score) / np.sum(y_true)\n",
        "\n",
        "\n",
        "def ctr_score(y_true, y_score, k=1):\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "    return np.mean(y_true)\n",
        "\n",
        "def acc(y_true, y_hat):\n",
        "    y_hat = torch.argmax(y_hat, dim=-1)\n",
        "    tot = y_true.shape[0]\n",
        "    hit = torch.sum(y_true == y_hat)\n",
        "    return hit.data.float() * 1.0 / tot\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1ZeoY8x-5Ds"
      },
      "source": [
        "**Ultis.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Dfr-ri4E-s3M"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import argparse\n",
        "import sys\n",
        "\n",
        "def setuplogger():\n",
        "    root = logging.getLogger()\n",
        "    root.setLevel(logging.INFO)\n",
        "    handler = logging.StreamHandler(sys.stdout)\n",
        "    handler.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter(\"[%(levelname)s %(asctime)s] %(message)s\")\n",
        "    handler.setFormatter(formatter)\n",
        "    root.addHandler(handler)\n",
        "\n",
        "\n",
        "def dump_args(args):\n",
        "    for arg in dir(args):\n",
        "        if not arg.startswith(\"_\"):\n",
        "            logging.info(f\"args[{arg}]={getattr(args, arg)}\")\n",
        "\n",
        "def load_matrix(embedding_file_path, word_dict, word_embedding_dim):\n",
        "    embedding_matrix = np.zeros(shape=(len(word_dict) + 1, word_embedding_dim))\n",
        "    have_word = []\n",
        "    if embedding_file_path is not None:\n",
        "        with open(embedding_file_path, 'rb') as f:\n",
        "            while True:\n",
        "                line = f.readline()\n",
        "                if len(line) == 0:\n",
        "                    break\n",
        "                line = line.split()\n",
        "                word = line[0].decode()\n",
        "                if word in word_dict:\n",
        "                    index = word_dict[word]\n",
        "                    tp = [float(x) for x in line[1:]]\n",
        "                    embedding_matrix[index] = np.array(tp)\n",
        "                    have_word.append(word)\n",
        "    return embedding_matrix, have_word\n",
        "\n",
        "\n",
        "def get_checkpoint(directory, ckpt_name):\n",
        "    ckpt_path = os.path.join(directory, ckpt_name)\n",
        "    if os.path.exists(ckpt_path):\n",
        "        return ckpt_path\n",
        "    else:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqNlsklUAi48"
      },
      "source": [
        "**preprocess.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_Ja_pvD0AYwg"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "import json\n",
        "\n",
        "\n",
        "def update_dict(dict, key, value=None):\n",
        "    if key not in dict:\n",
        "        if value is None:\n",
        "            dict[key] = len(dict) + 1\n",
        "        else:\n",
        "            dict[key] = value\n",
        "\n",
        "\n",
        "def read_custom_abstract(news_file, custom_abstract_dict):\n",
        "    news = {}\n",
        "    news_index = {}\n",
        "    category_dict = {}\n",
        "    subcategory_dict = {}\n",
        "    word_cnt = {}\n",
        "\n",
        "    with open(news_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            splited = line.strip('\\n').split('\\t')\n",
        "            doc_id, category, subcategory, title, abstract, url, entity_title, entity_abstract = splited\n",
        "            if doc_id in custom_abstract_dict:\n",
        "                abstract = custom_abstract_dict[doc_id]\n",
        "            news[doc_id] = [title.split(' '), category, subcategory, abstract.split(' ')]\n",
        "            news_index[doc_id] = len(news_index) + 1\n",
        "            for word in title.split(' '):\n",
        "                if word not in word_cnt:\n",
        "                    word_cnt[word] = 0\n",
        "                word_cnt[word] += 1\n",
        "            for word in abstract.split(' '):\n",
        "                if word not in word_cnt:\n",
        "                    word_cnt[word] = 0\n",
        "                word_cnt[word] += 1\n",
        "            if category not in category_dict:\n",
        "                category_dict[category] = len(category_dict) + 1\n",
        "            if subcategory not in subcategory_dict:\n",
        "                subcategory_dict[subcategory] = len(subcategory_dict) + 1\n",
        "\n",
        "    return news, news_index, category_dict, subcategory_dict, word_cnt\n",
        "\n",
        "def read_news(news_path, abstract_path, args, mode='train'):\n",
        "    news = {}\n",
        "    category_dict = {}\n",
        "    subcategory_dict = {}\n",
        "    news_index = {}\n",
        "    word_cnt = Counter()\n",
        "    if args.use_custom_abstract:\n",
        "      with open(abstract_path, 'r') as f:\n",
        "          abs = json.load(f)\n",
        "    with open(news_path, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f):\n",
        "            splited = line.strip('\\n').split('\\t')\n",
        "            doc_id, category, subcategory, title, abstract, url, _, _ = splited\n",
        "            update_dict(news_index, doc_id)\n",
        "\n",
        "            title = title.lower()\n",
        "            title = word_tokenize(title, language='english', preserve_line=True)\n",
        "\n",
        "            update_dict(news, doc_id, [title, category, subcategory, abs[doc_id] if doc_id in abs else abstract])\n",
        "            if mode == 'train':\n",
        "                if args.use_category:\n",
        "                    update_dict(category_dict, category)\n",
        "                if args.use_subcategory:\n",
        "                    update_dict(subcategory_dict, subcategory)\n",
        "                word_cnt.update(title)\n",
        "\n",
        "    if mode == 'train':\n",
        "        word = [k for k, v in word_cnt.items() if v > args.filter_num]\n",
        "        word_dict = {k: v for k, v in zip(word, range(1, len(word) + 1))}\n",
        "        return news, news_index, category_dict, subcategory_dict, word_dict\n",
        "    elif mode == 'test':\n",
        "        return news, news_index\n",
        "    else:\n",
        "        assert False, 'Wrong mode!'\n",
        "\n",
        "\n",
        "def get_doc_input(news, news_index, category_dict, subcategory_dict, word_dict, args):\n",
        "    news_num = len(news) + 1\n",
        "    news_title = np.zeros((news_num, args.num_words_title), dtype='int32')\n",
        "    news_category = np.zeros((news_num, 1), dtype='int32') if args.use_category else None\n",
        "    news_subcategory = np.zeros((news_num, 1), dtype='int32') if args.use_subcategory else None\n",
        "    news_abstract = np.zeros((news_num, args.num_words_abstract), dtype='int32') if args.use_abstract else None\n",
        "\n",
        "    for key in tqdm(news):\n",
        "        title, category, subcategory, abstract = news[key]\n",
        "        doc_index = news_index[key]\n",
        "\n",
        "        for word_id in range(min(args.num_words_title, len(title))):\n",
        "            if title[word_id] in word_dict:\n",
        "                news_title[doc_index, word_id] = word_dict[title[word_id]]\n",
        "\n",
        "        if args.use_category:\n",
        "            news_category[doc_index, 0] = category_dict[category] if category in category_dict else 0\n",
        "        if args.use_subcategory:\n",
        "            news_subcategory[doc_index, 0] = subcategory_dict[subcategory] if subcategory in subcategory_dict else 0\n",
        "        if args.use_abstract:\n",
        "            for word_id in range(min(args.num_words_abstract, len(abstract))):\n",
        "                if abstract[word_id] in word_dict:\n",
        "                    news_abstract[doc_index, word_id] = word_dict[abstract[word_id]]\n",
        "\n",
        "    return news_title, news_category, news_subcategory, news_abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MambLdtFlSxi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zXd8SNmIm34q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu43E3_6AqAA"
      },
      "source": [
        "**prepare_data.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5Qzn9-0kAuFF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import logging\n",
        "\n",
        "\n",
        "def get_sample(all_elements, num_sample):\n",
        "    if num_sample > len(all_elements):\n",
        "        return random.sample(all_elements * (num_sample // len(all_elements) + 1), num_sample)\n",
        "    else:\n",
        "        return random.sample(all_elements, num_sample)\n",
        "\n",
        "\n",
        "def prepare_training_data(train_data_dir, nGPU, npratio, seed):\n",
        "    random.seed(seed)\n",
        "    behaviors = []\n",
        "\n",
        "    behavior_file_path = os.path.join(train_data_dir, 'behaviors.tsv')\n",
        "    with open(behavior_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f):\n",
        "            iid, uid, time, history, imp = line.strip().split('\\t')\n",
        "            impressions = [x.split('-') for x in imp.split(' ')]\n",
        "            pos, neg = [], []\n",
        "            for news_ID, label in impressions:\n",
        "                if label == '0':\n",
        "                    neg.append(news_ID)\n",
        "                elif label == '1':\n",
        "                    pos.append(news_ID)\n",
        "            if len(pos) == 0 or len(neg) == 0:\n",
        "                continue\n",
        "            for pos_id in pos:\n",
        "                neg_candidate = get_sample(neg, npratio)\n",
        "                neg_str = ' '.join(neg_candidate)\n",
        "                new_line = '\\t'.join([iid, uid, time, history, pos_id, neg_str]) + '\\n'\n",
        "                behaviors.append(new_line)\n",
        "\n",
        "    random.shuffle(behaviors)\n",
        "\n",
        "    behaviors_per_file = [[] for _ in range(nGPU)]\n",
        "    for i, line in enumerate(behaviors):\n",
        "        behaviors_per_file[i % nGPU].append(line)\n",
        "\n",
        "    logging.info('Writing files...')\n",
        "    for i in range(nGPU):\n",
        "        processed_file_path = os.path.join(train_data_dir, f'behaviors_np{npratio}_{i}.tsv')\n",
        "        with open(processed_file_path, 'w') as f:\n",
        "            f.writelines(behaviors_per_file[i])\n",
        "\n",
        "    return len(behaviors)\n",
        "\n",
        "\n",
        "def prepare_testing_data(test_data_dir, nGPU):\n",
        "    behaviors = [[] for _ in range(nGPU)]\n",
        "\n",
        "    behavior_file_path = os.path.join(test_data_dir, 'behaviors.tsv')\n",
        "    with open(behavior_file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(tqdm(f)):\n",
        "            behaviors[i % nGPU].append(line)\n",
        "\n",
        "    logging.info('Writing files...')\n",
        "    for i in range(nGPU):\n",
        "        processed_file_path = os.path.join(test_data_dir, f'behaviors_{i}.tsv')\n",
        "        with open(processed_file_path, 'w') as f:\n",
        "            f.writelines(behaviors[i])\n",
        "\n",
        "    return sum([len(x) for x in behaviors])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VGYXTH1pMIBO"
      },
      "outputs": [],
      "source": [
        "def train(rank, args):\n",
        "\n",
        "    is_distributed = False\n",
        "    torch.cuda.set_device(rank)\n",
        "\n",
        "\n",
        "    news, news_index, category_dict, subcategory_dict, word_dict = read_news(\n",
        "        os.path.join(args.train_data_dir, 'news.tsv'), args.train_abstract_dir, args, mode='train')\n",
        "\n",
        "    news_title, news_category, news_subcategory, news_abstract = get_doc_input(\n",
        "        news, news_index, category_dict, subcategory_dict, word_dict, args)\n",
        "    news_combined = np.concatenate([x for x in [news_title, news_category, news_subcategory, news_abstract] if x is not None], axis=-1)\n",
        "\n",
        "    if rank == 0:\n",
        "        logging.info('Initializing word embedding matrix...')\n",
        "\n",
        "    embedding_matrix, have_word = load_matrix(args.glove_embedding_path,\n",
        "                                                    word_dict,\n",
        "                                                    args.word_embedding_dim)\n",
        "    if rank == 0:\n",
        "        logging.info(f'Word dict length: {len(word_dict)}')\n",
        "        logging.info(f'Have words: {len(have_word)}')\n",
        "        logging.info(f'Missing rate: {(len(word_dict) - len(have_word)) / len(word_dict)}')\n",
        "\n",
        "    model = Model(args, embedding_matrix, len(category_dict), len(subcategory_dict))\n",
        "\n",
        "    if args.load_ckpt_name is not None:\n",
        "        ckpt_path = get_checkpoint(args.model_dir, args.load_ckpt_name)\n",
        "        checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        logging.info(f\"Model loaded from {ckpt_path}.\")\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    if args.enable_gpu:\n",
        "        model = model.cuda(rank)\n",
        "\n",
        "    if is_distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n",
        "\n",
        "    # if rank == 0:\n",
        "    #     print(model)\n",
        "    #     for name, param in model.named_parameters():\n",
        "    #         print(name, param.requires_grad)\n",
        "\n",
        "    data_file_path = os.path.join(args.train_data_dir, f'behaviors_np{args.npratio}_{rank}.tsv')\n",
        "\n",
        "    dataset = DatasetTrain(data_file_path, news_index, news_combined, args)\n",
        "    dataloader = DataLoader(dataset, batch_size=args.batch_size)\n",
        "\n",
        "    logging.info('Training...')\n",
        "    for ep in range(args.start_epoch, args.epochs):\n",
        "        loss = 0.0\n",
        "        accuary = 0.0\n",
        "        for cnt, (log_ids, log_mask, input_ids, targets) in enumerate(dataloader):\n",
        "            if args.enable_gpu:\n",
        "                log_ids = log_ids.cuda(rank, non_blocking=True)\n",
        "                log_mask = log_mask.cuda(rank, non_blocking=True)\n",
        "                input_ids = input_ids.cuda(rank, non_blocking=True)\n",
        "                targets = targets.cuda(rank, non_blocking=True)\n",
        "\n",
        "            bz_loss, y_hat = model(log_ids, log_mask, input_ids, targets)\n",
        "            loss += bz_loss.data.float()\n",
        "            accuary += acc(targets, y_hat)\n",
        "            optimizer.zero_grad()\n",
        "            bz_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if cnt % args.log_steps == 0:\n",
        "                logging.info(\n",
        "                    '[{}] Ed: {}, train_loss: {:.5f}, acc: {:.5f}'.format(\n",
        "                        rank, cnt * args.batch_size, loss.data / cnt, accuary / cnt)\n",
        "                )\n",
        "\n",
        "            if rank == 0 and     cnt != 0 and cnt % args.save_steps == 0:\n",
        "                ckpt_path = os.path.join(args.model_dir, f'epoch-{ep+1}-{cnt}.pt')\n",
        "                torch.save(\n",
        "                    {\n",
        "                        'model_state_dict':\n",
        "                            {'.'.join(k.split('.')[1:]): v for k, v in model.state_dict().items()}\n",
        "                            if is_distributed else model.state_dict(),\n",
        "                        'category_dict': category_dict,\n",
        "                        'word_dict': word_dict,\n",
        "                        'subcategory_dict': subcategory_dict\n",
        "                    }, ckpt_path)\n",
        "                logging.info(f\"Model saved to {ckpt_path}.\")\n",
        "\n",
        "        logging.info('Training finish.')\n",
        "\n",
        "        if rank == 0:\n",
        "            ckpt_path = os.path.join(args.model_dir, f'epoch-{ep+1}.pt')\n",
        "            torch.save(\n",
        "                {\n",
        "                    'model_state_dict':\n",
        "                        {'.'.join(k.split('.')[1:]): v for k, v in model.state_dict().items()}\n",
        "                        if is_distributed else model.state_dict(),\n",
        "                    'category_dict': category_dict,\n",
        "                    'subcategory_dict': subcategory_dict,\n",
        "                    'word_dict': word_dict,\n",
        "                }, ckpt_path)\n",
        "            logging.info(f\"Model saved to {ckpt_path}.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgkOVqFzS6Uv",
        "outputId": "21c5d676-d2dc-4c6d-b2d0-7116daa3bcfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2025-02-25 00:16:01,242] args[batch_size]=32\n",
            "[INFO 2025-02-25 00:16:01,244] args[category_emb_dim]=100\n",
            "[INFO 2025-02-25 00:16:01,245] args[drop_rate]=0.2\n",
            "[INFO 2025-02-25 00:16:01,246] args[enable_gpu]=False\n",
            "[INFO 2025-02-25 00:16:01,247] args[epochs]=5\n",
            "[INFO 2025-02-25 00:16:01,248] args[filter_num]=3\n",
            "[INFO 2025-02-25 00:16:01,249] args[freeze_embedding]=False\n",
            "[INFO 2025-02-25 00:16:01,251] args[glove_embedding_path]=/home/vinmike/Downloads/glove.840B.300d.txt\n",
            "[INFO 2025-02-25 00:16:01,252] args[load_ckpt_name]=None\n",
            "[INFO 2025-02-25 00:16:01,253] args[log_steps]=100\n",
            "[INFO 2025-02-25 00:16:01,254] args[lr]=0.0003\n",
            "[INFO 2025-02-25 00:16:01,255] args[mode]=train\n",
            "[INFO 2025-02-25 00:16:01,255] args[model_dir]=/content/model\n",
            "[INFO 2025-02-25 00:16:01,256] args[nGPU]=1\n",
            "[INFO 2025-02-25 00:16:01,257] args[news_dim]=400\n",
            "[INFO 2025-02-25 00:16:01,258] args[news_query_vector_dim]=200\n",
            "[INFO 2025-02-25 00:16:01,258] args[npratio]=4\n",
            "[INFO 2025-02-25 00:16:01,259] args[num_attention_heads]=15\n",
            "[INFO 2025-02-25 00:16:01,259] args[num_words_abstract]=50\n",
            "[INFO 2025-02-25 00:16:01,260] args[num_words_title]=20\n",
            "[INFO 2025-02-25 00:16:01,260] args[prepare]=True\n",
            "[INFO 2025-02-25 00:16:01,261] args[save_steps]=10000\n",
            "[INFO 2025-02-25 00:16:01,261] args[seed]=0\n",
            "[INFO 2025-02-25 00:16:01,262] args[start_epoch]=0\n",
            "[INFO 2025-02-25 00:16:01,263] args[test_abstract_dir]=/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/genAbs0.json\n",
            "[INFO 2025-02-25 00:16:01,263] args[test_data_dir]=/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/MINDsmall_dev\n",
            "[INFO 2025-02-25 00:16:01,264] args[train_abstract_dir]=/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/genAbs0.json\n",
            "[INFO 2025-02-25 00:16:01,264] args[train_data_dir]=/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/MINDsmall_train\n",
            "[INFO 2025-02-25 00:16:01,265] args[use_abstract]=True\n",
            "[INFO 2025-02-25 00:16:01,265] args[use_category]=True\n",
            "[INFO 2025-02-25 00:16:01,265] args[use_custom_abstract]=True\n",
            "[INFO 2025-02-25 00:16:01,266] args[use_subcategory]=True\n",
            "[INFO 2025-02-25 00:16:01,266] args[user_log_length]=50\n",
            "[INFO 2025-02-25 00:16:01,267] args[user_log_mask]=False\n",
            "[INFO 2025-02-25 00:16:01,267] args[user_query_vector_dim]=200\n",
            "[INFO 2025-02-25 00:16:01,268] args[word_embedding_dim]=300\n"
          ]
        }
      ],
      "source": [
        "\n",
        "    import subprocess\n",
        "    setuplogger()\n",
        "    args = parse_args()\n",
        "    dump_args(args)\n",
        "    random.seed(args.seed)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5694OJnDLmEV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkoY3tHH6in5"
      },
      "source": [
        "# **NRMS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KXeVVGPl6sgG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class DotProductClickPredictor(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DotProductClickPredictor, self).__init__()\n",
        "\n",
        "    def forward(self, candidate_news_vector, user_vector):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            candidate_news_vector: batch_size, candidate_size, X\n",
        "            user_vector: batch_size, X\n",
        "        Returns:\n",
        "            (shape): batch_size\n",
        "        \"\"\"\n",
        "        # batch_size, candidate_size\n",
        "        probability = torch.bmm(candidate_news_vector,\n",
        "                                user_vector.unsqueeze(dim=-1)).squeeze(dim=-1)\n",
        "        return probability\n",
        "class AdditiveAttention(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A general additive attention module.\n",
        "    Originally for NAML.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 query_vector_dim,\n",
        "                 candidate_vector_dim,\n",
        "                 writer=None,\n",
        "                 tag=None,\n",
        "                 names=None):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "        self.linear = nn.Linear(candidate_vector_dim, query_vector_dim)\n",
        "        self.attention_query_vector = nn.Parameter(\n",
        "            torch.empty(query_vector_dim).uniform_(-0.1, 0.1))\n",
        "        # For tensorboard\n",
        "        self.writer = writer\n",
        "        self.tag = tag\n",
        "        self.names = names\n",
        "        self.local_step = 1\n",
        "\n",
        "    def forward(self, candidate_vector):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            candidate_vector: batch_size, candidate_size, candidate_vector_dim\n",
        "        Returns:\n",
        "            (shape) batch_size, candidate_vector_dim\n",
        "        \"\"\"\n",
        "        # batch_size, candidate_size, query_vector_dim\n",
        "        temp = torch.tanh(self.linear(candidate_vector))\n",
        "        # batch_size, candidate_size\n",
        "        candidate_weights = F.softmax(torch.matmul(\n",
        "            temp, self.attention_query_vector),\n",
        "                                      dim=1)\n",
        "        if self.writer is not None:\n",
        "            assert candidate_weights.size(1) == len(self.names)\n",
        "            if self.local_step % 10 == 0:\n",
        "                self.writer.add_scalars(\n",
        "                    self.tag, {\n",
        "                        x: y\n",
        "                        for x, y in zip(self.names,\n",
        "                                        candidate_weights.mean(dim=0))\n",
        "                    }, self.local_step)\n",
        "            self.local_step += 1\n",
        "        # batch_size, candidate_vector_dim\n",
        "        target = torch.bmm(candidate_weights.unsqueeze(dim=1),\n",
        "                           candidate_vector).squeeze(dim=1)\n",
        "        return target\n",
        "    \n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.d_k = d_k\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask=None):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k)\n",
        "        scores = torch.exp(scores)\n",
        "        if attn_mask is not None:\n",
        "            scores = scores * attn_mask\n",
        "        attn = scores / (torch.sum(scores, dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_attention_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        assert d_model % num_attention_heads == 0\n",
        "        self.d_k = d_model // num_attention_heads\n",
        "        self.d_v = d_model // num_attention_heads\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
        "\n",
        "    def forward(self, Q, K=None, V=None, length=None):\n",
        "        if K is None:\n",
        "            K = Q\n",
        "        if V is None:\n",
        "            V = Q\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.num_attention_heads,\n",
        "                               self.d_k).transpose(1, 2)\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.num_attention_heads,\n",
        "                               self.d_k).transpose(1, 2)\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.num_attention_heads,\n",
        "                               self.d_v).transpose(1, 2)\n",
        "\n",
        "        if length is not None:\n",
        "            maxlen = Q.size(1)\n",
        "            attn_mask = torch.arange(maxlen).expand(\n",
        "                batch_size, maxlen) < length.view(-1, 1)\n",
        "            attn_mask = attn_mask.unsqueeze(1).expand(batch_size, maxlen,\n",
        "                                                      maxlen)\n",
        "            attn_mask = attn_mask.unsqueeze(1).repeat(1,\n",
        "                                                      self.num_attention_heads,\n",
        "                                                      1, 1)\n",
        "        else:\n",
        "            attn_mask = None\n",
        "\n",
        "        context, attn = ScaledDotProductAttention(self.d_k)(q_s, k_s, v_s,\n",
        "                                                            attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(\n",
        "            batch_size, -1, self.num_attention_heads * self.d_v)\n",
        "        return context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "l3W3JjCZ6k1a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class NewsEncoder(nn.Module):\n",
        "    def __init__(self, args, embedding_matrix):\n",
        "        super(NewsEncoder, self).__init__()\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "        self.drop_rate = args.drop_rate\n",
        "        self.dim_per_head = args.news_dim // args.num_attention_heads\n",
        "        self.multi_head_self_attn = MultiHeadSelfAttention(\n",
        "            args.word_embedding_dim,\n",
        "            args.num_attention_heads,\n",
        "            # self.dim_per_head,\n",
        "            # self.dim_per_head\n",
        "        )\n",
        "        self.attn = AdditiveAttention(args.news_query_vector_dim, args.word_embedding_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        '''\n",
        "            x: batch_size, word_num\n",
        "            mask: batch_size, word_num\n",
        "        '''\n",
        "        word_vecs = F.dropout(self.embedding_matrix(x.long()),\n",
        "                              p=self.drop_rate,\n",
        "                              training=self.training)\n",
        "        multihead_text_vecs = self.multi_head_self_attn(word_vecs, word_vecs, word_vecs, mask)\n",
        "        multihead_text_vecs = F.dropout(multihead_text_vecs,\n",
        "                                        p=self.drop_rate,\n",
        "                                        training=self.training)\n",
        "        news_vec = self.attn(multihead_text_vecs)\n",
        "        return news_vec\n",
        "\n",
        "\n",
        "class UserEncoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(UserEncoder, self).__init__()\n",
        "        self.args = args\n",
        "        self.dim_per_head = args.news_dim // args.num_attention_heads\n",
        "        self.multi_head_self_attn = MultiHeadSelfAttention(args.word_embedding_dim, args.num_attention_heads)\n",
        "        self.attn = AdditiveAttention(args.user_query_vector_dim, args.word_embedding_dim)\n",
        "        self.pad_doc = nn.Parameter(torch.empty(1, args.news_dim).uniform_(-1, 1)).type(torch.FloatTensor)\n",
        "\n",
        "    def forward(self, user_vector):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            user_vector: batch_size, num_clicked_news_a_user, word_embedding_dim\n",
        "        Returns:\n",
        "            (shape) batch_size, word_embedding_dim\n",
        "        \"\"\"\n",
        "        # batch_size, num_clicked_news_a_user, word_embedding_dim\n",
        "        multihead_user_vector = self.multi_head_self_attn(user_vector)\n",
        "        # batch_size, word_embedding_dim\n",
        "        final_user_vector = self.attn(multihead_user_vector)\n",
        "        return final_user_vector\n",
        "    \n",
        "\n",
        "\n",
        "class NRMS(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    NRMS network.\n",
        "    Input 1 + K candidate news and a list of user clicked news, produce the click probability.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, embedding_matrix):\n",
        "        super(NRMS, self).__init__()\n",
        "        self.config = config\n",
        "        word_embedding = torch.from_numpy(embedding_matrix).float()\n",
        "        pretrained_word_embedding = nn.Embedding.from_pretrained(word_embedding,\n",
        "                                                      freeze=args.freeze_embedding,\n",
        "                                                      padding_idx=0)\n",
        " \n",
        "        self.news_encoder = NewsEncoder(config, pretrained_word_embedding)\n",
        "        self.user_encoder = UserEncoder(config)\n",
        "        self.click_predictor = DotProductClickPredictor()\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, clicked_news, candidate_news, label):\n",
        "        \n",
        "        # batch_size, 1 + K, word_embedding_dim\n",
        "        candidate_news_vector = torch.stack(\n",
        "            [self.news_encoder(x) for x in candidate_news])\n",
        "        \n",
        "        # batch_size, num_clicked_news_a_user, word_embedding_dim\n",
        "        clicked_news_vector = torch.stack(\n",
        "            [self.news_encoder(x) for x in clicked_news])\n",
        "        \n",
        "        # batch_size, word_embedding_dim\n",
        "        user_vector = self.user_encoder(clicked_news_vector)\n",
        "\n",
        "        # batch_size, 1 + K\n",
        "        click_probability = self.click_predictor(candidate_news_vector,\n",
        "                                                 user_vector)\n",
        "        loss = self.loss_fn(click_probability, label)\n",
        "        # loss = 0.5\n",
        "        return loss, click_probability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OZngQdLK9Xfa"
      },
      "outputs": [],
      "source": [
        "args.mode = 'train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ms-iPzD63S-",
        "outputId": "d88b5bd8-a142-4980-e8ce-f760e6068a8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2025-02-25 00:16:04,062] Preparing training data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "156965it [00:02, 72944.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2025-02-25 00:16:06,431] Writing files...\n",
            "[INFO 2025-02-25 00:16:06,731] 236344 training samples, 7385 batches in total.\n"
          ]
        }
      ],
      "source": [
        "if 'train' in args.mode:\n",
        "    if args.prepare:\n",
        "        logging.info('Preparing training data...')\n",
        "        total_sample_num = prepare_training_data(args.train_data_dir, args.nGPU, args.npratio, args.seed)\n",
        "    else:\n",
        "        total_sample_num = 0\n",
        "        for i in range(args.nGPU):\n",
        "            data_file_path = os.path.join(args.train_data_dir, f'behaviors_np{args.npratio}_{i}.tsv')\n",
        "            print(data_file_path)\n",
        "            if not os.path.exists(data_file_path):\n",
        "                logging.error(f'Splited training data {data_file_path} for GPU {i} does not exist. Please set the parameter --prepare as True and rerun the code.')\n",
        "                exit()\n",
        "            result = subprocess.getoutput(f'wc -l {data_file_path}')\n",
        "            total_sample_num += int(result.split(' ')[0])\n",
        "        logging.info('Skip training data preparation.')\n",
        "    logging.info(f'{total_sample_num} training samples, {total_sample_num // args.batch_size // args.nGPU} batches in total.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "51282it [00:03, 15300.76it/s]\n",
            "100%|██████████| 51282/51282 [00:00<00:00, 198057.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2025-02-25 00:16:11,195] Initializing word embedding matrix...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2025-02-25 00:16:44,853] Word dict length: 12519\n",
            "[INFO 2025-02-25 00:16:44,854] Have words: 11960\n",
            "[INFO 2025-02-25 00:16:44,858] Missing rate: 0.0446521287642783\n"
          ]
        }
      ],
      "source": [
        "rank = 0\n",
        "news, news_index, category_dict, subcategory_dict, word_dict = read_news(\n",
        "\t\tos.path.join(args.train_data_dir, 'news.tsv'), args.train_abstract_dir, args, mode='train')\n",
        "\n",
        "news_title, news_category, news_subcategory, news_abstract = get_doc_input(\n",
        "    news, news_index, category_dict, subcategory_dict, word_dict, args)\n",
        "news_combined = np.concatenate([x for x in [news_abstract] if x is not None], axis=-1)\n",
        "\n",
        "if rank == 0:\n",
        "    logging.info('Initializing word embedding matrix...')\n",
        "\n",
        "embedding_matrix, have_word = load_matrix(args.glove_embedding_path,\n",
        "                                                word_dict,\n",
        "                                                args.word_embedding_dim)\n",
        "if rank == 0:\n",
        "    logging.info(f'Word dict length: {len(word_dict)}')\n",
        "    logging.info(f'Have words: {len(have_word)}')\n",
        "    logging.info(f'Missing rate: {(len(word_dict) - len(have_word)) / len(word_dict)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qt4Qsf_r9RA_",
        "outputId": "f54e7ea2-1df5-49fc-d9af-75d422e45144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2025-02-25 00:22:59,748] Training...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2025-02-25 00:23:05,195] [0] Ed: 0, train_loss: inf, acc: nan\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = NRMS(args, embedding_matrix)\n",
        "is_distributed = False\n",
        "if args.load_ckpt_name is not None:\n",
        "\tckpt_path = get_checkpoint(args.model_dir, args.load_ckpt_name)\n",
        "\tcheckpoint = torch.load(ckpt_path, map_location='cpu')\n",
        "\tmodel.load_state_dict(checkpoint['model_state_dict'])\n",
        "\tlogging.info(f\"Model loaded from {ckpt_path}.\")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "if args.enable_gpu:\n",
        "\tmodel = model.cuda(rank)\n",
        "\n",
        "if is_distributed:\n",
        "\tmodel = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n",
        "\n",
        "data_file_path = os.path.join(args.train_data_dir, f'behaviors_np{args.npratio}_{rank}.tsv')\n",
        "\n",
        "dataset = DatasetTrain(data_file_path, news_index, news_combined, args)\n",
        "dataloader = DataLoader(dataset, batch_size=args.batch_size)\n",
        "\n",
        "logging.info('Training...')\n",
        "for ep in range(args.start_epoch, args.epochs):\n",
        "\tloss = 0.0\n",
        "\taccuary = 0.0\n",
        "\tfor cnt, (log_ids, input_ids, targets) in enumerate(dataloader):\n",
        "\t\tif args.enable_gpu:\n",
        "\t\t\tlog_ids = log_ids.cuda(rank, non_blocking=True)\n",
        "\t\t\t# log_mask = log_mask.cuda(rank, non_blocking=True)\n",
        "\t\t\tinput_ids = input_ids.cuda(rank, non_blocking=True)\n",
        "\t\t\ttargets = targets.cuda(rank, non_blocking=True)\n",
        "\t\t\t\n",
        "\t\t\n",
        "\t\tbz_loss, y_hat = model(log_ids, input_ids, targets)\n",
        "\t\tloss += bz_loss.data.float()\n",
        "\t\taccuary += acc(targets, y_hat)\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tbz_loss.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\tif cnt % args.log_steps == 0:\n",
        "\t\t\tlogging.info(\n",
        "\t\t\t\t'[{}] Ed: {}, train_loss: {:.5f}, acc: {:.5f}'.format(\n",
        "\t\t\t\t\trank, cnt * args.batch_size, loss.data / cnt, accuary / cnt)\n",
        "\t\t\t)\n",
        "\n",
        "\t\tif rank == 0 and cnt != 0 and cnt % args.save_steps == 0:\n",
        "\t\t\tckpt_path = os.path.join(args.model_dir, f'epoch-{ep+1}-{cnt}.pt')\n",
        "\t\t\ttorch.save(\n",
        "\t\t\t\t{\n",
        "\t\t\t\t\t'model_state_dict':\n",
        "\t\t\t\t\t\t{'.'.join(k.split('.')[1:]): v for k, v in model.state_dict().items()}\n",
        "\t\t\t\t\t\tif is_distributed else model.state_dict(),\n",
        "\t\t\t\t\t'category_dict': category_dict,\n",
        "\t\t\t\t\t'word_dict': word_dict,\n",
        "\t\t\t\t\t'subcategory_dict': subcategory_dict\n",
        "\t\t\t\t}, ckpt_path)\n",
        "\t\t\tlogging.info(f\"Model saved to {ckpt_path}.\")\n",
        "\n",
        "\tlogging.info('Training finish.')\n",
        "\n",
        "\tif rank == 0:\n",
        "\t\tckpt_path = os.path.join(args.model_dir, f'epoch-{ep+1}.pt')\n",
        "\t\ttorch.save(\n",
        "\t\t\t{\n",
        "\t\t\t\t'model_state_dict':\n",
        "\t\t\t\t\t{'.'.join(k.split('.')[1:]): v for k, v in model.state_dict().items()}\n",
        "\t\t\t\t\tif is_distributed else model.state_dict(),\n",
        "\t\t\t\t'category_dict': category_dict,\n",
        "\t\t\t\t'subcategory_dict': subcategory_dict,\n",
        "\t\t\t\t'word_dict': word_dict,\n",
        "\t\t\t}, ckpt_path)\n",
        "\t\tlogging.info(f\"Model saved to {ckpt_path}.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um0H4NwW69yY"
      },
      "outputs": [],
      "source": [
        "args.mode = 'test'\n",
        "args.user_log_mask=True\n",
        "args.batch_size=128\n",
        "args.load_ckpt_name= 'epoch-5.pt'\n",
        "args.prepare=True\n",
        "if 'test' in args.mode:\n",
        "        if args.prepare:\n",
        "            logging.info('Preparing testing data...')\n",
        "            total_sample_num = prepare_testing_data(args.test_data_dir, args.nGPU)\n",
        "        else:\n",
        "            total_sample_num = 0\n",
        "            for i in range(args.nGPU):\n",
        "                data_file_path = os.path.join(args.test_data_dir, f'behaviors_{i}.tsv')\n",
        "                if not os.path.exists(data_file_path):\n",
        "                    logging.error(f'Splited testing data {data_file_path} for GPU {i} does not exist. Please set the parameter --prepare as True and rerun the code.')\n",
        "                    exit()\n",
        "                result = subprocess.getoutput(f'wc -l {data_file_path}')\n",
        "                total_sample_num += int(result.split(' ')[0])\n",
        "            logging.info('Skip testing data preparation.')\n",
        "        logging.info(f'{total_sample_num} testing samples in total.')\n",
        "\n",
        "        test(0, args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soI3jlXBCfD9"
      },
      "source": [
        "# Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abHIlnIvCgsI",
        "outputId": "3803e4e4-1391-4da6-d1c6-3d2d301823f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label = random.randint(0, 4)\n",
        "label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrDpWw36CiCe",
        "outputId": "69073a5d-0155-4f19-c263-b088af183c82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-02-16 04:36:04--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
            "--2025-02-16 04:36:04--  https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  4.99MB/s    in 6m 49s  \n",
            "\n",
            "2025-02-16 04:42:54 (5.07 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
            "\n",
            "Archive:  glove.840B.300d.zip\n",
            "  inflating: glove.840B.300d.txt     \n",
            "--2025-02-16 04:43:48--  https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip\n",
            "Resolving mind201910small.blob.core.windows.net (mind201910small.blob.core.windows.net)... 20.150.34.36\n",
            "Connecting to mind201910small.blob.core.windows.net (mind201910small.blob.core.windows.net)|20.150.34.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 409 Public access is not permitted on this storage account.\n",
            "2025-02-16 04:43:48 ERROR 409: Public access is not permitted on this storage account..\n",
            "\n",
            "--2025-02-16 04:43:48--  https://mind201910small.blob.core.windows.net/release/MINDsmall_dev.zip\n",
            "Resolving mind201910small.blob.core.windows.net (mind201910small.blob.core.windows.net)... 20.150.34.36\n",
            "Connecting to mind201910small.blob.core.windows.net (mind201910small.blob.core.windows.net)|20.150.34.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 409 Public access is not permitted on this storage account.\n",
            "2025-02-16 04:43:48 ERROR 409: Public access is not permitted on this storage account..\n",
            "\n",
            "unzip:  cannot find or open MINDsmall_train.zip, MINDsmall_train.zip.zip or MINDsmall_train.zip.ZIP.\n",
            "unzip:  cannot find or open MINDsmall_dev.zip, MINDsmall_dev.zip.zip or MINDsmall_dev.zip.ZIP.\n",
            "rm: cannot remove 'MINDsmall_train.zip': No such file or directory\n",
            "rm: cannot remove 'MINDsmall_dev.zip': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!cd data\n",
        "\n",
        "# Dowload GloVe pre-trained word embedding and unzip\n",
        "!wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "!unzip glove.840B.300d.zip\n",
        "\n",
        "# Download MIND-small dataset and unzip\n",
        "!wget https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip\n",
        "!wget https://mind201910small.blob.core.windows.net/release/MINDsmall_dev.zip\n",
        "!unzip MINDsmall_train.zip -d MINDsmall_train\n",
        "!unzip MINDsmall_dev.zip -d MINDsmall_dev\n",
        "\n",
        "!rm glove.840B.300d.zip\n",
        "!rm MINDsmall_train.zip\n",
        "!rm MINDsmall_dev.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU3UxmipCje4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
