{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0bmeyydYVDX"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g0U2AfMZrhW",
        "outputId": "4aeb46e5-abfe-4bef-acbf-186517fae21f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpVwnUaQMkjX",
        "outputId": "fd085245-8e67-4639-bcee-bb6697a114fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘model’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "E7i79ErILbl8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "import logging\n",
        "import os\n",
        "import torch.optim as optim\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WxUXYhf2MjaM"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "@dataclass\n",
        "class Args:\n",
        "    nGPU: int = 1\n",
        "    seed: int = 0\n",
        "    prepare: bool = True\n",
        "    mode: str = \"train\"\n",
        "    train_data_dir: str = \"/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_train\"\n",
        "    test_data_dir: str = \"/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_dev\"\n",
        "    model_dir: str = '/content/model'\n",
        "    batch_size: int = 32\n",
        "    npratio: int = 4\n",
        "    enable_gpu: bool = True\n",
        "    filter_num: int = 3\n",
        "    log_steps: int = 100\n",
        "    epochs: int = 5\n",
        "    lr: float = 0.0003\n",
        "    num_words_title: int = 20\n",
        "    num_words_abstract: int = 50\n",
        "    user_log_length: int = 50\n",
        "    word_embedding_dim: int = 300\n",
        "    glove_embedding_path: str = '/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/glove.840B.300d.txt'\n",
        "    freeze_embedding: bool = False\n",
        "    news_dim: int = 400\n",
        "    news_query_vector_dim: int = 200\n",
        "    user_query_vector_dim: int = 200\n",
        "    num_attention_heads: int = 20\n",
        "    user_log_mask: bool = False\n",
        "    drop_rate: float = 0.2\n",
        "    save_steps: int = 10000\n",
        "    start_epoch: int = 0\n",
        "    load_ckpt_name: Optional[str] = None\n",
        "    use_category: bool = True\n",
        "    use_subcategory: bool = True\n",
        "    category_emb_dim: int = 100\n",
        "\n",
        "def parse_args():\n",
        "  return Args()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hQ793p9QQyy2",
        "outputId": "6458e01e-d070-412f-cdb1-749c8f39c8b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[batch_size]=32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,561] args[batch_size]=32\n",
            "[INFO 2024-12-03 17:03:54,561] args[batch_size]=32\n",
            "[INFO 2024-12-03 17:03:54,561] args[batch_size]=32\n",
            "[INFO 2024-12-03 17:03:54,561] args[batch_size]=32\n",
            "[INFO 2024-12-03 17:03:54,561] args[batch_size]=32\n",
            "[INFO 2024-12-03 17:03:54,561] args[batch_size]=32\n",
            "[INFO 2024-12-03 17:03:54,561] args[batch_size]=32\n",
            "[INFO 2024-12-03 17:03:54,561] args[batch_size]=32\n",
            "[INFO 2024-12-03 17:03:54,561] args[batch_size]=32\n",
            "[INFO 2024-12-03 17:03:54,561] args[batch_size]=32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[category_emb_dim]=100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,579] args[category_emb_dim]=100\n",
            "[INFO 2024-12-03 17:03:54,579] args[category_emb_dim]=100\n",
            "[INFO 2024-12-03 17:03:54,579] args[category_emb_dim]=100\n",
            "[INFO 2024-12-03 17:03:54,579] args[category_emb_dim]=100\n",
            "[INFO 2024-12-03 17:03:54,579] args[category_emb_dim]=100\n",
            "[INFO 2024-12-03 17:03:54,579] args[category_emb_dim]=100\n",
            "[INFO 2024-12-03 17:03:54,579] args[category_emb_dim]=100\n",
            "[INFO 2024-12-03 17:03:54,579] args[category_emb_dim]=100\n",
            "[INFO 2024-12-03 17:03:54,579] args[category_emb_dim]=100\n",
            "[INFO 2024-12-03 17:03:54,579] args[category_emb_dim]=100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[drop_rate]=0.2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,590] args[drop_rate]=0.2\n",
            "[INFO 2024-12-03 17:03:54,590] args[drop_rate]=0.2\n",
            "[INFO 2024-12-03 17:03:54,590] args[drop_rate]=0.2\n",
            "[INFO 2024-12-03 17:03:54,590] args[drop_rate]=0.2\n",
            "[INFO 2024-12-03 17:03:54,590] args[drop_rate]=0.2\n",
            "[INFO 2024-12-03 17:03:54,590] args[drop_rate]=0.2\n",
            "[INFO 2024-12-03 17:03:54,590] args[drop_rate]=0.2\n",
            "[INFO 2024-12-03 17:03:54,590] args[drop_rate]=0.2\n",
            "[INFO 2024-12-03 17:03:54,590] args[drop_rate]=0.2\n",
            "[INFO 2024-12-03 17:03:54,590] args[drop_rate]=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[enable_gpu]=True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,602] args[enable_gpu]=True\n",
            "[INFO 2024-12-03 17:03:54,602] args[enable_gpu]=True\n",
            "[INFO 2024-12-03 17:03:54,602] args[enable_gpu]=True\n",
            "[INFO 2024-12-03 17:03:54,602] args[enable_gpu]=True\n",
            "[INFO 2024-12-03 17:03:54,602] args[enable_gpu]=True\n",
            "[INFO 2024-12-03 17:03:54,602] args[enable_gpu]=True\n",
            "[INFO 2024-12-03 17:03:54,602] args[enable_gpu]=True\n",
            "[INFO 2024-12-03 17:03:54,602] args[enable_gpu]=True\n",
            "[INFO 2024-12-03 17:03:54,602] args[enable_gpu]=True\n",
            "[INFO 2024-12-03 17:03:54,602] args[enable_gpu]=True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[epochs]=5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,612] args[epochs]=5\n",
            "[INFO 2024-12-03 17:03:54,612] args[epochs]=5\n",
            "[INFO 2024-12-03 17:03:54,612] args[epochs]=5\n",
            "[INFO 2024-12-03 17:03:54,612] args[epochs]=5\n",
            "[INFO 2024-12-03 17:03:54,612] args[epochs]=5\n",
            "[INFO 2024-12-03 17:03:54,612] args[epochs]=5\n",
            "[INFO 2024-12-03 17:03:54,612] args[epochs]=5\n",
            "[INFO 2024-12-03 17:03:54,612] args[epochs]=5\n",
            "[INFO 2024-12-03 17:03:54,612] args[epochs]=5\n",
            "[INFO 2024-12-03 17:03:54,612] args[epochs]=5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[filter_num]=3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,623] args[filter_num]=3\n",
            "[INFO 2024-12-03 17:03:54,623] args[filter_num]=3\n",
            "[INFO 2024-12-03 17:03:54,623] args[filter_num]=3\n",
            "[INFO 2024-12-03 17:03:54,623] args[filter_num]=3\n",
            "[INFO 2024-12-03 17:03:54,623] args[filter_num]=3\n",
            "[INFO 2024-12-03 17:03:54,623] args[filter_num]=3\n",
            "[INFO 2024-12-03 17:03:54,623] args[filter_num]=3\n",
            "[INFO 2024-12-03 17:03:54,623] args[filter_num]=3\n",
            "[INFO 2024-12-03 17:03:54,623] args[filter_num]=3\n",
            "[INFO 2024-12-03 17:03:54,623] args[filter_num]=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[freeze_embedding]=False\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,633] args[freeze_embedding]=False\n",
            "[INFO 2024-12-03 17:03:54,633] args[freeze_embedding]=False\n",
            "[INFO 2024-12-03 17:03:54,633] args[freeze_embedding]=False\n",
            "[INFO 2024-12-03 17:03:54,633] args[freeze_embedding]=False\n",
            "[INFO 2024-12-03 17:03:54,633] args[freeze_embedding]=False\n",
            "[INFO 2024-12-03 17:03:54,633] args[freeze_embedding]=False\n",
            "[INFO 2024-12-03 17:03:54,633] args[freeze_embedding]=False\n",
            "[INFO 2024-12-03 17:03:54,633] args[freeze_embedding]=False\n",
            "[INFO 2024-12-03 17:03:54,633] args[freeze_embedding]=False\n",
            "[INFO 2024-12-03 17:03:54,633] args[freeze_embedding]=False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[glove_embedding_path]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/glove.840B.300d.txt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,647] args[glove_embedding_path]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/glove.840B.300d.txt\n",
            "[INFO 2024-12-03 17:03:54,647] args[glove_embedding_path]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/glove.840B.300d.txt\n",
            "[INFO 2024-12-03 17:03:54,647] args[glove_embedding_path]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/glove.840B.300d.txt\n",
            "[INFO 2024-12-03 17:03:54,647] args[glove_embedding_path]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/glove.840B.300d.txt\n",
            "[INFO 2024-12-03 17:03:54,647] args[glove_embedding_path]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/glove.840B.300d.txt\n",
            "[INFO 2024-12-03 17:03:54,647] args[glove_embedding_path]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/glove.840B.300d.txt\n",
            "[INFO 2024-12-03 17:03:54,647] args[glove_embedding_path]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/glove.840B.300d.txt\n",
            "[INFO 2024-12-03 17:03:54,647] args[glove_embedding_path]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/glove.840B.300d.txt\n",
            "[INFO 2024-12-03 17:03:54,647] args[glove_embedding_path]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/glove.840B.300d.txt\n",
            "[INFO 2024-12-03 17:03:54,647] args[glove_embedding_path]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/glove.840B.300d.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[load_ckpt_name]=None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,659] args[load_ckpt_name]=None\n",
            "[INFO 2024-12-03 17:03:54,659] args[load_ckpt_name]=None\n",
            "[INFO 2024-12-03 17:03:54,659] args[load_ckpt_name]=None\n",
            "[INFO 2024-12-03 17:03:54,659] args[load_ckpt_name]=None\n",
            "[INFO 2024-12-03 17:03:54,659] args[load_ckpt_name]=None\n",
            "[INFO 2024-12-03 17:03:54,659] args[load_ckpt_name]=None\n",
            "[INFO 2024-12-03 17:03:54,659] args[load_ckpt_name]=None\n",
            "[INFO 2024-12-03 17:03:54,659] args[load_ckpt_name]=None\n",
            "[INFO 2024-12-03 17:03:54,659] args[load_ckpt_name]=None\n",
            "[INFO 2024-12-03 17:03:54,659] args[load_ckpt_name]=None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[log_steps]=100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,673] args[log_steps]=100\n",
            "[INFO 2024-12-03 17:03:54,673] args[log_steps]=100\n",
            "[INFO 2024-12-03 17:03:54,673] args[log_steps]=100\n",
            "[INFO 2024-12-03 17:03:54,673] args[log_steps]=100\n",
            "[INFO 2024-12-03 17:03:54,673] args[log_steps]=100\n",
            "[INFO 2024-12-03 17:03:54,673] args[log_steps]=100\n",
            "[INFO 2024-12-03 17:03:54,673] args[log_steps]=100\n",
            "[INFO 2024-12-03 17:03:54,673] args[log_steps]=100\n",
            "[INFO 2024-12-03 17:03:54,673] args[log_steps]=100\n",
            "[INFO 2024-12-03 17:03:54,673] args[log_steps]=100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[lr]=0.0003\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,686] args[lr]=0.0003\n",
            "[INFO 2024-12-03 17:03:54,686] args[lr]=0.0003\n",
            "[INFO 2024-12-03 17:03:54,686] args[lr]=0.0003\n",
            "[INFO 2024-12-03 17:03:54,686] args[lr]=0.0003\n",
            "[INFO 2024-12-03 17:03:54,686] args[lr]=0.0003\n",
            "[INFO 2024-12-03 17:03:54,686] args[lr]=0.0003\n",
            "[INFO 2024-12-03 17:03:54,686] args[lr]=0.0003\n",
            "[INFO 2024-12-03 17:03:54,686] args[lr]=0.0003\n",
            "[INFO 2024-12-03 17:03:54,686] args[lr]=0.0003\n",
            "[INFO 2024-12-03 17:03:54,686] args[lr]=0.0003\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[mode]=train\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,698] args[mode]=train\n",
            "[INFO 2024-12-03 17:03:54,698] args[mode]=train\n",
            "[INFO 2024-12-03 17:03:54,698] args[mode]=train\n",
            "[INFO 2024-12-03 17:03:54,698] args[mode]=train\n",
            "[INFO 2024-12-03 17:03:54,698] args[mode]=train\n",
            "[INFO 2024-12-03 17:03:54,698] args[mode]=train\n",
            "[INFO 2024-12-03 17:03:54,698] args[mode]=train\n",
            "[INFO 2024-12-03 17:03:54,698] args[mode]=train\n",
            "[INFO 2024-12-03 17:03:54,698] args[mode]=train\n",
            "[INFO 2024-12-03 17:03:54,698] args[mode]=train\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[model_dir]=/content/model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,713] args[model_dir]=/content/model\n",
            "[INFO 2024-12-03 17:03:54,713] args[model_dir]=/content/model\n",
            "[INFO 2024-12-03 17:03:54,713] args[model_dir]=/content/model\n",
            "[INFO 2024-12-03 17:03:54,713] args[model_dir]=/content/model\n",
            "[INFO 2024-12-03 17:03:54,713] args[model_dir]=/content/model\n",
            "[INFO 2024-12-03 17:03:54,713] args[model_dir]=/content/model\n",
            "[INFO 2024-12-03 17:03:54,713] args[model_dir]=/content/model\n",
            "[INFO 2024-12-03 17:03:54,713] args[model_dir]=/content/model\n",
            "[INFO 2024-12-03 17:03:54,713] args[model_dir]=/content/model\n",
            "[INFO 2024-12-03 17:03:54,713] args[model_dir]=/content/model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[nGPU]=1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,722] args[nGPU]=1\n",
            "[INFO 2024-12-03 17:03:54,722] args[nGPU]=1\n",
            "[INFO 2024-12-03 17:03:54,722] args[nGPU]=1\n",
            "[INFO 2024-12-03 17:03:54,722] args[nGPU]=1\n",
            "[INFO 2024-12-03 17:03:54,722] args[nGPU]=1\n",
            "[INFO 2024-12-03 17:03:54,722] args[nGPU]=1\n",
            "[INFO 2024-12-03 17:03:54,722] args[nGPU]=1\n",
            "[INFO 2024-12-03 17:03:54,722] args[nGPU]=1\n",
            "[INFO 2024-12-03 17:03:54,722] args[nGPU]=1\n",
            "[INFO 2024-12-03 17:03:54,722] args[nGPU]=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[news_dim]=400\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,731] args[news_dim]=400\n",
            "[INFO 2024-12-03 17:03:54,731] args[news_dim]=400\n",
            "[INFO 2024-12-03 17:03:54,731] args[news_dim]=400\n",
            "[INFO 2024-12-03 17:03:54,731] args[news_dim]=400\n",
            "[INFO 2024-12-03 17:03:54,731] args[news_dim]=400\n",
            "[INFO 2024-12-03 17:03:54,731] args[news_dim]=400\n",
            "[INFO 2024-12-03 17:03:54,731] args[news_dim]=400\n",
            "[INFO 2024-12-03 17:03:54,731] args[news_dim]=400\n",
            "[INFO 2024-12-03 17:03:54,731] args[news_dim]=400\n",
            "[INFO 2024-12-03 17:03:54,731] args[news_dim]=400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[news_query_vector_dim]=200\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,745] args[news_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,745] args[news_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,745] args[news_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,745] args[news_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,745] args[news_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,745] args[news_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,745] args[news_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,745] args[news_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,745] args[news_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,745] args[news_query_vector_dim]=200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[npratio]=4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,755] args[npratio]=4\n",
            "[INFO 2024-12-03 17:03:54,755] args[npratio]=4\n",
            "[INFO 2024-12-03 17:03:54,755] args[npratio]=4\n",
            "[INFO 2024-12-03 17:03:54,755] args[npratio]=4\n",
            "[INFO 2024-12-03 17:03:54,755] args[npratio]=4\n",
            "[INFO 2024-12-03 17:03:54,755] args[npratio]=4\n",
            "[INFO 2024-12-03 17:03:54,755] args[npratio]=4\n",
            "[INFO 2024-12-03 17:03:54,755] args[npratio]=4\n",
            "[INFO 2024-12-03 17:03:54,755] args[npratio]=4\n",
            "[INFO 2024-12-03 17:03:54,755] args[npratio]=4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[num_attention_heads]=20\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,765] args[num_attention_heads]=20\n",
            "[INFO 2024-12-03 17:03:54,765] args[num_attention_heads]=20\n",
            "[INFO 2024-12-03 17:03:54,765] args[num_attention_heads]=20\n",
            "[INFO 2024-12-03 17:03:54,765] args[num_attention_heads]=20\n",
            "[INFO 2024-12-03 17:03:54,765] args[num_attention_heads]=20\n",
            "[INFO 2024-12-03 17:03:54,765] args[num_attention_heads]=20\n",
            "[INFO 2024-12-03 17:03:54,765] args[num_attention_heads]=20\n",
            "[INFO 2024-12-03 17:03:54,765] args[num_attention_heads]=20\n",
            "[INFO 2024-12-03 17:03:54,765] args[num_attention_heads]=20\n",
            "[INFO 2024-12-03 17:03:54,765] args[num_attention_heads]=20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[num_words_abstract]=50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,776] args[num_words_abstract]=50\n",
            "[INFO 2024-12-03 17:03:54,776] args[num_words_abstract]=50\n",
            "[INFO 2024-12-03 17:03:54,776] args[num_words_abstract]=50\n",
            "[INFO 2024-12-03 17:03:54,776] args[num_words_abstract]=50\n",
            "[INFO 2024-12-03 17:03:54,776] args[num_words_abstract]=50\n",
            "[INFO 2024-12-03 17:03:54,776] args[num_words_abstract]=50\n",
            "[INFO 2024-12-03 17:03:54,776] args[num_words_abstract]=50\n",
            "[INFO 2024-12-03 17:03:54,776] args[num_words_abstract]=50\n",
            "[INFO 2024-12-03 17:03:54,776] args[num_words_abstract]=50\n",
            "[INFO 2024-12-03 17:03:54,776] args[num_words_abstract]=50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[num_words_title]=20\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,787] args[num_words_title]=20\n",
            "[INFO 2024-12-03 17:03:54,787] args[num_words_title]=20\n",
            "[INFO 2024-12-03 17:03:54,787] args[num_words_title]=20\n",
            "[INFO 2024-12-03 17:03:54,787] args[num_words_title]=20\n",
            "[INFO 2024-12-03 17:03:54,787] args[num_words_title]=20\n",
            "[INFO 2024-12-03 17:03:54,787] args[num_words_title]=20\n",
            "[INFO 2024-12-03 17:03:54,787] args[num_words_title]=20\n",
            "[INFO 2024-12-03 17:03:54,787] args[num_words_title]=20\n",
            "[INFO 2024-12-03 17:03:54,787] args[num_words_title]=20\n",
            "[INFO 2024-12-03 17:03:54,787] args[num_words_title]=20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[prepare]=True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,798] args[prepare]=True\n",
            "[INFO 2024-12-03 17:03:54,798] args[prepare]=True\n",
            "[INFO 2024-12-03 17:03:54,798] args[prepare]=True\n",
            "[INFO 2024-12-03 17:03:54,798] args[prepare]=True\n",
            "[INFO 2024-12-03 17:03:54,798] args[prepare]=True\n",
            "[INFO 2024-12-03 17:03:54,798] args[prepare]=True\n",
            "[INFO 2024-12-03 17:03:54,798] args[prepare]=True\n",
            "[INFO 2024-12-03 17:03:54,798] args[prepare]=True\n",
            "[INFO 2024-12-03 17:03:54,798] args[prepare]=True\n",
            "[INFO 2024-12-03 17:03:54,798] args[prepare]=True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[save_steps]=10000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,809] args[save_steps]=10000\n",
            "[INFO 2024-12-03 17:03:54,809] args[save_steps]=10000\n",
            "[INFO 2024-12-03 17:03:54,809] args[save_steps]=10000\n",
            "[INFO 2024-12-03 17:03:54,809] args[save_steps]=10000\n",
            "[INFO 2024-12-03 17:03:54,809] args[save_steps]=10000\n",
            "[INFO 2024-12-03 17:03:54,809] args[save_steps]=10000\n",
            "[INFO 2024-12-03 17:03:54,809] args[save_steps]=10000\n",
            "[INFO 2024-12-03 17:03:54,809] args[save_steps]=10000\n",
            "[INFO 2024-12-03 17:03:54,809] args[save_steps]=10000\n",
            "[INFO 2024-12-03 17:03:54,809] args[save_steps]=10000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[seed]=0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,820] args[seed]=0\n",
            "[INFO 2024-12-03 17:03:54,820] args[seed]=0\n",
            "[INFO 2024-12-03 17:03:54,820] args[seed]=0\n",
            "[INFO 2024-12-03 17:03:54,820] args[seed]=0\n",
            "[INFO 2024-12-03 17:03:54,820] args[seed]=0\n",
            "[INFO 2024-12-03 17:03:54,820] args[seed]=0\n",
            "[INFO 2024-12-03 17:03:54,820] args[seed]=0\n",
            "[INFO 2024-12-03 17:03:54,820] args[seed]=0\n",
            "[INFO 2024-12-03 17:03:54,820] args[seed]=0\n",
            "[INFO 2024-12-03 17:03:54,820] args[seed]=0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[start_epoch]=0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,831] args[start_epoch]=0\n",
            "[INFO 2024-12-03 17:03:54,831] args[start_epoch]=0\n",
            "[INFO 2024-12-03 17:03:54,831] args[start_epoch]=0\n",
            "[INFO 2024-12-03 17:03:54,831] args[start_epoch]=0\n",
            "[INFO 2024-12-03 17:03:54,831] args[start_epoch]=0\n",
            "[INFO 2024-12-03 17:03:54,831] args[start_epoch]=0\n",
            "[INFO 2024-12-03 17:03:54,831] args[start_epoch]=0\n",
            "[INFO 2024-12-03 17:03:54,831] args[start_epoch]=0\n",
            "[INFO 2024-12-03 17:03:54,831] args[start_epoch]=0\n",
            "[INFO 2024-12-03 17:03:54,831] args[start_epoch]=0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[test_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_dev\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,841] args[test_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_dev\n",
            "[INFO 2024-12-03 17:03:54,841] args[test_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_dev\n",
            "[INFO 2024-12-03 17:03:54,841] args[test_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_dev\n",
            "[INFO 2024-12-03 17:03:54,841] args[test_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_dev\n",
            "[INFO 2024-12-03 17:03:54,841] args[test_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_dev\n",
            "[INFO 2024-12-03 17:03:54,841] args[test_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_dev\n",
            "[INFO 2024-12-03 17:03:54,841] args[test_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_dev\n",
            "[INFO 2024-12-03 17:03:54,841] args[test_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_dev\n",
            "[INFO 2024-12-03 17:03:54,841] args[test_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_dev\n",
            "[INFO 2024-12-03 17:03:54,841] args[test_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_dev\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[train_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_train\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,856] args[train_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_train\n",
            "[INFO 2024-12-03 17:03:54,856] args[train_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_train\n",
            "[INFO 2024-12-03 17:03:54,856] args[train_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_train\n",
            "[INFO 2024-12-03 17:03:54,856] args[train_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_train\n",
            "[INFO 2024-12-03 17:03:54,856] args[train_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_train\n",
            "[INFO 2024-12-03 17:03:54,856] args[train_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_train\n",
            "[INFO 2024-12-03 17:03:54,856] args[train_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_train\n",
            "[INFO 2024-12-03 17:03:54,856] args[train_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_train\n",
            "[INFO 2024-12-03 17:03:54,856] args[train_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_train\n",
            "[INFO 2024-12-03 17:03:54,856] args[train_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_train\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[use_category]=True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,866] args[use_category]=True\n",
            "[INFO 2024-12-03 17:03:54,866] args[use_category]=True\n",
            "[INFO 2024-12-03 17:03:54,866] args[use_category]=True\n",
            "[INFO 2024-12-03 17:03:54,866] args[use_category]=True\n",
            "[INFO 2024-12-03 17:03:54,866] args[use_category]=True\n",
            "[INFO 2024-12-03 17:03:54,866] args[use_category]=True\n",
            "[INFO 2024-12-03 17:03:54,866] args[use_category]=True\n",
            "[INFO 2024-12-03 17:03:54,866] args[use_category]=True\n",
            "[INFO 2024-12-03 17:03:54,866] args[use_category]=True\n",
            "[INFO 2024-12-03 17:03:54,866] args[use_category]=True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[use_subcategory]=True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,876] args[use_subcategory]=True\n",
            "[INFO 2024-12-03 17:03:54,876] args[use_subcategory]=True\n",
            "[INFO 2024-12-03 17:03:54,876] args[use_subcategory]=True\n",
            "[INFO 2024-12-03 17:03:54,876] args[use_subcategory]=True\n",
            "[INFO 2024-12-03 17:03:54,876] args[use_subcategory]=True\n",
            "[INFO 2024-12-03 17:03:54,876] args[use_subcategory]=True\n",
            "[INFO 2024-12-03 17:03:54,876] args[use_subcategory]=True\n",
            "[INFO 2024-12-03 17:03:54,876] args[use_subcategory]=True\n",
            "[INFO 2024-12-03 17:03:54,876] args[use_subcategory]=True\n",
            "[INFO 2024-12-03 17:03:54,876] args[use_subcategory]=True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[user_log_length]=50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,886] args[user_log_length]=50\n",
            "[INFO 2024-12-03 17:03:54,886] args[user_log_length]=50\n",
            "[INFO 2024-12-03 17:03:54,886] args[user_log_length]=50\n",
            "[INFO 2024-12-03 17:03:54,886] args[user_log_length]=50\n",
            "[INFO 2024-12-03 17:03:54,886] args[user_log_length]=50\n",
            "[INFO 2024-12-03 17:03:54,886] args[user_log_length]=50\n",
            "[INFO 2024-12-03 17:03:54,886] args[user_log_length]=50\n",
            "[INFO 2024-12-03 17:03:54,886] args[user_log_length]=50\n",
            "[INFO 2024-12-03 17:03:54,886] args[user_log_length]=50\n",
            "[INFO 2024-12-03 17:03:54,886] args[user_log_length]=50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[user_log_mask]=False\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,895] args[user_log_mask]=False\n",
            "[INFO 2024-12-03 17:03:54,895] args[user_log_mask]=False\n",
            "[INFO 2024-12-03 17:03:54,895] args[user_log_mask]=False\n",
            "[INFO 2024-12-03 17:03:54,895] args[user_log_mask]=False\n",
            "[INFO 2024-12-03 17:03:54,895] args[user_log_mask]=False\n",
            "[INFO 2024-12-03 17:03:54,895] args[user_log_mask]=False\n",
            "[INFO 2024-12-03 17:03:54,895] args[user_log_mask]=False\n",
            "[INFO 2024-12-03 17:03:54,895] args[user_log_mask]=False\n",
            "[INFO 2024-12-03 17:03:54,895] args[user_log_mask]=False\n",
            "[INFO 2024-12-03 17:03:54,895] args[user_log_mask]=False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[user_query_vector_dim]=200\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,905] args[user_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,905] args[user_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,905] args[user_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,905] args[user_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,905] args[user_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,905] args[user_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,905] args[user_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,905] args[user_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,905] args[user_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:03:54,905] args[user_query_vector_dim]=200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[word_embedding_dim]=300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:03:54,918] args[word_embedding_dim]=300\n",
            "[INFO 2024-12-03 17:03:54,918] args[word_embedding_dim]=300\n",
            "[INFO 2024-12-03 17:03:54,918] args[word_embedding_dim]=300\n",
            "[INFO 2024-12-03 17:03:54,918] args[word_embedding_dim]=300\n",
            "[INFO 2024-12-03 17:03:54,918] args[word_embedding_dim]=300\n",
            "[INFO 2024-12-03 17:03:54,918] args[word_embedding_dim]=300\n",
            "[INFO 2024-12-03 17:03:54,918] args[word_embedding_dim]=300\n",
            "[INFO 2024-12-03 17:03:54,918] args[word_embedding_dim]=300\n",
            "[INFO 2024-12-03 17:03:54,918] args[word_embedding_dim]=300\n",
            "[INFO 2024-12-03 17:03:54,918] args[word_embedding_dim]=300\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-43e74810760f>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdump_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Log the argument values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/random.py\u001b[0m in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_in_bad_fork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mcb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mdefault_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_generators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "setuplogger()\n",
        "args = Args()\n",
        "dump_args(args)  # Log the argument values\n",
        "random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "Path(args.model_dir).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_SLKbAZpKFK"
      },
      "source": [
        "**Dataset.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pqMrlk2u-Hiv"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import IterableDataset, Dataset\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "class DatasetTrain(IterableDataset):\n",
        "    def __init__(self, filename, news_index, news_combined, args):\n",
        "        super(DatasetTrain).__init__()\n",
        "        self.filename = filename\n",
        "        self.news_index = news_index\n",
        "        self.news_combined = news_combined\n",
        "        self.args = args\n",
        "\n",
        "    def trans_to_nindex(self, nids):\n",
        "        return [self.news_index[i] if i in self.news_index else 0 for i in nids]\n",
        "\n",
        "    def pad_to_fix_len(self, x, fix_length, padding_front=True, padding_value=0):\n",
        "        if padding_front:\n",
        "            pad_x = [padding_value] * (fix_length - len(x)) + x[-fix_length:]\n",
        "            mask = [0] * (fix_length - len(x)) + [1] * min(fix_length, len(x))\n",
        "        else:\n",
        "            pad_x = x[-fix_length:] + [padding_value] * (fix_length - len(x))\n",
        "            mask = [1] * min(fix_length, len(x)) + [0] * (fix_length - len(x))\n",
        "        return pad_x, np.array(mask, dtype='float32')\n",
        "\n",
        "    def line_mapper(self, line):\n",
        "        line = line.strip().split('\\t')\n",
        "        click_docs = line[3].split()\n",
        "        sess_pos = line[4].split()\n",
        "        sess_neg = line[5].split()\n",
        "\n",
        "        click_docs, log_mask = self.pad_to_fix_len(self.trans_to_nindex(click_docs), self.args.user_log_length)\n",
        "        user_feature = self.news_combined[click_docs]\n",
        "\n",
        "        pos = self.trans_to_nindex(sess_pos)\n",
        "        neg = self.trans_to_nindex(sess_neg)\n",
        "\n",
        "        label = random.randint(0, self.args.npratio)\n",
        "        sample_news = neg[:label] + pos + neg[label:]\n",
        "        news_feature = self.news_combined[sample_news]\n",
        "\n",
        "        return user_feature, log_mask, news_feature, label\n",
        "\n",
        "    def __iter__(self):\n",
        "        file_iter = open(self.filename)\n",
        "        return map(self.line_mapper, file_iter)\n",
        "\n",
        "\n",
        "class DatasetTest(DatasetTrain):\n",
        "    def __init__(self, filename, news_index, news_scoring, args):\n",
        "        super(DatasetTrain).__init__()\n",
        "        self.filename = filename\n",
        "        self.news_index = news_index\n",
        "        self.news_scoring = news_scoring\n",
        "        self.args = args\n",
        "\n",
        "    def line_mapper(self, line):\n",
        "        line = line.strip().split('\\t')\n",
        "        click_docs = line[3].split()\n",
        "        click_docs, log_mask = self.pad_to_fix_len(self.trans_to_nindex(click_docs), self.args.user_log_length)\n",
        "        user_feature = self.news_scoring[click_docs]\n",
        "\n",
        "        candidate_news = self.trans_to_nindex([i.split('-')[0] for i in line[4].split()])\n",
        "        labels = np.array([int(i.split('-')[1]) for i in line[4].split()])\n",
        "        news_feature = self.news_scoring[candidate_news]\n",
        "\n",
        "        return user_feature, log_mask, news_feature, labels\n",
        "\n",
        "    def __iter__(self):\n",
        "        file_iter = open(self.filename)\n",
        "        return map(self.line_mapper, file_iter)\n",
        "\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1IjisEE-LHb"
      },
      "source": [
        "**Metric.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ppd8qdG1-Nr3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def dcg_score(y_true, y_score, k=10):\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "    gains = 2**y_true - 1\n",
        "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
        "    return np.sum(gains / discounts)\n",
        "\n",
        "\n",
        "def ndcg_score(y_true, y_score, k=10):\n",
        "    best = dcg_score(y_true, y_true, k)\n",
        "    actual = dcg_score(y_true, y_score, k)\n",
        "    return actual / best\n",
        "\n",
        "\n",
        "def mrr_score(y_true, y_score):\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order)\n",
        "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
        "    return np.sum(rr_score) / np.sum(y_true)\n",
        "\n",
        "\n",
        "def ctr_score(y_true, y_score, k=1):\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "    return np.mean(y_true)\n",
        "\n",
        "def acc(y_true, y_hat):\n",
        "    y_hat = torch.argmax(y_hat, dim=-1)\n",
        "    tot = y_true.shape[0]\n",
        "    hit = torch.sum(y_true == y_hat)\n",
        "    return hit.data.float() * 1.0 / tot\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1ZeoY8x-5Ds"
      },
      "source": [
        "**Ultis.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Dfr-ri4E-s3M"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import argparse\n",
        "import sys\n",
        "\n",
        "def setuplogger():\n",
        "    root = logging.getLogger()\n",
        "    root.setLevel(logging.INFO)\n",
        "    handler = logging.StreamHandler(sys.stdout)\n",
        "    handler.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter(\"[%(levelname)s %(asctime)s] %(message)s\")\n",
        "    handler.setFormatter(formatter)\n",
        "    root.addHandler(handler)\n",
        "\n",
        "\n",
        "def dump_args(args):\n",
        "    for arg in dir(args):\n",
        "        if not arg.startswith(\"_\"):\n",
        "            logging.info(f\"args[{arg}]={getattr(args, arg)}\")\n",
        "\n",
        "def load_matrix(embedding_file_path, word_dict, word_embedding_dim):\n",
        "    embedding_matrix = np.zeros(shape=(len(word_dict) + 1, word_embedding_dim))\n",
        "    have_word = []\n",
        "    if embedding_file_path is not None:\n",
        "        with open(embedding_file_path, 'rb') as f:\n",
        "            while True:\n",
        "                line = f.readline()\n",
        "                if len(line) == 0:\n",
        "                    break\n",
        "                line = line.split()\n",
        "                word = line[0].decode()\n",
        "                if word in word_dict:\n",
        "                    index = word_dict[word]\n",
        "                    tp = [float(x) for x in line[1:]]\n",
        "                    embedding_matrix[index] = np.array(tp)\n",
        "                    have_word.append(word)\n",
        "    return embedding_matrix, have_word\n",
        "\n",
        "\n",
        "def get_checkpoint(directory, ckpt_name):\n",
        "    ckpt_path = os.path.join(directory, ckpt_name)\n",
        "    if os.path.exists(ckpt_path):\n",
        "        return ckpt_path\n",
        "    else:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvlabjELAPTD"
      },
      "source": [
        "**Model_ultis.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XW3eigdb-7qN"
      },
      "outputs": [],
      "source": [
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size):\n",
        "        super(AttentionPooling, self).__init__()\n",
        "        self.att_fc1 = nn.Linear(emb_size, hidden_size)\n",
        "        self.att_fc2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: batch_size, candidate_size, emb_dim\n",
        "            attn_mask: batch_size, candidate_size\n",
        "        Returns:\n",
        "            (shape) batch_size, emb_dim\n",
        "        \"\"\"\n",
        "        e = self.att_fc1(x)\n",
        "        e = nn.Tanh()(e)\n",
        "        alpha = self.att_fc2(e)\n",
        "        alpha = torch.exp(alpha)\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            alpha = alpha * attn_mask.unsqueeze(2)\n",
        "\n",
        "        alpha = alpha / (torch.sum(alpha, dim=1, keepdim=True) + 1e-8)\n",
        "        x = torch.bmm(x.permute(0, 2, 1), alpha).squeeze(dim=-1)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiNcPBElAV2B"
      },
      "source": [
        "# NAML.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eD9GNW8tAYTb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class NewsEncoder(nn.Module):\n",
        "    def __init__(self, args, embedding_matrix, num_category, num_subcategory):\n",
        "        super(NewsEncoder, self).__init__()\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "        self.drop_rate = args.drop_rate\n",
        "        self.num_words_title = args.num_words_title\n",
        "        self.use_category = args.use_category\n",
        "        self.use_subcategory = args.use_subcategory\n",
        "        if args.use_category:\n",
        "            self.category_emb = nn.Embedding(num_category + 1, args.category_emb_dim, padding_idx=0)\n",
        "            self.category_dense = nn.Linear(args.category_emb_dim, args.news_dim)\n",
        "        if args.use_subcategory:\n",
        "            self.subcategory_emb = nn.Embedding(num_subcategory + 1, args.category_emb_dim, padding_idx=0)\n",
        "            self.subcategory_dense = nn.Linear(args.category_emb_dim, args.news_dim)\n",
        "        if args.use_category or args.use_subcategory:\n",
        "            self.final_attn = AttentionPooling(args.news_dim, args.news_query_vector_dim)\n",
        "        self.cnn = nn.Conv1d(\n",
        "            in_channels=args.word_embedding_dim,\n",
        "            out_channels=args.news_dim,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "        self.attn = AttentionPooling(args.news_dim, args.news_query_vector_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        '''\n",
        "            x: batch_size, word_num\n",
        "            mask: batch_size, word_num\n",
        "        '''\n",
        "        title = torch.narrow(x, -1, 0, self.num_words_title).long()\n",
        "        word_vecs = F.dropout(self.embedding_matrix(title),\n",
        "                              p=self.drop_rate,\n",
        "                              training=self.training)\n",
        "        context_word_vecs = self.cnn(word_vecs.transpose(1, 2)).transpose(1, 2)\n",
        "        title_vecs = self.attn(context_word_vecs, mask)\n",
        "        all_vecs = [title_vecs]\n",
        "\n",
        "        start = self.num_words_title\n",
        "        if self.use_category:\n",
        "            category = torch.narrow(x, -1, start, 1).squeeze(dim=-1).long()\n",
        "            category_vecs = self.category_dense(self.category_emb(category))\n",
        "            all_vecs.append(category_vecs)\n",
        "            start += 1\n",
        "        if self.use_subcategory:\n",
        "            subcategory = torch.narrow(x, -1, start, 1).squeeze(dim=-1).long()\n",
        "            subcategory_vecs = self.subcategory_dense(self.subcategory_emb(subcategory))\n",
        "            all_vecs.append(subcategory_vecs)\n",
        "\n",
        "        if len(all_vecs) == 1:\n",
        "            news_vecs = all_vecs[0]\n",
        "        else:\n",
        "            all_vecs = torch.stack(all_vecs, dim=1)\n",
        "            news_vecs = self.final_attn(all_vecs)\n",
        "        return news_vecs\n",
        "\n",
        "\n",
        "class UserEncoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(UserEncoder, self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.attn = AttentionPooling(args.news_dim, args.user_query_vector_dim)\n",
        "        self.pad_doc = nn.Parameter(torch.empty(1, args.news_dim).uniform_(-1, 1)).type(torch.FloatTensor)\n",
        "\n",
        "    def forward(self, news_vecs, log_mask=None):\n",
        "        '''\n",
        "            news_vecs: batch_size, history_num, news_dim\n",
        "            log_mask: batch_size, history_num\n",
        "        '''\n",
        "        bz = news_vecs.shape[0]\n",
        "        if self.args.user_log_mask:\n",
        "            user_vec = self.attn(news_vecs, log_mask)\n",
        "        else:\n",
        "            padding_doc = self.pad_doc.unsqueeze(dim=0).expand(bz, self.args.user_log_length, -1)\n",
        "            news_vecs = news_vecs * log_mask.unsqueeze(dim=-1) + padding_doc * (1 - log_mask.unsqueeze(dim=-1))\n",
        "            user_vec = self.attn(news_vecs)\n",
        "        return user_vec\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, args, embedding_matrix, num_category, num_subcategory, **kwargs):\n",
        "        super(Model, self).__init__()\n",
        "        self.args = args\n",
        "        pretrained_word_embedding = torch.from_numpy(embedding_matrix).float()\n",
        "        word_embedding = nn.Embedding.from_pretrained(pretrained_word_embedding,\n",
        "                                                      freeze=args.freeze_embedding,\n",
        "                                                      padding_idx=0)\n",
        "\n",
        "        self.news_encoder = NewsEncoder(args, word_embedding, num_category, num_subcategory)\n",
        "        self.user_encoder = UserEncoder(args)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, history, history_mask, candidate, label):\n",
        "        '''\n",
        "            history: batch_size, history_length, num_word_title\n",
        "            history_mask: batch_size, history_length\n",
        "            candidate: batch_size, 1+K, num_word_title\n",
        "            label: batch_size, 1+K\n",
        "        '''\n",
        "        num_words = history.shape[-1]\n",
        "        candidate_news = candidate.reshape(-1, num_words)\n",
        "        candidate_news_vecs = self.news_encoder(candidate_news).reshape(-1, 1 + self.args.npratio, self.args.news_dim)\n",
        "\n",
        "        history_news = history.reshape(-1, num_words)\n",
        "        history_news_vecs = self.news_encoder(history_news).reshape(-1, self.args.user_log_length, self.args.news_dim)\n",
        "\n",
        "        user_vec = self.user_encoder(history_news_vecs, history_mask)\n",
        "        score = torch.bmm(candidate_news_vecs, user_vec.unsqueeze(dim=-1)).squeeze(dim=-1)\n",
        "        loss = self.loss_fn(score, label)\n",
        "        return loss, score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqNlsklUAi48"
      },
      "source": [
        "**preprocess.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_Ja_pvD0AYwg"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "def update_dict(dict, key, value=None):\n",
        "    if key not in dict:\n",
        "        if value is None:\n",
        "            dict[key] = len(dict) + 1\n",
        "        else:\n",
        "            dict[key] = value\n",
        "\n",
        "\n",
        "def read_news(news_path, args, mode='train'):\n",
        "    news = {}\n",
        "    category_dict = {}\n",
        "    subcategory_dict = {}\n",
        "    news_index = {}\n",
        "    word_cnt = Counter()\n",
        "\n",
        "    with open(news_path, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f):\n",
        "            splited = line.strip('\\n').split('\\t')\n",
        "            doc_id, category, subcategory, title, abstract, url, _, _ = splited\n",
        "            update_dict(news_index, doc_id)\n",
        "\n",
        "            title = title.lower()\n",
        "            title = word_tokenize(title, language='english')\n",
        "            update_dict(news, doc_id, [title, category, subcategory])\n",
        "            if mode == 'train':\n",
        "                if args.use_category:\n",
        "                    update_dict(category_dict, category)\n",
        "                if args.use_subcategory:\n",
        "                    update_dict(subcategory_dict, subcategory)\n",
        "                word_cnt.update(title)\n",
        "\n",
        "    if mode == 'train':\n",
        "        word = [k for k, v in word_cnt.items() if v > args.filter_num]\n",
        "        word_dict = {k: v for k, v in zip(word, range(1, len(word) + 1))}\n",
        "        return news, news_index, category_dict, subcategory_dict, word_dict\n",
        "    elif mode == 'test':\n",
        "        return news, news_index\n",
        "    else:\n",
        "        assert False, 'Wrong mode!'\n",
        "\n",
        "\n",
        "def get_doc_input(news, news_index, category_dict, subcategory_dict, word_dict, args):\n",
        "    news_num = len(news) + 1\n",
        "    news_title = np.zeros((news_num, args.num_words_title), dtype='int32')\n",
        "    news_category = np.zeros((news_num, 1), dtype='int32') if args.use_category else None\n",
        "    news_subcategory = np.zeros((news_num, 1), dtype='int32') if args.use_subcategory else None\n",
        "\n",
        "    for key in tqdm(news):\n",
        "        title, category, subcategory = news[key]\n",
        "        doc_index = news_index[key]\n",
        "\n",
        "        for word_id in range(min(args.num_words_title, len(title))):\n",
        "            if title[word_id] in word_dict:\n",
        "                news_title[doc_index, word_id] = word_dict[title[word_id]]\n",
        "\n",
        "        if args.use_category:\n",
        "            news_category[doc_index, 0] = category_dict[category] if category in category_dict else 0\n",
        "        if args.use_subcategory:\n",
        "            news_subcategory[doc_index, 0] = subcategory_dict[subcategory] if subcategory in subcategory_dict else 0\n",
        "\n",
        "    return news_title, news_category, news_subcategory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu43E3_6AqAA"
      },
      "source": [
        "**prepare_data.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5Qzn9-0kAuFF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import logging\n",
        "\n",
        "\n",
        "def get_sample(all_elements, num_sample):\n",
        "    if num_sample > len(all_elements):\n",
        "        return random.sample(all_elements * (num_sample // len(all_elements) + 1), num_sample)\n",
        "    else:\n",
        "        return random.sample(all_elements, num_sample)\n",
        "\n",
        "\n",
        "def prepare_training_data(train_data_dir, nGPU, npratio, seed):\n",
        "    random.seed(seed)\n",
        "    behaviors = []\n",
        "\n",
        "    behavior_file_path = os.path.join(train_data_dir, 'behaviors.tsv')\n",
        "    with open(behavior_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f):\n",
        "            iid, uid, time, history, imp = line.strip().split('\\t')\n",
        "            impressions = [x.split('-') for x in imp.split(' ')]\n",
        "            pos, neg = [], []\n",
        "            for news_ID, label in impressions:\n",
        "                if label == '0':\n",
        "                    neg.append(news_ID)\n",
        "                elif label == '1':\n",
        "                    pos.append(news_ID)\n",
        "            if len(pos) == 0 or len(neg) == 0:\n",
        "                continue\n",
        "            for pos_id in pos:\n",
        "                neg_candidate = get_sample(neg, npratio)\n",
        "                neg_str = ' '.join(neg_candidate)\n",
        "                new_line = '\\t'.join([iid, uid, time, history, pos_id, neg_str]) + '\\n'\n",
        "                behaviors.append(new_line)\n",
        "\n",
        "    random.shuffle(behaviors)\n",
        "\n",
        "    behaviors_per_file = [[] for _ in range(nGPU)]\n",
        "    for i, line in enumerate(behaviors):\n",
        "        behaviors_per_file[i % nGPU].append(line)\n",
        "\n",
        "    logging.info('Writing files...')\n",
        "    for i in range(nGPU):\n",
        "        processed_file_path = os.path.join(train_data_dir, f'behaviors_np{npratio}_{i}.tsv')\n",
        "        with open(processed_file_path, 'w') as f:\n",
        "            f.writelines(behaviors_per_file[i])\n",
        "\n",
        "    return len(behaviors)\n",
        "\n",
        "\n",
        "def prepare_testing_data(test_data_dir, nGPU):\n",
        "    behaviors = [[] for _ in range(nGPU)]\n",
        "\n",
        "    behavior_file_path = os.path.join(test_data_dir, 'behaviors.tsv')\n",
        "    with open(behavior_file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(tqdm(f)):\n",
        "            behaviors[i % nGPU].append(line)\n",
        "\n",
        "    logging.info('Writing files...')\n",
        "    for i in range(nGPU):\n",
        "        processed_file_path = os.path.join(test_data_dir, f'behaviors_{i}.tsv')\n",
        "        with open(processed_file_path, 'w') as f:\n",
        "            f.writelines(behaviors[i])\n",
        "\n",
        "    return sum([len(x) for x in behaviors])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VGYXTH1pMIBO"
      },
      "outputs": [],
      "source": [
        "def train(rank, args):\n",
        "\n",
        "    is_distributed = False\n",
        "    torch.cuda.set_device(rank)\n",
        "\n",
        "    news, news_index, category_dict, subcategory_dict, word_dict = read_news(\n",
        "        os.path.join(args.train_data_dir, 'news.tsv'), args, mode='train')\n",
        "\n",
        "    news_title, news_category, news_subcategory = get_doc_input(\n",
        "        news, news_index, category_dict, subcategory_dict, word_dict, args)\n",
        "    news_combined = np.concatenate([x for x in [news_title, news_category, news_subcategory] if x is not None], axis=-1)\n",
        "\n",
        "    if rank == 0:\n",
        "        logging.info('Initializing word embedding matrix...')\n",
        "\n",
        "    embedding_matrix, have_word = load_matrix(args.glove_embedding_path,\n",
        "                                                    word_dict,\n",
        "                                                    args.word_embedding_dim)\n",
        "    if rank == 0:\n",
        "        logging.info(f'Word dict length: {len(word_dict)}')\n",
        "        logging.info(f'Have words: {len(have_word)}')\n",
        "        logging.info(f'Missing rate: {(len(word_dict) - len(have_word)) / len(word_dict)}')\n",
        "\n",
        "    model = Model(args, embedding_matrix, len(category_dict), len(subcategory_dict))\n",
        "\n",
        "    if args.load_ckpt_name is not None:\n",
        "        ckpt_path = get_checkpoint(args.model_dir, args.load_ckpt_name)\n",
        "        checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        logging.info(f\"Model loaded from {ckpt_path}.\")\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    if args.enable_gpu:\n",
        "        model = model.cuda(rank)\n",
        "\n",
        "    if is_distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n",
        "\n",
        "    # if rank == 0:\n",
        "    #     print(model)\n",
        "    #     for name, param in model.named_parameters():\n",
        "    #         print(name, param.requires_grad)\n",
        "\n",
        "    data_file_path = os.path.join(args.train_data_dir, f'behaviors_np{args.npratio}_{rank}.tsv')\n",
        "\n",
        "    dataset = DatasetTrain(data_file_path, news_index, news_combined, args)\n",
        "    dataloader = DataLoader(dataset, batch_size=args.batch_size)\n",
        "\n",
        "    logging.info('Training...')\n",
        "    for ep in range(args.start_epoch, args.epochs):\n",
        "        loss = 0.0\n",
        "        accuary = 0.0\n",
        "        for cnt, (log_ids, log_mask, input_ids, targets) in enumerate(dataloader):\n",
        "            if args.enable_gpu:\n",
        "                log_ids = log_ids.cuda(rank, non_blocking=True)\n",
        "                log_mask = log_mask.cuda(rank, non_blocking=True)\n",
        "                input_ids = input_ids.cuda(rank, non_blocking=True)\n",
        "                targets = targets.cuda(rank, non_blocking=True)\n",
        "\n",
        "            bz_loss, y_hat = model(log_ids, log_mask, input_ids, targets)\n",
        "            loss += bz_loss.data.float()\n",
        "            accuary += acc(targets, y_hat)\n",
        "            optimizer.zero_grad()\n",
        "            bz_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if cnt % args.log_steps == 0:\n",
        "                logging.info(\n",
        "                    '[{}] Ed: {}, train_loss: {:.5f}, acc: {:.5f}'.format(\n",
        "                        rank, cnt * args.batch_size, loss.data / cnt, accuary / cnt)\n",
        "                )\n",
        "\n",
        "            if rank == 0 and     cnt != 0 and cnt % args.save_steps == 0:\n",
        "                ckpt_path = os.path.join(args.model_dir, f'epoch-{ep+1}-{cnt}.pt')\n",
        "                torch.save(\n",
        "                    {\n",
        "                        'model_state_dict':\n",
        "                            {'.'.join(k.split('.')[1:]): v for k, v in model.state_dict().items()}\n",
        "                            if is_distributed else model.state_dict(),\n",
        "                        'category_dict': category_dict,\n",
        "                        'word_dict': word_dict,\n",
        "                        'subcategory_dict': subcategory_dict\n",
        "                    }, ckpt_path)\n",
        "                logging.info(f\"Model saved to {ckpt_path}.\")\n",
        "\n",
        "        logging.info('Training finish.')\n",
        "\n",
        "        if rank == 0:\n",
        "            ckpt_path = os.path.join(args.model_dir, f'epoch-{ep+1}.pt')\n",
        "            torch.save(\n",
        "                {\n",
        "                    'model_state_dict':\n",
        "                        {'.'.join(k.split('.')[1:]): v for k, v in model.state_dict().items()}\n",
        "                        if is_distributed else model.state_dict(),\n",
        "                    'category_dict': category_dict,\n",
        "                    'subcategory_dict': subcategory_dict,\n",
        "                    'word_dict': word_dict,\n",
        "                }, ckpt_path)\n",
        "            logging.info(f\"Model saved to {ckpt_path}.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgkOVqFzS6Uv",
        "outputId": "7771d01b-bced-4516-9a5d-2652ad940369"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[batch_size]=32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,048] args[batch_size]=32\n",
            "[INFO 2024-12-03 17:09:07,048] args[batch_size]=32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[category_emb_dim]=100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,053] args[category_emb_dim]=100\n",
            "[INFO 2024-12-03 17:09:07,053] args[category_emb_dim]=100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[drop_rate]=0.2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,059] args[drop_rate]=0.2\n",
            "[INFO 2024-12-03 17:09:07,059] args[drop_rate]=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[enable_gpu]=True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,061] args[enable_gpu]=True\n",
            "[INFO 2024-12-03 17:09:07,061] args[enable_gpu]=True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[epochs]=5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,067] args[epochs]=5\n",
            "[INFO 2024-12-03 17:09:07,067] args[epochs]=5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[filter_num]=3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,070] args[filter_num]=3\n",
            "[INFO 2024-12-03 17:09:07,070] args[filter_num]=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[freeze_embedding]=False\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,075] args[freeze_embedding]=False\n",
            "[INFO 2024-12-03 17:09:07,075] args[freeze_embedding]=False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[glove_embedding_path]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/glove.840B.300d.txt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,080] args[glove_embedding_path]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/glove.840B.300d.txt\n",
            "[INFO 2024-12-03 17:09:07,080] args[glove_embedding_path]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/glove.840B.300d.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[load_ckpt_name]=None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,083] args[load_ckpt_name]=None\n",
            "[INFO 2024-12-03 17:09:07,083] args[load_ckpt_name]=None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[log_steps]=100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,089] args[log_steps]=100\n",
            "[INFO 2024-12-03 17:09:07,089] args[log_steps]=100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[lr]=0.0003\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,092] args[lr]=0.0003\n",
            "[INFO 2024-12-03 17:09:07,092] args[lr]=0.0003\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[mode]=train\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,094] args[mode]=train\n",
            "[INFO 2024-12-03 17:09:07,094] args[mode]=train\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[model_dir]=/content/model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,097] args[model_dir]=/content/model\n",
            "[INFO 2024-12-03 17:09:07,097] args[model_dir]=/content/model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[nGPU]=1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,100] args[nGPU]=1\n",
            "[INFO 2024-12-03 17:09:07,100] args[nGPU]=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[news_dim]=400\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,102] args[news_dim]=400\n",
            "[INFO 2024-12-03 17:09:07,102] args[news_dim]=400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[news_query_vector_dim]=200\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,106] args[news_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:09:07,106] args[news_query_vector_dim]=200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[npratio]=4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,109] args[npratio]=4\n",
            "[INFO 2024-12-03 17:09:07,109] args[npratio]=4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[num_attention_heads]=20\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,114] args[num_attention_heads]=20\n",
            "[INFO 2024-12-03 17:09:07,114] args[num_attention_heads]=20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[num_words_abstract]=50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,117] args[num_words_abstract]=50\n",
            "[INFO 2024-12-03 17:09:07,117] args[num_words_abstract]=50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[num_words_title]=20\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,121] args[num_words_title]=20\n",
            "[INFO 2024-12-03 17:09:07,121] args[num_words_title]=20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[prepare]=True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,125] args[prepare]=True\n",
            "[INFO 2024-12-03 17:09:07,125] args[prepare]=True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[save_steps]=10000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,128] args[save_steps]=10000\n",
            "[INFO 2024-12-03 17:09:07,128] args[save_steps]=10000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[seed]=0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,131] args[seed]=0\n",
            "[INFO 2024-12-03 17:09:07,131] args[seed]=0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[start_epoch]=0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,136] args[start_epoch]=0\n",
            "[INFO 2024-12-03 17:09:07,136] args[start_epoch]=0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[test_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_dev\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,138] args[test_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_dev\n",
            "[INFO 2024-12-03 17:09:07,138] args[test_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_dev\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[train_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_train\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,141] args[train_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_train\n",
            "[INFO 2024-12-03 17:09:07,141] args[train_data_dir]=/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/MINDsmall_train\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[use_category]=True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,145] args[use_category]=True\n",
            "[INFO 2024-12-03 17:09:07,145] args[use_category]=True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[use_subcategory]=True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,151] args[use_subcategory]=True\n",
            "[INFO 2024-12-03 17:09:07,151] args[use_subcategory]=True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[user_log_length]=50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,154] args[user_log_length]=50\n",
            "[INFO 2024-12-03 17:09:07,154] args[user_log_length]=50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[user_log_mask]=False\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,157] args[user_log_mask]=False\n",
            "[INFO 2024-12-03 17:09:07,157] args[user_log_mask]=False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[user_query_vector_dim]=200\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,163] args[user_query_vector_dim]=200\n",
            "[INFO 2024-12-03 17:09:07,163] args[user_query_vector_dim]=200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:args[word_embedding_dim]=300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:07,171] args[word_embedding_dim]=300\n",
            "[INFO 2024-12-03 17:09:07,171] args[word_embedding_dim]=300\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "setuplogger()\n",
        "args = parse_args()\n",
        "dump_args(args)\n",
        "random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "Path(args.model_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq_Df-xiThtr",
        "outputId": "1580fffd-06a3-46ac-9a01-18f865fbbf00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:Preparing training data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:11,296] Preparing training data...\n",
            "[INFO 2024-12-03 17:09:11,296] Preparing training data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "156965it [00:06, 23559.86it/s]\n",
            "INFO:root:Writing files...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:19,921] Writing files...\n",
            "[INFO 2024-12-03 17:09:19,921] Writing files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:236344 training samples, 7385 batches in total.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:09:25,918] 236344 training samples, 7385 batches in total.\n",
            "[INFO 2024-12-03 17:09:25,918] 236344 training samples, 7385 batches in total.\n"
          ]
        }
      ],
      "source": [
        "if 'train' in args.mode:\n",
        "    if args.prepare:\n",
        "        logging.info('Preparing training data...')\n",
        "        total_sample_num = prepare_training_data(args.train_data_dir, args.nGPU, args.npratio, args.seed)\n",
        "    else:\n",
        "        total_sample_num = 0\n",
        "        for i in range(args.nGPU):\n",
        "            data_file_path = os.path.join(args.train_data_dir, f'behaviors_np{args.npratio}_{i}.tsv')\n",
        "            if not os.path.exists(data_file_path):\n",
        "                logging.error(f'Splited training data {data_file_path} for GPU {i} does not exist. Please set the parameter --prepare as True and rerun the code.')\n",
        "                exit()\n",
        "            result = subprocess.getoutput(f'wc -l {data_file_path}')\n",
        "            total_sample_num += int(result.split(' ')[0])\n",
        "        logging.info('Skip training data preparation.')\n",
        "    logging.info(f'{total_sample_num} training samples, {total_sample_num // args.batch_size // args.nGPU} batches in total.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D8PfQOdZdUT",
        "outputId": "3420ad96-5900-4a6e-af8f-4f8d8d8cc0f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "51282it [00:08, 6221.36it/s]\n",
            "100%|██████████| 51282/51282 [00:00<00:00, 187361.98it/s]\n",
            "INFO:root:Initializing word embedding matrix...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:12:19,763] Initializing word embedding matrix...\n",
            "[INFO 2024-12-03 17:12:19,763] Initializing word embedding matrix...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:Word dict length: 12506\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:14:33,203] Word dict length: 12506\n",
            "[INFO 2024-12-03 17:14:33,203] Word dict length: 12506\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:Have words: 11947\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:14:33,211] Have words: 11947\n",
            "[INFO 2024-12-03 17:14:33,211] Have words: 11947\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:Missing rate: 0.0446985446985447\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:14:33,216] Missing rate: 0.0446985446985447\n",
            "[INFO 2024-12-03 17:14:33,216] Missing rate: 0.0446985446985447\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:Training...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:14:38,119] Training...\n",
            "[INFO 2024-12-03 17:14:38,119] Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 0, train_loss: inf, acc: inf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:14:40,776] [0] Ed: 0, train_loss: inf, acc: inf\n",
            "[INFO 2024-12-03 17:14:40,776] [0] Ed: 0, train_loss: inf, acc: inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 3200, train_loss: 1.67251, acc: 0.34875\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:14:44,358] [0] Ed: 3200, train_loss: 1.67251, acc: 0.34875\n",
            "[INFO 2024-12-03 17:14:44,358] [0] Ed: 3200, train_loss: 1.67251, acc: 0.34875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 6400, train_loss: 1.58799, acc: 0.35797\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:14:47,862] [0] Ed: 6400, train_loss: 1.58799, acc: 0.35797\n",
            "[INFO 2024-12-03 17:14:47,862] [0] Ed: 6400, train_loss: 1.58799, acc: 0.35797\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 9600, train_loss: 1.55419, acc: 0.36375\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:14:51,386] [0] Ed: 9600, train_loss: 1.55419, acc: 0.36375\n",
            "[INFO 2024-12-03 17:14:51,386] [0] Ed: 9600, train_loss: 1.55419, acc: 0.36375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 12800, train_loss: 1.53120, acc: 0.37141\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:14:54,921] [0] Ed: 12800, train_loss: 1.53120, acc: 0.37141\n",
            "[INFO 2024-12-03 17:14:54,921] [0] Ed: 12800, train_loss: 1.53120, acc: 0.37141\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 16000, train_loss: 1.51636, acc: 0.37538\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:14:58,472] [0] Ed: 16000, train_loss: 1.51636, acc: 0.37538\n",
            "[INFO 2024-12-03 17:14:58,472] [0] Ed: 16000, train_loss: 1.51636, acc: 0.37538\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 19200, train_loss: 1.50522, acc: 0.38094\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:15:02,034] [0] Ed: 19200, train_loss: 1.50522, acc: 0.38094\n",
            "[INFO 2024-12-03 17:15:02,034] [0] Ed: 19200, train_loss: 1.50522, acc: 0.38094\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 22400, train_loss: 1.49737, acc: 0.38424\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:15:05,613] [0] Ed: 22400, train_loss: 1.49737, acc: 0.38424\n",
            "[INFO 2024-12-03 17:15:05,613] [0] Ed: 22400, train_loss: 1.49737, acc: 0.38424\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 25600, train_loss: 1.49150, acc: 0.38754\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:15:09,207] [0] Ed: 25600, train_loss: 1.49150, acc: 0.38754\n",
            "[INFO 2024-12-03 17:15:09,207] [0] Ed: 25600, train_loss: 1.49150, acc: 0.38754\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 28800, train_loss: 1.48501, acc: 0.39104\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:15:12,807] [0] Ed: 28800, train_loss: 1.48501, acc: 0.39104\n",
            "[INFO 2024-12-03 17:15:12,807] [0] Ed: 28800, train_loss: 1.48501, acc: 0.39104\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 32000, train_loss: 1.48091, acc: 0.39350\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:15:16,430] [0] Ed: 32000, train_loss: 1.48091, acc: 0.39350\n",
            "[INFO 2024-12-03 17:15:16,430] [0] Ed: 32000, train_loss: 1.48091, acc: 0.39350\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 35200, train_loss: 1.47713, acc: 0.39557\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:15:20,068] [0] Ed: 35200, train_loss: 1.47713, acc: 0.39557\n",
            "[INFO 2024-12-03 17:15:20,068] [0] Ed: 35200, train_loss: 1.47713, acc: 0.39557\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 38400, train_loss: 1.47187, acc: 0.39755\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:15:23,712] [0] Ed: 38400, train_loss: 1.47187, acc: 0.39755\n",
            "[INFO 2024-12-03 17:15:23,712] [0] Ed: 38400, train_loss: 1.47187, acc: 0.39755\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 41600, train_loss: 1.46654, acc: 0.40096\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:15:27,378] [0] Ed: 41600, train_loss: 1.46654, acc: 0.40096\n",
            "[INFO 2024-12-03 17:15:27,378] [0] Ed: 41600, train_loss: 1.46654, acc: 0.40096\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 44800, train_loss: 1.46288, acc: 0.40355\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:15:31,060] [0] Ed: 44800, train_loss: 1.46288, acc: 0.40355\n",
            "[INFO 2024-12-03 17:15:31,060] [0] Ed: 44800, train_loss: 1.46288, acc: 0.40355\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 48000, train_loss: 1.45989, acc: 0.40460\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:15:34,757] [0] Ed: 48000, train_loss: 1.45989, acc: 0.40460\n",
            "[INFO 2024-12-03 17:15:34,757] [0] Ed: 48000, train_loss: 1.45989, acc: 0.40460\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 51200, train_loss: 1.45683, acc: 0.40559\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:15:38,466] [0] Ed: 51200, train_loss: 1.45683, acc: 0.40559\n",
            "[INFO 2024-12-03 17:15:38,466] [0] Ed: 51200, train_loss: 1.45683, acc: 0.40559\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 54400, train_loss: 1.45386, acc: 0.40728\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:15:42,186] [0] Ed: 54400, train_loss: 1.45386, acc: 0.40728\n",
            "[INFO 2024-12-03 17:15:42,186] [0] Ed: 54400, train_loss: 1.45386, acc: 0.40728\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 57600, train_loss: 1.45103, acc: 0.40875\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:15:45,925] [0] Ed: 57600, train_loss: 1.45103, acc: 0.40875\n",
            "[INFO 2024-12-03 17:15:45,925] [0] Ed: 57600, train_loss: 1.45103, acc: 0.40875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 60800, train_loss: 1.44864, acc: 0.40923\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:15:49,675] [0] Ed: 60800, train_loss: 1.44864, acc: 0.40923\n",
            "[INFO 2024-12-03 17:15:49,675] [0] Ed: 60800, train_loss: 1.44864, acc: 0.40923\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 64000, train_loss: 1.44558, acc: 0.41078\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:15:53,445] [0] Ed: 64000, train_loss: 1.44558, acc: 0.41078\n",
            "[INFO 2024-12-03 17:15:53,445] [0] Ed: 64000, train_loss: 1.44558, acc: 0.41078\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 67200, train_loss: 1.44281, acc: 0.41298\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:15:57,231] [0] Ed: 67200, train_loss: 1.44281, acc: 0.41298\n",
            "[INFO 2024-12-03 17:15:57,231] [0] Ed: 67200, train_loss: 1.44281, acc: 0.41298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 70400, train_loss: 1.44047, acc: 0.41391\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:16:01,040] [0] Ed: 70400, train_loss: 1.44047, acc: 0.41391\n",
            "[INFO 2024-12-03 17:16:01,040] [0] Ed: 70400, train_loss: 1.44047, acc: 0.41391\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 73600, train_loss: 1.43777, acc: 0.41522\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:16:04,864] [0] Ed: 73600, train_loss: 1.43777, acc: 0.41522\n",
            "[INFO 2024-12-03 17:16:04,864] [0] Ed: 73600, train_loss: 1.43777, acc: 0.41522\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 76800, train_loss: 1.43523, acc: 0.41673\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:16:08,701] [0] Ed: 76800, train_loss: 1.43523, acc: 0.41673\n",
            "[INFO 2024-12-03 17:16:08,701] [0] Ed: 76800, train_loss: 1.43523, acc: 0.41673\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 80000, train_loss: 1.43389, acc: 0.41719\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:16:12,564] [0] Ed: 80000, train_loss: 1.43389, acc: 0.41719\n",
            "[INFO 2024-12-03 17:16:12,564] [0] Ed: 80000, train_loss: 1.43389, acc: 0.41719\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 83200, train_loss: 1.43144, acc: 0.41840\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:16:16,446] [0] Ed: 83200, train_loss: 1.43144, acc: 0.41840\n",
            "[INFO 2024-12-03 17:16:16,446] [0] Ed: 83200, train_loss: 1.43144, acc: 0.41840\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] Ed: 86400, train_loss: 1.42970, acc: 0.41935\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-12-03 17:16:20,340] [0] Ed: 86400, train_loss: 1.42970, acc: 0.41935\n",
            "[INFO 2024-12-03 17:16:20,340] [0] Ed: 86400, train_loss: 1.42970, acc: 0.41935\n"
          ]
        }
      ],
      "source": [
        "train(0, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnZ9gUKYdGRM"
      },
      "outputs": [],
      "source": [
        "def test(rank, args):\n",
        "    torch.cuda.set_device(rank)\n",
        "\n",
        "    if args.load_ckpt_name is not None:\n",
        "        ckpt_path = get_checkpoint(args.model_dir, args.load_ckpt_name)\n",
        "\n",
        "    assert ckpt_path is not None, 'No checkpoint found.'\n",
        "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
        "\n",
        "    subcategory_dict = checkpoint['subcategory_dict']\n",
        "    category_dict = checkpoint['category_dict']\n",
        "    word_dict = checkpoint['word_dict']\n",
        "\n",
        "    dummy_embedding_matrix = np.zeros((len(word_dict) + 1, args.word_embedding_dim))\n",
        "    model = Model(args, dummy_embedding_matrix, len(category_dict), len(subcategory_dict))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    logging.info(f\"Model loaded from {ckpt_path}\")\n",
        "\n",
        "    if args.enable_gpu:\n",
        "        model.cuda(rank)\n",
        "\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    news, news_index = read_news(os.path.join(args.test_data_dir, 'news.tsv'), mode='test')\n",
        "    news_title, news_category, news_subcategory = to_num_input(\n",
        "        news, news_index, category_dict, subcategory_dict, word_dict)\n",
        "    news_combined = np.concatenate([x for x in [news_title, news_category, news_subcategory] if x is not None], axis=-1)\n",
        "\n",
        "    news_dataset = NewsDataset(news_combined)\n",
        "    news_dataloader = DataLoader(news_dataset,\n",
        "                                 batch_size=args.batch_size,\n",
        "                                 num_workers=4)\n",
        "\n",
        "    news_scoring = []\n",
        "    with torch.no_grad():\n",
        "        for input_ids in tqdm(news_dataloader):\n",
        "            input_ids = input_ids.cuda(rank)\n",
        "            news_vec = model.news_encoder(input_ids)\n",
        "            news_vec = news_vec.to(torch.device(\"cpu\")).detach().numpy()\n",
        "            news_scoring.extend(news_vec)\n",
        "\n",
        "    news_scoring = np.array(news_scoring)\n",
        "    logging.info(\"news scoring num: {}\".format(news_scoring.shape[0]))\n",
        "\n",
        "    if rank == 0:\n",
        "        doc_sim = 0\n",
        "        for _ in tqdm(range(1000000)):\n",
        "            i = random.randrange(1, len(news_scoring))\n",
        "            j = random.randrange(1, len(news_scoring))\n",
        "            if i != j:\n",
        "                doc_sim += np.dot(news_scoring[i], news_scoring[j]) / (np.linalg.norm(news_scoring[i]) * np.linalg.norm(news_scoring[j]))\n",
        "        logging.info(f'News doc-sim: {doc_sim / 1000000}')\n",
        "\n",
        "    data_file_path = os.path.join(args.test_data_dir, f'behaviors_{rank}.tsv')\n",
        "\n",
        "    def collate_fn(tuple_list):\n",
        "        log_vecs = torch.FloatTensor([x[0] for x in tuple_list])\n",
        "        log_mask = torch.FloatTensor([x[1] for x in tuple_list])\n",
        "        news_vecs = [x[2] for x in tuple_list]\n",
        "        labels = [x[3] for x in tuple_list]\n",
        "        return (log_vecs, log_mask, news_vecs, labels)\n",
        "\n",
        "    dataset = DatasetTest(data_file_path, news_index, news_scoring, args)\n",
        "    dataloader = DataLoader(dataset, batch_size=args.batch_size, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "    AUC = []\n",
        "    MRR = []\n",
        "    nDCG5 = []\n",
        "    nDCG10 = []\n",
        "\n",
        "    def print_metrics(rank, cnt, x):\n",
        "        logging.info(\"[{}] {} samples: {}\".format(rank, cnt, '\\t'.join([\"{:0.2f}\".format(i * 100) for i in x])))\n",
        "\n",
        "    def get_mean(arr):\n",
        "        return [np.array(i).mean() for i in arr]\n",
        "\n",
        "    def get_sum(arr):\n",
        "        return [np.array(i).sum() for i in arr]\n",
        "\n",
        "    local_sample_num = 0\n",
        "\n",
        "    for cnt, (log_vecs, log_mask, news_vecs, labels) in enumerate(dataloader):\n",
        "        local_sample_num += log_vecs.shape[0]\n",
        "\n",
        "        if args.enable_gpu:\n",
        "            log_vecs = log_vecs.cuda(rank, non_blocking=True)\n",
        "            log_mask = log_mask.cuda(rank, non_blocking=True)\n",
        "\n",
        "        user_vecs = model.user_encoder(log_vecs, log_mask).to(torch.device(\"cpu\")).detach().numpy()\n",
        "\n",
        "        for user_vec, news_vec, label in zip(user_vecs, news_vecs, labels):\n",
        "            if label.mean() == 0 or label.mean() == 1:\n",
        "                continue\n",
        "\n",
        "            score = np.dot(news_vec, user_vec)\n",
        "\n",
        "            auc = roc_auc_score(label, score)\n",
        "            mrr = mrr_score(label, score)\n",
        "            ndcg5 = ndcg_score(label, score, k=5)\n",
        "            ndcg10 = ndcg_score(label, score, k=10)\n",
        "\n",
        "            AUC.append(auc)\n",
        "            MRR.append(mrr)\n",
        "            nDCG5.append(ndcg5)\n",
        "            nDCG10.append(ndcg10)\n",
        "\n",
        "        if cnt % args.log_steps == 0:\n",
        "            print_metrics(rank, local_sample_num, get_mean([AUC, MRR, nDCG5, nDCG10]))\n",
        "\n",
        "    logging.info('[{}] local_sample_num: {}'.format(rank, local_sample_num))\n",
        "    print_metrics('*', local_sample_num, get_mean([AUC, MRR, nDCG5, nDCG10]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaJL9wmsUIoy",
        "outputId": "2cbada9c-f094-4702-c5f7-4f86f87e5159"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:Preparing testing data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-09-27 17:12:23,902] Preparing testing data...\n",
            "[INFO 2024-09-27 17:12:23,902] Preparing testing data...\n",
            "[INFO 2024-09-27 17:12:23,902] Preparing testing data...\n",
            "[INFO 2024-09-27 17:12:23,902] Preparing testing data...\n",
            "[INFO 2024-09-27 17:12:23,902] Preparing testing data...\n",
            "[INFO 2024-09-27 17:12:23,902] Preparing testing data...\n",
            "[INFO 2024-09-27 17:12:23,902] Preparing testing data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "73152it [00:00, 1164625.40it/s]\n",
            "INFO:root:Writing files...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-09-27 17:12:23,983] Writing files...\n",
            "[INFO 2024-09-27 17:12:23,983] Writing files...\n",
            "[INFO 2024-09-27 17:12:23,983] Writing files...\n",
            "[INFO 2024-09-27 17:12:23,983] Writing files...\n",
            "[INFO 2024-09-27 17:12:23,983] Writing files...\n",
            "[INFO 2024-09-27 17:12:23,983] Writing files...\n",
            "[INFO 2024-09-27 17:12:23,983] Writing files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:73152 testing samples in total.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-09-27 17:12:24,075] 73152 testing samples in total.\n",
            "[INFO 2024-09-27 17:12:24,075] 73152 testing samples in total.\n",
            "[INFO 2024-09-27 17:12:24,075] 73152 testing samples in total.\n",
            "[INFO 2024-09-27 17:12:24,075] 73152 testing samples in total.\n",
            "[INFO 2024-09-27 17:12:24,075] 73152 testing samples in total.\n",
            "[INFO 2024-09-27 17:12:24,075] 73152 testing samples in total.\n",
            "[INFO 2024-09-27 17:12:24,075] 73152 testing samples in total.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-38-edcac83fca55>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
            "INFO:root:Model loaded from /content/model/epoch-6.pt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-09-27 17:12:24,119] Model loaded from /content/model/epoch-6.pt\n",
            "[INFO 2024-09-27 17:12:24,119] Model loaded from /content/model/epoch-6.pt\n",
            "[INFO 2024-09-27 17:12:24,119] Model loaded from /content/model/epoch-6.pt\n",
            "[INFO 2024-09-27 17:12:24,119] Model loaded from /content/model/epoch-6.pt\n",
            "[INFO 2024-09-27 17:12:24,119] Model loaded from /content/model/epoch-6.pt\n",
            "[INFO 2024-09-27 17:12:24,119] Model loaded from /content/model/epoch-6.pt\n",
            "[INFO 2024-09-27 17:12:24,119] Model loaded from /content/model/epoch-6.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "42416it [00:03, 11140.80it/s]\n",
            "100%|██████████| 42416/42416 [00:00<00:00, 186874.65it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|██████████| 332/332 [00:01<00:00, 286.00it/s]\n",
            "INFO:root:news scoring num: 42417\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-09-27 17:12:29,441] news scoring num: 42417\n",
            "[INFO 2024-09-27 17:12:29,441] news scoring num: 42417\n",
            "[INFO 2024-09-27 17:12:29,441] news scoring num: 42417\n",
            "[INFO 2024-09-27 17:12:29,441] news scoring num: 42417\n",
            "[INFO 2024-09-27 17:12:29,441] news scoring num: 42417\n",
            "[INFO 2024-09-27 17:12:29,441] news scoring num: 42417\n",
            "[INFO 2024-09-27 17:12:29,441] news scoring num: 42417\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000000/1000000 [00:18<00:00, 52936.60it/s]\n",
            "INFO:root:News doc-sim: 0.15279813125327654\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-09-27 17:12:48,350] News doc-sim: 0.15279813125327654\n",
            "[INFO 2024-09-27 17:12:48,350] News doc-sim: 0.15279813125327654\n",
            "[INFO 2024-09-27 17:12:48,350] News doc-sim: 0.15279813125327654\n",
            "[INFO 2024-09-27 17:12:48,350] News doc-sim: 0.15279813125327654\n",
            "[INFO 2024-09-27 17:12:48,350] News doc-sim: 0.15279813125327654\n",
            "[INFO 2024-09-27 17:12:48,350] News doc-sim: 0.15279813125327654\n",
            "[INFO 2024-09-27 17:12:48,350] News doc-sim: 0.15279813125327654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-38-edcac83fca55>:58: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  log_vecs = torch.FloatTensor([x[0] for x in tuple_list])\n",
            "INFO:root:[0] 128 samples: 67.26\t32.59\t34.54\t41.83\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-09-27 17:12:49,690] [0] 128 samples: 67.26\t32.59\t34.54\t41.83\n",
            "[INFO 2024-09-27 17:12:49,690] [0] 128 samples: 67.26\t32.59\t34.54\t41.83\n",
            "[INFO 2024-09-27 17:12:49,690] [0] 128 samples: 67.26\t32.59\t34.54\t41.83\n",
            "[INFO 2024-09-27 17:12:49,690] [0] 128 samples: 67.26\t32.59\t34.54\t41.83\n",
            "[INFO 2024-09-27 17:12:49,690] [0] 128 samples: 67.26\t32.59\t34.54\t41.83\n",
            "[INFO 2024-09-27 17:12:49,690] [0] 128 samples: 67.26\t32.59\t34.54\t41.83\n",
            "[INFO 2024-09-27 17:12:49,690] [0] 128 samples: 67.26\t32.59\t34.54\t41.83\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] 12928 samples: 66.37\t31.78\t35.00\t41.21\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-09-27 17:14:35,372] [0] 12928 samples: 66.37\t31.78\t35.00\t41.21\n",
            "[INFO 2024-09-27 17:14:35,372] [0] 12928 samples: 66.37\t31.78\t35.00\t41.21\n",
            "[INFO 2024-09-27 17:14:35,372] [0] 12928 samples: 66.37\t31.78\t35.00\t41.21\n",
            "[INFO 2024-09-27 17:14:35,372] [0] 12928 samples: 66.37\t31.78\t35.00\t41.21\n",
            "[INFO 2024-09-27 17:14:35,372] [0] 12928 samples: 66.37\t31.78\t35.00\t41.21\n",
            "[INFO 2024-09-27 17:14:35,372] [0] 12928 samples: 66.37\t31.78\t35.00\t41.21\n",
            "[INFO 2024-09-27 17:14:35,372] [0] 12928 samples: 66.37\t31.78\t35.00\t41.21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] 25728 samples: 66.23\t31.92\t35.07\t41.29\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-09-27 17:16:21,275] [0] 25728 samples: 66.23\t31.92\t35.07\t41.29\n",
            "[INFO 2024-09-27 17:16:21,275] [0] 25728 samples: 66.23\t31.92\t35.07\t41.29\n",
            "[INFO 2024-09-27 17:16:21,275] [0] 25728 samples: 66.23\t31.92\t35.07\t41.29\n",
            "[INFO 2024-09-27 17:16:21,275] [0] 25728 samples: 66.23\t31.92\t35.07\t41.29\n",
            "[INFO 2024-09-27 17:16:21,275] [0] 25728 samples: 66.23\t31.92\t35.07\t41.29\n",
            "[INFO 2024-09-27 17:16:21,275] [0] 25728 samples: 66.23\t31.92\t35.07\t41.29\n",
            "[INFO 2024-09-27 17:16:21,275] [0] 25728 samples: 66.23\t31.92\t35.07\t41.29\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] 38528 samples: 66.15\t31.74\t34.91\t41.14\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-09-27 17:18:04,494] [0] 38528 samples: 66.15\t31.74\t34.91\t41.14\n",
            "[INFO 2024-09-27 17:18:04,494] [0] 38528 samples: 66.15\t31.74\t34.91\t41.14\n",
            "[INFO 2024-09-27 17:18:04,494] [0] 38528 samples: 66.15\t31.74\t34.91\t41.14\n",
            "[INFO 2024-09-27 17:18:04,494] [0] 38528 samples: 66.15\t31.74\t34.91\t41.14\n",
            "[INFO 2024-09-27 17:18:04,494] [0] 38528 samples: 66.15\t31.74\t34.91\t41.14\n",
            "[INFO 2024-09-27 17:18:04,494] [0] 38528 samples: 66.15\t31.74\t34.91\t41.14\n",
            "[INFO 2024-09-27 17:18:04,494] [0] 38528 samples: 66.15\t31.74\t34.91\t41.14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] 51328 samples: 66.10\t31.55\t34.68\t40.95\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-09-27 17:19:49,479] [0] 51328 samples: 66.10\t31.55\t34.68\t40.95\n",
            "[INFO 2024-09-27 17:19:49,479] [0] 51328 samples: 66.10\t31.55\t34.68\t40.95\n",
            "[INFO 2024-09-27 17:19:49,479] [0] 51328 samples: 66.10\t31.55\t34.68\t40.95\n",
            "[INFO 2024-09-27 17:19:49,479] [0] 51328 samples: 66.10\t31.55\t34.68\t40.95\n",
            "[INFO 2024-09-27 17:19:49,479] [0] 51328 samples: 66.10\t31.55\t34.68\t40.95\n",
            "[INFO 2024-09-27 17:19:49,479] [0] 51328 samples: 66.10\t31.55\t34.68\t40.95\n",
            "[INFO 2024-09-27 17:19:49,479] [0] 51328 samples: 66.10\t31.55\t34.68\t40.95\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] 64128 samples: 66.09\t31.59\t34.73\t41.00\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-09-27 17:21:33,301] [0] 64128 samples: 66.09\t31.59\t34.73\t41.00\n",
            "[INFO 2024-09-27 17:21:33,301] [0] 64128 samples: 66.09\t31.59\t34.73\t41.00\n",
            "[INFO 2024-09-27 17:21:33,301] [0] 64128 samples: 66.09\t31.59\t34.73\t41.00\n",
            "[INFO 2024-09-27 17:21:33,301] [0] 64128 samples: 66.09\t31.59\t34.73\t41.00\n",
            "[INFO 2024-09-27 17:21:33,301] [0] 64128 samples: 66.09\t31.59\t34.73\t41.00\n",
            "[INFO 2024-09-27 17:21:33,301] [0] 64128 samples: 66.09\t31.59\t34.73\t41.00\n",
            "[INFO 2024-09-27 17:21:33,301] [0] 64128 samples: 66.09\t31.59\t34.73\t41.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[0] local_sample_num: 73152\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-09-27 17:22:46,772] [0] local_sample_num: 73152\n",
            "[INFO 2024-09-27 17:22:46,772] [0] local_sample_num: 73152\n",
            "[INFO 2024-09-27 17:22:46,772] [0] local_sample_num: 73152\n",
            "[INFO 2024-09-27 17:22:46,772] [0] local_sample_num: 73152\n",
            "[INFO 2024-09-27 17:22:46,772] [0] local_sample_num: 73152\n",
            "[INFO 2024-09-27 17:22:46,772] [0] local_sample_num: 73152\n",
            "[INFO 2024-09-27 17:22:46,772] [0] local_sample_num: 73152\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[*] 73152 samples: 66.08\t31.60\t34.75\t41.02\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2024-09-27 17:22:46,859] [*] 73152 samples: 66.08\t31.60\t34.75\t41.02\n",
            "[INFO 2024-09-27 17:22:46,859] [*] 73152 samples: 66.08\t31.60\t34.75\t41.02\n",
            "[INFO 2024-09-27 17:22:46,859] [*] 73152 samples: 66.08\t31.60\t34.75\t41.02\n",
            "[INFO 2024-09-27 17:22:46,859] [*] 73152 samples: 66.08\t31.60\t34.75\t41.02\n",
            "[INFO 2024-09-27 17:22:46,859] [*] 73152 samples: 66.08\t31.60\t34.75\t41.02\n",
            "[INFO 2024-09-27 17:22:46,859] [*] 73152 samples: 66.08\t31.60\t34.75\t41.02\n",
            "[INFO 2024-09-27 17:22:46,859] [*] 73152 samples: 66.08\t31.60\t34.75\t41.02\n"
          ]
        }
      ],
      "source": [
        "args.mode = 'test'\n",
        "args.user_log_mask=True\n",
        "args.batch_size=128\n",
        "args.load_ckpt_name= 'epoch-6.pt'\n",
        "args.prepare=True\n",
        "if 'test' in args.mode:\n",
        "        if args.prepare:\n",
        "            logging.info('Preparing testing data...')\n",
        "            total_sample_num = prepare_testing_data(args.test_data_dir, args.nGPU)\n",
        "        else:\n",
        "            total_sample_num = 0\n",
        "            for i in range(args.nGPU):\n",
        "                data_file_path = os.path.join(args.test_data_dir, f'behaviors_{i}.tsv')\n",
        "                if not os.path.exists(data_file_path):\n",
        "                    logging.error(f'Splited testing data {data_file_path} for GPU {i} does not exist. Please set the parameter --prepare as True and rerun the code.')\n",
        "                    exit()\n",
        "                result = subprocess.getoutput(f'wc -l {data_file_path}')\n",
        "                total_sample_num += int(result.split(' ')[0])\n",
        "            logging.info('Skip testing data preparation.')\n",
        "        logging.info(f'{total_sample_num} testing samples in total.')\n",
        "\n",
        "        test(0, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nltxAifrdySu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
