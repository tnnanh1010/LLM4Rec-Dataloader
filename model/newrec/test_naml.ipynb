{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO 2025-02-08 20:51:24,970] args[batch_size]=32\n",
      "[INFO 2025-02-08 20:51:24,970] args[batch_size]=32\n",
      "[INFO 2025-02-08 20:51:24,971] args[category_emb_dim]=100\n",
      "[INFO 2025-02-08 20:51:24,971] args[category_emb_dim]=100\n",
      "[INFO 2025-02-08 20:51:24,973] args[custom_abstract_dir]=\n",
      "[INFO 2025-02-08 20:51:24,973] args[custom_abstract_dir]=\n",
      "[INFO 2025-02-08 20:51:24,974] args[drop_rate]=0.2\n",
      "[INFO 2025-02-08 20:51:24,974] args[drop_rate]=0.2\n",
      "[INFO 2025-02-08 20:51:24,976] args[enable_gpu]=True\n",
      "[INFO 2025-02-08 20:51:24,976] args[enable_gpu]=True\n",
      "[INFO 2025-02-08 20:51:24,976] args[epochs]=5\n",
      "[INFO 2025-02-08 20:51:24,976] args[epochs]=5\n",
      "[INFO 2025-02-08 20:51:24,977] args[filter_num]=3\n",
      "[INFO 2025-02-08 20:51:24,977] args[filter_num]=3\n",
      "[INFO 2025-02-08 20:51:24,978] args[freeze_embedding]=False\n",
      "[INFO 2025-02-08 20:51:24,978] args[freeze_embedding]=False\n",
      "[INFO 2025-02-08 20:51:24,979] args[glove_embedding_path]=/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/glove.840B.300d.txt\n",
      "[INFO 2025-02-08 20:51:24,979] args[glove_embedding_path]=/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/glove.840B.300d.txt\n",
      "[INFO 2025-02-08 20:51:24,980] args[load_ckpt_name]=None\n",
      "[INFO 2025-02-08 20:51:24,980] args[load_ckpt_name]=None\n",
      "[INFO 2025-02-08 20:51:24,981] args[log_steps]=100\n",
      "[INFO 2025-02-08 20:51:24,981] args[log_steps]=100\n",
      "[INFO 2025-02-08 20:51:24,982] args[lr]=0.0003\n",
      "[INFO 2025-02-08 20:51:24,982] args[lr]=0.0003\n",
      "[INFO 2025-02-08 20:51:24,983] args[mode]=train\n",
      "[INFO 2025-02-08 20:51:24,983] args[mode]=train\n",
      "[INFO 2025-02-08 20:51:24,983] args[model_dir]=/content/model\n",
      "[INFO 2025-02-08 20:51:24,983] args[model_dir]=/content/model\n",
      "[INFO 2025-02-08 20:51:24,984] args[nGPU]=1\n",
      "[INFO 2025-02-08 20:51:24,984] args[nGPU]=1\n",
      "[INFO 2025-02-08 20:51:24,985] args[news_dim]=400\n",
      "[INFO 2025-02-08 20:51:24,985] args[news_dim]=400\n",
      "[INFO 2025-02-08 20:51:24,986] args[news_query_vector_dim]=200\n",
      "[INFO 2025-02-08 20:51:24,986] args[news_query_vector_dim]=200\n",
      "[INFO 2025-02-08 20:51:24,987] args[npratio]=4\n",
      "[INFO 2025-02-08 20:51:24,987] args[npratio]=4\n",
      "[INFO 2025-02-08 20:51:24,987] args[num_attention_heads]=20\n",
      "[INFO 2025-02-08 20:51:24,987] args[num_attention_heads]=20\n",
      "[INFO 2025-02-08 20:51:24,988] args[num_words_abstract]=50\n",
      "[INFO 2025-02-08 20:51:24,988] args[num_words_abstract]=50\n",
      "[INFO 2025-02-08 20:51:24,989] args[num_words_title]=20\n",
      "[INFO 2025-02-08 20:51:24,989] args[num_words_title]=20\n",
      "[INFO 2025-02-08 20:51:24,990] args[prepare]=True\n",
      "[INFO 2025-02-08 20:51:24,990] args[prepare]=True\n",
      "[INFO 2025-02-08 20:51:24,991] args[save_steps]=10000\n",
      "[INFO 2025-02-08 20:51:24,991] args[save_steps]=10000\n",
      "[INFO 2025-02-08 20:51:24,992] args[seed]=0\n",
      "[INFO 2025-02-08 20:51:24,992] args[seed]=0\n",
      "[INFO 2025-02-08 20:51:24,992] args[start_epoch]=0\n",
      "[INFO 2025-02-08 20:51:24,992] args[start_epoch]=0\n",
      "[INFO 2025-02-08 20:51:24,993] args[test_data_dir]=/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/MINDsmall_dev\n",
      "[INFO 2025-02-08 20:51:24,993] args[test_data_dir]=/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/MINDsmall_dev\n",
      "[INFO 2025-02-08 20:51:24,994] args[train_data_dir]=/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/MINDsmall_train\n",
      "[INFO 2025-02-08 20:51:24,994] args[train_data_dir]=/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/MINDsmall_train\n",
      "[INFO 2025-02-08 20:51:24,996] args[use_abstract]=True\n",
      "[INFO 2025-02-08 20:51:24,996] args[use_abstract]=True\n",
      "[INFO 2025-02-08 20:51:24,997] args[use_category]=True\n",
      "[INFO 2025-02-08 20:51:24,997] args[use_category]=True\n",
      "[INFO 2025-02-08 20:51:24,997] args[use_custom_abstract]=False\n",
      "[INFO 2025-02-08 20:51:24,997] args[use_custom_abstract]=False\n",
      "[INFO 2025-02-08 20:51:24,998] args[use_subcategory]=True\n",
      "[INFO 2025-02-08 20:51:24,998] args[use_subcategory]=True\n",
      "[INFO 2025-02-08 20:51:24,998] args[user_log_length]=50\n",
      "[INFO 2025-02-08 20:51:24,998] args[user_log_length]=50\n",
      "[INFO 2025-02-08 20:51:24,999] args[user_log_mask]=False\n",
      "[INFO 2025-02-08 20:51:24,999] args[user_log_mask]=False\n",
      "[INFO 2025-02-08 20:51:25,000] args[user_query_vector_dim]=200\n",
      "[INFO 2025-02-08 20:51:25,000] args[user_query_vector_dim]=200\n",
      "[INFO 2025-02-08 20:51:25,001] args[word_embedding_dim]=300\n",
      "[INFO 2025-02-08 20:51:25,001] args[word_embedding_dim]=300\n",
      "[INFO 2025-02-08 20:51:25,009] Preparing training data...\n",
      "[INFO 2025-02-08 20:51:25,009] Preparing training data...\n"
     ]
    }
   ],
   "source": [
    "from reader import Reader\n",
    "from dataloader import DatasetTrain, DatasetTest, NewsDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from argument import parse_naml_args\n",
    "from log import dump_args, setuplogger, get_checkpoint\n",
    "import random\n",
    "import logging\n",
    "import torch.optim as optim\n",
    "from model.naml import Model\n",
    "setuplogger()\n",
    "args = parse_naml_args()\n",
    "dump_args(args)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "\n",
    "reader = Reader(args)\n",
    "logging.info('Preparing training data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51282it [00:03, 16823.75it/s]\n",
      "156965it [00:02, 69104.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO 2025-02-08 19:57:50,159] Writing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73152it [00:00, 1073430.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO 2025-02-08 19:57:50,487] Writing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if (args.use_custom_abstract):\n",
    "      custom_abstract_df = pd.read_csv(args.custom_abstract_dir)\n",
    "      custom_abstract_dict = custom_abstract_df.set_index('news_id')['abstract'].to_dict()\n",
    "      news, news_index, category_dict, subcategory_dict, word_cnt = reader.read_custom_abstract(\n",
    "          os.path.join(args.train_data_dir, 'news.tsv'), custom_abstract_dict)\n",
    "else:    \n",
    "      news, news_index, category_dict, subcategory_dict, word_dict = reader.read_news(\n",
    "      os.path.join(args.train_data_dir, 'news.tsv'), args, mode='train')\n",
    "num_training_behaviors = reader.prepare_training_data(args.train_data_dir)\n",
    "num_testing_behaviors = reader.prepare_testing_data(args.test_data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51282/51282 [00:00<00:00, 63255.84it/s]\n"
     ]
    }
   ],
   "source": [
    "news_title, news_category, news_subcategory, news_abstract = reader.get_doc_input(\n",
    "        news, news_index, category_dict, subcategory_dict, word_dict, args)\n",
    "news_combined = np.concatenate([x for x in [news_title, news_category, news_subcategory, news_abstract] if x is not None], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     0,     0,     0],\n",
       "       [    1,     2,     3, ...,  1770,  1434,     0],\n",
       "       [   12,    13,    14, ...,     0,  7578,    46],\n",
       "       ...,\n",
       "       [  586,     0,  3536, ...,     0,  3215, 10852],\n",
       "       [   33,     1,  7140, ...,    28,  1915,     0],\n",
       "       [   76,  1137,   407, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO 2025-02-08 19:57:51,408] Initializing word embedding matrix...\n",
      "[INFO 2025-02-08 19:58:26,051] Word dict length: 12519\n",
      "[INFO 2025-02-08 19:58:26,052] Have words: 11960\n",
      "[INFO 2025-02-08 19:58:26,053] Missing rate: 0.0446521287642783\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.info('Initializing word embedding matrix...')\n",
    "\n",
    "embedding_matrix, have_word = reader.load_matrix(args.glove_embedding_path,\n",
    "                                            word_dict,\n",
    "                                            args.word_embedding_dim)\n",
    "logging.info(f'Word dict length: {len(word_dict)}')\n",
    "logging.info(f'Have words: {len(have_word)}')\n",
    "logging.info(f'Missing rate: {(len(word_dict) - len(have_word)) / len(word_dict)}')\n",
    "\n",
    "model = Model(args, embedding_matrix, len(category_dict), len(subcategory_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.load_ckpt_name is not None:\n",
    "        ckpt_path = get_checkpoint(args.model_dir, args.load_ckpt_name)\n",
    "        checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        logging.info(f\"Model loaded from {ckpt_path}.\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# if args.enable_gpu:\n",
    "#     model = model.cuda(0)\n",
    "\n",
    "# if rank == 0:\n",
    "#     print(model)\n",
    "#     for name, param in model.named_parameters():\n",
    "#         print(name, param.requires_grad)\n",
    "\n",
    "data_file_path = os.path.join(args.train_data_dir, f'behaviors_np{args.npratio}_{0}.tsv')\n",
    "\n",
    "dataset = DatasetTrain(data_file_path, news_index, news_combined, args)\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader') \n",
    "\n",
    "# Import the metrics\n",
    "from metric import acc, auc_score, ndcg_score, mrr_score, ctr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO 2025-02-08 20:16:34,443] Training...\n",
      "[INFO 2025-02-08 20:16:37,812] [0] Ed: 0, train_loss: inf, acc: inf\n",
      "[INFO 2025-02-08 20:24:07,513] [0] Ed: 3200, train_loss: 1.59379, acc: 0.34156\n",
      "[INFO 2025-02-08 20:32:05,838] [0] Ed: 6400, train_loss: 1.54577, acc: 0.35703\n",
      "[INFO 2025-02-08 20:40:10,140] [0] Ed: 9600, train_loss: 1.52555, acc: 0.36542\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m accuary \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc(targets, y_hat)\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m \u001b[43mbz_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cnt \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39mlog_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "rank = 0\n",
    "logging.info('Training...')\n",
    "for ep in range(args.start_epoch, args.epochs):\n",
    "    loss = 0.0\n",
    "    accuary = 0.0\n",
    "    for cnt, (log_ids, log_mask, input_ids, targets) in enumerate(dataloader):\n",
    "        # if args.enable_gpu:\n",
    "        #     log_ids = log_ids.cuda(rank, non_blocking=True)\n",
    "        #     log_mask = log_mask.cuda(rank, non_blocking=True)\n",
    "        #     input_ids = input_ids.cuda(rank, non_blocking=True)\n",
    "        #     targets = targets.cuda(rank, non_blocking=True)\n",
    "\n",
    "        bz_loss, y_hat = model(log_ids, log_mask, input_ids, targets)\n",
    "        loss += bz_loss.data.float()\n",
    "        accuary += acc(targets, y_hat)\n",
    "        optimizer.zero_grad()\n",
    "        bz_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if cnt % args.log_steps == 0:\n",
    "            logging.info(\n",
    "                '[{}] Ed: {}, train_loss: {:.5f}, acc: {:.5f}'.format(\n",
    "                    rank, cnt * args.batch_size, loss.data / cnt, accuary / cnt)\n",
    "            )\n",
    "\n",
    "        if rank == 0 and     cnt != 0 and cnt % args.save_steps == 0:\n",
    "            ckpt_path = os.path.join(args.model_dir, f'epoch-{ep+1}-{cnt}.pt')\n",
    "            torch.save(\n",
    "                {\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'category_dict': category_dict,\n",
    "                    'word_dict': word_dict,\n",
    "                    'subcategory_dict': subcategory_dict\n",
    "                }, ckpt_path)\n",
    "            logging.info(f\"Model saved to {ckpt_path}.\")\n",
    "\n",
    "    logging.info('Training finish.')\n",
    "\n",
    "    if rank == 0:\n",
    "        ckpt_path = os.path.join(args.model_dir, f'epoch-{ep+1}.pt')\n",
    "        torch.save(\n",
    "            {\n",
    "                'model_state_dict':model.state_dict(),\n",
    "                'category_dict': category_dict,\n",
    "                'subcategory_dict': subcategory_dict,\n",
    "                'word_dict': word_dict,\n",
    "            }, ckpt_path)\n",
    "        logging.info(f\"Model saved to {ckpt_path}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73152it [00:00, 657307.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO 2025-02-08 20:52:52,304] Writing files...\n",
      "[INFO 2025-02-08 20:52:52,304] Writing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args.mode = 'test'\n",
    "args.user_log_mask=True\n",
    "args.batch_size=128\n",
    "args.load_ckpt_name= 'epoch-5.pt'\n",
    "args.prepare=True\n",
    "total_sample_num = reader.prepare_testing_data(args.test_data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "No checkpoint found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mload_ckpt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     ckpt_path \u001b[38;5;241m=\u001b[39m get_checkpoint(args\u001b[38;5;241m.\u001b[39mmodel_dir, args\u001b[38;5;241m.\u001b[39mload_ckpt_name)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m ckpt_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo checkpoint found.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(ckpt_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m subcategory_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubcategory_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: No checkpoint found."
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "# torch.cuda.set_device(rank)\n",
    "\n",
    "if args.load_ckpt_name is not None:\n",
    "    ckpt_path = get_checkpoint(args.model_dir, args.load_ckpt_name)\n",
    "\n",
    "assert ckpt_path is not None, 'No checkpoint found.'\n",
    "checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "\n",
    "subcategory_dict = checkpoint['subcategory_dict']\n",
    "category_dict = checkpoint['category_dict']\n",
    "word_dict = checkpoint['word_dict']\n",
    "\n",
    "dummy_embedding_matrix = np.zeros((len(word_dict) + 1, args.word_embedding_dim))\n",
    "model = Model(args, dummy_embedding_matrix, len(category_dict), len(subcategory_dict))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "logging.info(f\"Model loaded from {ckpt_path}\")\n",
    "\n",
    "if args.enable_gpu:\n",
    "    model.cuda(rank)\n",
    "\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "if (args.use_custom_abstract):\n",
    "    custom_abstract_df = pd.read_csv(args.custom_abstract_dir)\n",
    "    custom_abstract_dict = custom_abstract_df.set_index('news_id')['abstract'].to_dict()\n",
    "    news, news_index, category_dict, subcategory_dict, word_cnt = reader.read_custom_abstract(\n",
    "        os.path.join(args.train_data_dir, 'news.tsv'), custom_abstract_dict)\n",
    "else:\n",
    "    news, news_index, category_dict, subcategory_dict, word_dict = reader.read_news(\n",
    "        os.path.join(args.train_data_dir, 'news.tsv'), args, mode='train')\n",
    "news_title, news_category, news_subcategory, news_abstract = reader.get_doc_input(\n",
    "    news, news_index, category_dict, subcategory_dict, word_dict, args)\n",
    "news_combined = np.concatenate([x for x in [news_title, news_category, news_subcategory, news_abstract] if x is not None], axis=-1)\n",
    "\n",
    "news_dataset = NewsDataset(news_combined)\n",
    "news_dataloader = DataLoader(news_dataset,\n",
    "                                batch_size=args.batch_size,\n",
    "                                num_workers=4)\n",
    "\n",
    "news_scoring = []\n",
    "with torch.no_grad():\n",
    "    for input_ids in tqdm(news_dataloader):\n",
    "        input_ids = input_ids.cuda(rank)\n",
    "        news_vec = model.news_encoder(input_ids)\n",
    "        news_vec = news_vec.to(torch.device(\"cpu\")).detach().numpy()\n",
    "        news_scoring.extend(news_vec)\n",
    "\n",
    "news_scoring = np.array(news_scoring)\n",
    "logging.info(\"news scoring num: {}\".format(news_scoring.shape[0]))\n",
    "\n",
    "if rank == 0:\n",
    "    doc_sim = 0\n",
    "    for _ in tqdm(range(1000000)):\n",
    "        i = random.randrange(1, len(news_scoring))\n",
    "        j = random.randrange(1, len(news_scoring))\n",
    "        if i != j:\n",
    "            doc_sim += np.dot(news_scoring[i], news_scoring[j]) / (np.linalg.norm(news_scoring[i]) * np.linalg.norm(news_scoring[j]))\n",
    "    logging.info(f'News doc-sim: {doc_sim / 1000000}')\n",
    "\n",
    "data_file_path = os.path.join(args.test_data_dir, f'behaviors_{rank}.tsv')\n",
    "\n",
    "def collate_fn(tuple_list):\n",
    "    log_vecs = torch.FloatTensor([x[0] for x in tuple_list])\n",
    "    log_mask = torch.FloatTensor([x[1] for x in tuple_list])\n",
    "    news_vecs = [x[2] for x in tuple_list]\n",
    "    labels = [x[3] for x in tuple_list]\n",
    "    return (log_vecs, log_mask, news_vecs, labels)\n",
    "\n",
    "dataset = DatasetTest(data_file_path, news_index, news_scoring, args)\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, collate_fn=collate_fn)\n",
    "\n",
    "AUC = []\n",
    "MRR = []\n",
    "nDCG5 = []\n",
    "nDCG10 = []\n",
    "\n",
    "def print_metrics(rank, cnt, x):\n",
    "    logging.info(\"[{}] {} samples: {}\".format(rank, cnt, '\\t'.join([\"{:0.2f}\".format(i * 100) for i in x])))\n",
    "\n",
    "def get_mean(arr):\n",
    "    return [np.array(i).mean() for i in arr]\n",
    "\n",
    "def get_sum(arr):\n",
    "    return [np.array(i).sum() for i in arr]\n",
    "\n",
    "local_sample_num = 0\n",
    "\n",
    "for cnt, (log_vecs, log_mask, news_vecs, labels) in enumerate(dataloader):\n",
    "    local_sample_num += log_vecs.shape[0]\n",
    "\n",
    "    if args.enable_gpu:\n",
    "        log_vecs = log_vecs.cuda(rank, non_blocking=True)\n",
    "        log_mask = log_mask.cuda(rank, non_blocking=True)\n",
    "\n",
    "    user_vecs = model.user_encoder(log_vecs, log_mask).to(torch.device(\"cpu\")).detach().numpy()\n",
    "\n",
    "    for user_vec, news_vec, label in zip(user_vecs, news_vecs, labels):\n",
    "        if label.mean() == 0 or label.mean() == 1:\n",
    "            continue\n",
    "\n",
    "        score = np.dot(news_vec, user_vec)\n",
    "\n",
    "        auc = auc_score(label, score)\n",
    "        mrr = mrr_score(label, score)\n",
    "        ndcg5 = ndcg_score(label, score, k=5)\n",
    "        ndcg10 = ndcg_score(label, score, k=10)\n",
    "\n",
    "        AUC.append(auc)\n",
    "        MRR.append(mrr)\n",
    "        nDCG5.append(ndcg5)\n",
    "        nDCG10.append(ndcg10)\n",
    "\n",
    "    if cnt % args.log_steps == 0:\n",
    "        print_metrics(rank, local_sample_num, get_mean([AUC, MRR, nDCG5, nDCG10]))\n",
    "\n",
    "logging.info('[{}] local_sample_num: {}'.format(rank, local_sample_num))\n",
    "\n",
    "print_metrics('*', local_sample_num, get_mean([AUC, MRR, nDCG5, nDCG10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
