{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jb2bDxXnL4Ot",
        "outputId": "a5cf2e33-b982-4137-ce69-b654c3071002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E7i79ErILbl8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "import logging\n",
        "import os\n",
        "import torch.optim as optim\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WxUXYhf2MjaM"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "@dataclass\n",
        "class Args:\n",
        "    nGPU: int = 1\n",
        "    seed: int = 0\n",
        "    prepare: bool = True\n",
        "    mode: str = \"train\"\n",
        "    train_data_dir: str = \"/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/MINDsmall_train\"\n",
        "    test_data_dir: str = \"/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/MINDsmall_dev\"\n",
        "    train_abstract_dir: str = '/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/genAbs0.json'\n",
        "    # \"/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/train_gen_abs.json\"\n",
        "    test_abstract_dir: str = '/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/genAbs0.json'\n",
        "    # \"/content/drive/MyDrive/Colab Notebooks/NewsRecommendation/data/Dev_gen_abs.json\"\n",
        "    model_dir: str = '/content/model'\n",
        "    batch_size: int = 32\n",
        "    npratio: int = 4\n",
        "    enable_gpu: bool = False\n",
        "    filter_num: int = 3\n",
        "    log_steps: int = 100\n",
        "    epochs: int = 5\n",
        "    lr: float = 0.0003\n",
        "    num_words_title: int = 20\n",
        "    num_words_abstract: int = 50\n",
        "    user_log_length: int = 50\n",
        "    word_embedding_dim: int = 300\n",
        "    glove_embedding_path: str = '/home/vinmike/Downloads/glove.840B.300d.txt'\n",
        "    freeze_embedding: bool = False\n",
        "    news_dim: int = 400\n",
        "    news_query_vector_dim: int = 200\n",
        "    user_query_vector_dim: int = 200\n",
        "    num_attention_heads: int = 15\n",
        "    user_log_mask: bool = False\n",
        "    dropout_probability = 0.2\n",
        "    save_steps: int = 10000\n",
        "    start_epoch: int = 0\n",
        "    load_ckpt_name: Optional[str] = None\n",
        "    use_category: bool = True\n",
        "    use_subcategory: bool = True\n",
        "    use_abstract: bool = True\n",
        "    use_custom_abstract: bool = True\n",
        "    category_emb_dim: int = 100\n",
        "    num_filters = 300\n",
        "    window_size = 3\n",
        "    long_short_term_method = 'ini'\n",
        "    masking_probability = 0.5\n",
        "    num_users = 1 + 50000\n",
        "    num_categories = 1 + 274\n",
        "\n",
        "def parse_args():\n",
        "  return Args()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_SLKbAZpKFK"
      },
      "source": [
        "**Dataset.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pqMrlk2u-Hiv"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import IterableDataset, Dataset\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "\n",
        "\n",
        "class DatasetTrain(IterableDataset):\n",
        "    def __init__(self, filename, news_index, news_combined, user2id_path, args):\n",
        "        super(DatasetTrain).__init__()\n",
        "        self.filename = filename\n",
        "        self.news_index = news_index  # Maps news ID -> index\n",
        "        self.news_combined = news_combined  # News feature matrix\n",
        "        self.args = args\n",
        "\n",
        "        # Load user2id mapping from file\n",
        "        with open(user2id_path, \"r\") as f:\n",
        "            self.user2id = json.load(f)\n",
        "\n",
        "    def trans_to_nindex(self, nids):\n",
        "        return [self.news_index[i] if i in self.news_index else 0 for i in nids]\n",
        "\n",
        "    def pad_to_fix_len(self, x, fix_length, padding_front=True, padding_value=0):\n",
        "        if padding_front:\n",
        "            pad_x = [padding_value] * (fix_length - len(x)) + x[-fix_length:]\n",
        "            mask = [0] * (fix_length - len(x)) + [1] * min(fix_length, len(x))\n",
        "        else:\n",
        "            pad_x = x[-fix_length:] + [padding_value] * (fix_length - len(x))\n",
        "            mask = [1] * min(fix_length, len(x)) + [0] * (fix_length - len(x))\n",
        "        return pad_x, np.array(mask, dtype='float32')\n",
        "\n",
        "    def line_mapper(self, line):\n",
        "        line = line.strip().split(\"\\t\")\n",
        "\n",
        "        user_id = line[0]  # First column is user ID\n",
        "        click_docs = line[3].split()  # Clicked news IDs\n",
        "        sess_pos = line[4].split()  # Positive candidate news IDs\n",
        "        sess_neg = line[5].split()  # Negative candidate news IDs\n",
        "\n",
        "        # Convert user ID to index, default to \"unk\" if missing\n",
        "        user_index = self.user2id.get(user_id, self.user2id[\"unk\"])\n",
        "\n",
        "        # Convert clicked news IDs to indices print(f\"user shape: {user.shape}\")\n",
        "        \n",
        "        clicked_news = self.trans_to_nindex(click_docs)\n",
        "        clicked_news_length = len(clicked_news)\n",
        "        \n",
        "\n",
        "        # Pad clicked news to fixed length\n",
        "        clicked_news, _ = self.pad_to_fix_len(clicked_news, self.args.user_log_length)\n",
        "        clicked_news_feature = self.news_combined[clicked_news]\n",
        "\n",
        "        # Sample candidate news (positive + negative)\n",
        "        label = random.randint(0, self.args.npratio)\n",
        "        sample_news = sess_neg[:label] + sess_pos + sess_neg[label:]\n",
        "        sample_news, _ = self.pad_to_fix_len(self.trans_to_nindex(sample_news), self.args.user_log_length)\n",
        "\n",
        "        candidate_news_feature = self.news_combined[sample_news]\n",
        "        \n",
        "        return user_index, clicked_news_length, candidate_news_feature, clicked_news_feature, label\n",
        "\n",
        "    def __iter__(self):\n",
        "        file_iter = open(self.filename)\n",
        "        return map(self.line_mapper, file_iter)\n",
        "\n",
        "\n",
        "\n",
        "class DatasetTest(DatasetTrain):\n",
        "    def __init__(self, filename, news_index, news_scoring, args):\n",
        "        super(DatasetTrain).__init__()\n",
        "        self.filename = filename\n",
        "        self.news_index = news_index\n",
        "        self.news_scoring = news_scoring\n",
        "        self.args = args\n",
        "\n",
        "    def line_mapper(self, line):\n",
        "        line = line.strip().split('\\t')\n",
        "        click_docs = line[3].split()\n",
        "        click_docs, log_mask = self.pad_to_fix_len(self.trans_to_nindex(click_docs), self.args.user_log_length)\n",
        "        user_feature = self.news_scoring[click_docs]\n",
        "\n",
        "        candidate_news = self.trans_to_nindex([i.split('-')[0] for i in line[4].split()])\n",
        "        labels = np.array([int(i.split('-')[1]) for i in line[4].split()])\n",
        "        news_feature = self.news_scoring[candidate_news]\n",
        "\n",
        "        return user_feature, log_mask, news_feature, labels\n",
        "\n",
        "    def __iter__(self):\n",
        "        file_iter = open(self.filename)\n",
        "        return map(self.line_mapper, file_iter)\n",
        "\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1IjisEE-LHb"
      },
      "source": [
        "**Metric.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ppd8qdG1-Nr3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def dcg_score(y_true, y_score, k=10):\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "    gains = 2**y_true - 1\n",
        "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
        "    return np.sum(gains / discounts)\n",
        "\n",
        "\n",
        "def ndcg_score(y_true, y_score, k=10):\n",
        "    best = dcg_score(y_true, y_true, k)\n",
        "    actual = dcg_score(y_true, y_score, k)\n",
        "    return actual / best\n",
        "\n",
        "\n",
        "def mrr_score(y_true, y_score):\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order)\n",
        "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
        "    return np.sum(rr_score) / np.sum(y_true)\n",
        "\n",
        "\n",
        "def ctr_score(y_true, y_score, k=1):\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "    return np.mean(y_true)\n",
        "\n",
        "def acc(y_true, y_hat):\n",
        "    y_hat = torch.argmax(y_hat, dim=-1)\n",
        "    tot = y_true.shape[0]\n",
        "    hit = torch.sum(y_true == y_hat)\n",
        "    return hit.data.float() * 1.0 / tot\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1ZeoY8x-5Ds"
      },
      "source": [
        "**Ultis.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Dfr-ri4E-s3M"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import argparse\n",
        "import sys\n",
        "\n",
        "def setuplogger():\n",
        "    root = logging.getLogger()\n",
        "    root.setLevel(logging.INFO)\n",
        "    handler = logging.StreamHandler(sys.stdout)\n",
        "    handler.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter(\"[%(levelname)s %(asctime)s] %(message)s\")\n",
        "    handler.setFormatter(formatter)\n",
        "    root.addHandler(handler)\n",
        "\n",
        "\n",
        "def dump_args(args):\n",
        "    for arg in dir(args):\n",
        "        if not arg.startswith(\"_\"):\n",
        "            logging.info(f\"args[{arg}]={getattr(args, arg)}\")\n",
        "\n",
        "def load_matrix(embedding_file_path, word_dict, word_embedding_dim):\n",
        "    embedding_matrix = np.zeros(shape=(len(word_dict) + 1, word_embedding_dim))\n",
        "    have_word = []\n",
        "    if embedding_file_path is not None:\n",
        "        with open(embedding_file_path, 'rb') as f:\n",
        "            while True:\n",
        "                line = f.readline()\n",
        "                if len(line) == 0:\n",
        "                    break\n",
        "                line = line.split()\n",
        "                word = line[0].decode()\n",
        "                if word in word_dict:\n",
        "                    index = word_dict[word]\n",
        "                    tp = [float(x) for x in line[1:]]\n",
        "                    embedding_matrix[index] = np.array(tp)\n",
        "                    have_word.append(word)\n",
        "    return embedding_matrix, have_word\n",
        "\n",
        "\n",
        "def get_checkpoint(directory, ckpt_name):\n",
        "    ckpt_path = os.path.join(directory, ckpt_name)\n",
        "    if os.path.exists(ckpt_path):\n",
        "        return ckpt_path\n",
        "    else:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqNlsklUAi48"
      },
      "source": [
        "**preprocess.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_Ja_pvD0AYwg"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "import json\n",
        "\n",
        "\n",
        "def update_dict(dict, key, value=None):\n",
        "    if key not in dict:\n",
        "        if value is None:\n",
        "            dict[key] = len(dict) + 1\n",
        "        else:\n",
        "            dict[key] = value\n",
        "\n",
        "\n",
        "def read_custom_abstract(news_file, custom_abstract_dict):\n",
        "    news = {}\n",
        "    news_index = {}\n",
        "    category_dict = {}\n",
        "    subcategory_dict = {}\n",
        "    word_cnt = {}\n",
        "\n",
        "    with open(news_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            splited = line.strip('\\n').split('\\t')\n",
        "            doc_id, category, subcategory, title, abstract, url, entity_title, entity_abstract = splited\n",
        "            if doc_id in custom_abstract_dict:\n",
        "                abstract = custom_abstract_dict[doc_id]\n",
        "            news[doc_id] = [title.split(' '), category, subcategory, abstract.split(' ')]\n",
        "            news_index[doc_id] = len(news_index) + 1\n",
        "            for word in title.split(' '):\n",
        "                if word not in word_cnt:\n",
        "                    word_cnt[word] = 0\n",
        "                word_cnt[word] += 1\n",
        "            for word in abstract.split(' '):\n",
        "                if word not in word_cnt:\n",
        "                    word_cnt[word] = 0\n",
        "                word_cnt[word] += 1\n",
        "            if category not in category_dict:\n",
        "                category_dict[category] = len(category_dict) + 1\n",
        "            if subcategory not in subcategory_dict:\n",
        "                subcategory_dict[subcategory] = len(subcategory_dict) + 1\n",
        "\n",
        "    return news, news_index, category_dict, subcategory_dict, word_cnt\n",
        "\n",
        "def read_news(news_path, abstract_path, args, mode='train'):\n",
        "    news = {}\n",
        "    category_dict = {}\n",
        "    subcategory_dict = {}\n",
        "    news_index = {}\n",
        "    word_cnt = Counter()\n",
        "    if args.use_custom_abstract:\n",
        "      with open(abstract_path, 'r') as f:\n",
        "          abs = json.load(f)\n",
        "    with open(news_path, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f):\n",
        "            splited = line.strip('\\n').split('\\t')\n",
        "            doc_id, category, subcategory, title, abstract, url, _, _ = splited\n",
        "            update_dict(news_index, doc_id)\n",
        "\n",
        "            title = title.lower()\n",
        "            title = word_tokenize(title, language='english', preserve_line=True)\n",
        "\n",
        "            update_dict(news, doc_id, [title, category, subcategory, abs[doc_id] if doc_id in abs else abstract])\n",
        "            if mode == 'train':\n",
        "                if args.use_category:\n",
        "                    update_dict(category_dict, category)\n",
        "                if args.use_subcategory:\n",
        "                    update_dict(subcategory_dict, subcategory)\n",
        "                word_cnt.update(title)\n",
        "\n",
        "    if mode == 'train':\n",
        "        word = [k for k, v in word_cnt.items() if v > args.filter_num]\n",
        "        word_dict = {k: v for k, v in zip(word, range(1, len(word) + 1))}\n",
        "        return news, news_index, category_dict, subcategory_dict, word_dict\n",
        "    elif mode == 'test':\n",
        "        return news, news_index\n",
        "    else:\n",
        "        assert False, 'Wrong mode!'\n",
        "\n",
        "\n",
        "def get_doc_input(news, news_index, category_dict, subcategory_dict, word_dict, args):\n",
        "    news_num = len(news) + 1\n",
        "    news_title = np.zeros((news_num, args.num_words_title), dtype='int32')\n",
        "    news_category = np.zeros((news_num, 1), dtype='int32') if args.use_category else None\n",
        "    news_subcategory = np.zeros((news_num, 1), dtype='int32') if args.use_subcategory else None\n",
        "    news_abstract = np.zeros((news_num, args.num_words_abstract), dtype='int32') if args.use_abstract else None\n",
        "\n",
        "    for key in tqdm(news):\n",
        "        title, category, subcategory, abstract = news[key]\n",
        "        doc_index = news_index[key]\n",
        "\n",
        "        for word_id in range(min(args.num_words_title, len(title))):\n",
        "            if title[word_id] in word_dict:\n",
        "                news_title[doc_index, word_id] = word_dict[title[word_id]]\n",
        "\n",
        "        if args.use_category:\n",
        "            news_category[doc_index, 0] = category_dict[category] if category in category_dict else 0\n",
        "        if args.use_subcategory:\n",
        "            news_subcategory[doc_index, 0] = subcategory_dict[subcategory] if subcategory in subcategory_dict else 0\n",
        "        if args.use_abstract:\n",
        "            for word_id in range(min(args.num_words_abstract, len(abstract))):\n",
        "                if abstract[word_id] in word_dict:\n",
        "                    news_abstract[doc_index, word_id] = word_dict[abstract[word_id]]\n",
        "\n",
        "    return news_title, news_category, news_subcategory, news_abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MambLdtFlSxi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zXd8SNmIm34q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu43E3_6AqAA"
      },
      "source": [
        "**prepare_data.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5Qzn9-0kAuFF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import logging\n",
        "\n",
        "\n",
        "def get_sample(all_elements, num_sample):\n",
        "    if num_sample > len(all_elements):\n",
        "        return random.sample(all_elements * (num_sample // len(all_elements) + 1), num_sample)\n",
        "    else:\n",
        "        return random.sample(all_elements, num_sample)\n",
        "\n",
        "\n",
        "def prepare_training_data(train_data_dir, nGPU, npratio, seed):\n",
        "    random.seed(seed)\n",
        "    behaviors = []\n",
        "\n",
        "    behavior_file_path = os.path.join(train_data_dir, 'behaviors.tsv')\n",
        "    with open(behavior_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f):\n",
        "            iid, uid, time, history, imp = line.strip().split('\\t')\n",
        "            impressions = [x.split('-') for x in imp.split(' ')]\n",
        "            pos, neg = [], []\n",
        "            for news_ID, label in impressions:\n",
        "                if label == '0':\n",
        "                    neg.append(news_ID)\n",
        "                elif label == '1':\n",
        "                    pos.append(news_ID)\n",
        "            if len(pos) == 0 or len(neg) == 0:\n",
        "                continue\n",
        "            for pos_id in pos:\n",
        "                neg_candidate = get_sample(neg, npratio)\n",
        "                neg_str = ' '.join(neg_candidate)\n",
        "                new_line = '\\t'.join([iid, uid, time, history, pos_id, neg_str]) + '\\n'\n",
        "                behaviors.append(new_line)\n",
        "\n",
        "    random.shuffle(behaviors)\n",
        "\n",
        "    behaviors_per_file = [[] for _ in range(nGPU)]\n",
        "    for i, line in enumerate(behaviors):\n",
        "        behaviors_per_file[i % nGPU].append(line)\n",
        "\n",
        "    logging.info('Writing files...')\n",
        "    for i in range(nGPU):\n",
        "        processed_file_path = os.path.join(train_data_dir, f'behaviors_np{npratio}_{i}.tsv')\n",
        "        with open(processed_file_path, 'w') as f:\n",
        "            f.writelines(behaviors_per_file[i])\n",
        "\n",
        "    return len(behaviors)\n",
        "\n",
        "\n",
        "def prepare_testing_data(test_data_dir, nGPU):\n",
        "    behaviors = [[] for _ in range(nGPU)]\n",
        "\n",
        "    behavior_file_path = os.path.join(test_data_dir, 'behaviors.tsv')\n",
        "    with open(behavior_file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(tqdm(f)):\n",
        "            behaviors[i % nGPU].append(line)\n",
        "\n",
        "    logging.info('Writing files...')\n",
        "    for i in range(nGPU):\n",
        "        processed_file_path = os.path.join(test_data_dir, f'behaviors_{i}.tsv')\n",
        "        with open(processed_file_path, 'w') as f:\n",
        "            f.writelines(behaviors[i])\n",
        "\n",
        "    return sum([len(x) for x in behaviors])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VGYXTH1pMIBO"
      },
      "outputs": [],
      "source": [
        "def train(rank, args):\n",
        "\n",
        "    is_distributed = False\n",
        "    torch.cuda.set_device(rank)\n",
        "\n",
        "\n",
        "    news, news_index, category_dict, subcategory_dict, word_dict = read_news(\n",
        "        os.path.join(args.train_data_dir, 'news.tsv'), args.train_abstract_dir, args, mode='train')\n",
        "\n",
        "    news_title, news_category, news_subcategory, news_abstract = get_doc_input(\n",
        "        news, news_index, category_dict, subcategory_dict, word_dict, args)\n",
        "    news_combined = np.concatenate([x for x in [news_title, news_category, news_subcategory, news_abstract] if x is not None], axis=-1)\n",
        "\n",
        "    if rank == 0:\n",
        "        logging.info('Initializing word embedding matrix...')\n",
        "\n",
        "    embedding_matrix, have_word = load_matrix(args.glove_embedding_path,\n",
        "                                                    word_dict,\n",
        "                                                    args.word_embedding_dim)\n",
        "    if rank == 0:\n",
        "        logging.info(f'Word dict length: {len(word_dict)}')\n",
        "        logging.info(f'Have words: {len(have_word)}')\n",
        "        logging.info(f'Missing rate: {(len(word_dict) - len(have_word)) / len(word_dict)}')\n",
        "\n",
        "    model = Model(args, embedding_matrix, len(category_dict), len(subcategory_dict))\n",
        "\n",
        "    if args.load_ckpt_name is not None:\n",
        "        ckpt_path = get_checkpoint(args.model_dir, args.load_ckpt_name)\n",
        "        checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        logging.info(f\"Model loaded from {ckpt_path}.\")\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    if args.enable_gpu:\n",
        "        model = model.cuda(rank)\n",
        "\n",
        "    if is_distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n",
        "\n",
        "    # if rank == 0:\n",
        "    #     print(model)\n",
        "    #     for name, param in model.named_parameters():\n",
        "    #         print(name, param.requires_grad)\n",
        "\n",
        "    data_file_path = os.path.join(args.train_data_dir, f'behaviors_np{args.npratio}_{rank}.tsv')\n",
        "\n",
        "    dataset = DatasetTrain(data_file_path, news_index, news_combined, args)\n",
        "    dataloader = DataLoader(dataset, batch_size=args.batch_size)\n",
        "\n",
        "    logging.info('Training...')\n",
        "    for ep in range(args.start_epoch, args.epochs):\n",
        "        loss = 0.0\n",
        "        accuary = 0.0\n",
        "        for cnt, (log_ids, log_mask, input_ids, targets) in enumerate(dataloader):\n",
        "            if args.enable_gpu:\n",
        "                log_ids = log_ids.cuda(rank, non_blocking=True)\n",
        "                log_mask = log_mask.cuda(rank, non_blocking=True)\n",
        "                input_ids = input_ids.cuda(rank, non_blocking=True)\n",
        "                targets = targets.cuda(rank, non_blocking=True)\n",
        "\n",
        "            bz_loss, y_hat = model(log_ids, log_mask, input_ids, targets)\n",
        "            loss += bz_loss.data.float()\n",
        "            accuary += acc(targets, y_hat)\n",
        "            optimizer.zero_grad()\n",
        "            bz_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if cnt % args.log_steps == 0:\n",
        "                logging.info(\n",
        "                    '[{}] Ed: {}, train_loss: {:.5f}, acc: {:.5f}'.format(\n",
        "                        rank, cnt * args.batch_size, loss.data / cnt, accuary / cnt)\n",
        "                )\n",
        "\n",
        "            if rank == 0 and     cnt != 0 and cnt % args.save_steps == 0:\n",
        "                ckpt_path = os.path.join(args.model_dir, f'epoch-{ep+1}-{cnt}.pt')\n",
        "                torch.save(\n",
        "                    {\n",
        "                        'model_state_dict':\n",
        "                            {'.'.join(k.split('.')[1:]): v for k, v in model.state_dict().items()}\n",
        "                            if is_distributed else model.state_dict(),\n",
        "                        'category_dict': category_dict,\n",
        "                        'word_dict': word_dict,\n",
        "                        'subcategory_dict': subcategory_dict\n",
        "                    }, ckpt_path)\n",
        "                logging.info(f\"Model saved to {ckpt_path}.\")\n",
        "\n",
        "        logging.info('Training finish.')\n",
        "\n",
        "        if rank == 0:\n",
        "            ckpt_path = os.path.join(args.model_dir, f'epoch-{ep+1}.pt')\n",
        "            torch.save(\n",
        "                {\n",
        "                    'model_state_dict':\n",
        "                        {'.'.join(k.split('.')[1:]): v for k, v in model.state_dict().items()}\n",
        "                        if is_distributed else model.state_dict(),\n",
        "                    'category_dict': category_dict,\n",
        "                    'subcategory_dict': subcategory_dict,\n",
        "                    'word_dict': word_dict,\n",
        "                }, ckpt_path)\n",
        "            logging.info(f\"Model saved to {ckpt_path}.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgkOVqFzS6Uv",
        "outputId": "21c5d676-d2dc-4c6d-b2d0-7116daa3bcfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2025-02-25 00:45:22,237] args[batch_size]=32\n",
            "[INFO 2025-02-25 00:45:22,238] args[category_emb_dim]=100\n",
            "[INFO 2025-02-25 00:45:22,239] args[dropout_probability]=0.2\n",
            "[INFO 2025-02-25 00:45:22,240] args[enable_gpu]=False\n",
            "[INFO 2025-02-25 00:45:22,241] args[epochs]=5\n",
            "[INFO 2025-02-25 00:45:22,242] args[filter_num]=3\n",
            "[INFO 2025-02-25 00:45:22,243] args[freeze_embedding]=False\n",
            "[INFO 2025-02-25 00:45:22,243] args[glove_embedding_path]=/home/vinmike/Downloads/glove.840B.300d.txt\n",
            "[INFO 2025-02-25 00:45:22,244] args[load_ckpt_name]=None\n",
            "[INFO 2025-02-25 00:45:22,245] args[log_steps]=100\n",
            "[INFO 2025-02-25 00:45:22,246] args[long_short_term_method]=ini\n",
            "[INFO 2025-02-25 00:45:22,247] args[lr]=0.0003\n",
            "[INFO 2025-02-25 00:45:22,247] args[masking_probability]=0.5\n",
            "[INFO 2025-02-25 00:45:22,248] args[mode]=train\n",
            "[INFO 2025-02-25 00:45:22,249] args[model_dir]=/content/model\n",
            "[INFO 2025-02-25 00:45:22,252] args[nGPU]=1\n",
            "[INFO 2025-02-25 00:45:22,252] args[news_dim]=400\n",
            "[INFO 2025-02-25 00:45:22,253] args[news_query_vector_dim]=200\n",
            "[INFO 2025-02-25 00:45:22,253] args[npratio]=4\n",
            "[INFO 2025-02-25 00:45:22,254] args[num_attention_heads]=15\n",
            "[INFO 2025-02-25 00:45:22,255] args[num_categories]=275\n",
            "[INFO 2025-02-25 00:45:22,256] args[num_filters]=300\n",
            "[INFO 2025-02-25 00:45:22,257] args[num_users]=50001\n",
            "[INFO 2025-02-25 00:45:22,258] args[num_words_abstract]=50\n",
            "[INFO 2025-02-25 00:45:22,258] args[num_words_title]=20\n",
            "[INFO 2025-02-25 00:45:22,259] args[prepare]=True\n",
            "[INFO 2025-02-25 00:45:22,260] args[save_steps]=10000\n",
            "[INFO 2025-02-25 00:45:22,261] args[seed]=0\n",
            "[INFO 2025-02-25 00:45:22,262] args[start_epoch]=0\n",
            "[INFO 2025-02-25 00:45:22,262] args[test_abstract_dir]=/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/genAbs0.json\n",
            "[INFO 2025-02-25 00:45:22,263] args[test_data_dir]=/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/MINDsmall_dev\n",
            "[INFO 2025-02-25 00:45:22,264] args[train_abstract_dir]=/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/genAbs0.json\n",
            "[INFO 2025-02-25 00:45:22,264] args[train_data_dir]=/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/MINDsmall_train\n",
            "[INFO 2025-02-25 00:45:22,265] args[use_abstract]=True\n",
            "[INFO 2025-02-25 00:45:22,265] args[use_category]=True\n",
            "[INFO 2025-02-25 00:45:22,266] args[use_custom_abstract]=True\n",
            "[INFO 2025-02-25 00:45:22,266] args[use_subcategory]=True\n",
            "[INFO 2025-02-25 00:45:22,266] args[user_log_length]=50\n",
            "[INFO 2025-02-25 00:45:22,267] args[user_log_mask]=False\n",
            "[INFO 2025-02-25 00:45:22,267] args[user_query_vector_dim]=200\n",
            "[INFO 2025-02-25 00:45:22,268] args[window_size]=3\n",
            "[INFO 2025-02-25 00:45:22,268] args[word_embedding_dim]=300\n"
          ]
        }
      ],
      "source": [
        "\n",
        "    import subprocess\n",
        "    setuplogger()\n",
        "    args = parse_args()\n",
        "    dump_args(args)\n",
        "    random.seed(args.seed)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5694OJnDLmEV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkoY3tHH6in5"
      },
      "source": [
        "# **NRMS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KXeVVGPl6sgG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class DotProductClickPredictor(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DotProductClickPredictor, self).__init__()\n",
        "\n",
        "    def forward(self, candidate_news_vector, user_vector):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            candidate_news_vector: batch_size, candidate_size, X\n",
        "            user_vector: batch_size, X\n",
        "        Returns:\n",
        "            (shape): batch_size\n",
        "        \"\"\"\n",
        "        # batch_size, candidate_size\n",
        "        probability = torch.bmm(candidate_news_vector,\n",
        "                                user_vector.unsqueeze(dim=-1)).squeeze(dim=-1)\n",
        "        return probability\n",
        "class AdditiveAttention(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A general additive attention module.\n",
        "    Originally for NAML.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 query_vector_dim,\n",
        "                 candidate_vector_dim,\n",
        "                 writer=None,\n",
        "                 tag=None,\n",
        "                 names=None):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "        self.linear = nn.Linear(candidate_vector_dim, query_vector_dim)\n",
        "        self.attention_query_vector = nn.Parameter(\n",
        "            torch.empty(query_vector_dim).uniform_(-0.1, 0.1))\n",
        "        # For tensorboard\n",
        "        self.writer = writer\n",
        "        self.tag = tag\n",
        "        self.names = names\n",
        "        self.local_step = 1\n",
        "\n",
        "    def forward(self, candidate_vector):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            candidate_vector: batch_size, candidate_size, candidate_vector_dim\n",
        "        Returns:\n",
        "            (shape) batch_size, candidate_vector_dim\n",
        "        \"\"\"\n",
        "        # batch_size, candidate_size, query_vector_dim\n",
        "        temp = torch.tanh(self.linear(candidate_vector))\n",
        "        # batch_size, candidate_size\n",
        "        candidate_weights = F.softmax(torch.matmul(\n",
        "            temp, self.attention_query_vector),\n",
        "                                      dim=1)\n",
        "        if self.writer is not None:\n",
        "            assert candidate_weights.size(1) == len(self.names)\n",
        "            if self.local_step % 10 == 0:\n",
        "                self.writer.add_scalars(\n",
        "                    self.tag, {\n",
        "                        x: y\n",
        "                        for x, y in zip(self.names,\n",
        "                                        candidate_weights.mean(dim=0))\n",
        "                    }, self.local_step)\n",
        "            self.local_step += 1\n",
        "        # batch_size, candidate_vector_dim\n",
        "        target = torch.bmm(candidate_weights.unsqueeze(dim=1),\n",
        "                           candidate_vector).squeeze(dim=1)\n",
        "        return target\n",
        "    \n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.d_k = d_k\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask=None):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k)\n",
        "        scores = torch.exp(scores)\n",
        "        if attn_mask is not None:\n",
        "            scores = scores * attn_mask\n",
        "        attn = scores / (torch.sum(scores, dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_attention_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        assert d_model % num_attention_heads == 0\n",
        "        self.d_k = d_model // num_attention_heads\n",
        "        self.d_v = d_model // num_attention_heads\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
        "\n",
        "    def forward(self, Q, K=None, V=None, length=None):\n",
        "        if K is None:\n",
        "            K = Q\n",
        "        if V is None:\n",
        "            V = Q\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.num_attention_heads,\n",
        "                               self.d_k).transpose(1, 2)\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.num_attention_heads,\n",
        "                               self.d_k).transpose(1, 2)\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.num_attention_heads,\n",
        "                               self.d_v).transpose(1, 2)\n",
        "\n",
        "        if length is not None:\n",
        "            maxlen = Q.size(1)\n",
        "            attn_mask = torch.arange(maxlen).expand(\n",
        "                batch_size, maxlen) < length.view(-1, 1)\n",
        "            attn_mask = attn_mask.unsqueeze(1).expand(batch_size, maxlen,\n",
        "                                                      maxlen)\n",
        "            attn_mask = attn_mask.unsqueeze(1).repeat(1,\n",
        "                                                      self.num_attention_heads,\n",
        "                                                      1, 1)\n",
        "        else:\n",
        "            attn_mask = None\n",
        "\n",
        "        context, attn = ScaledDotProductAttention(self.d_k)(q_s, k_s, v_s,\n",
        "                                                            attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(\n",
        "            batch_size, -1, self.num_attention_heads * self.d_v)\n",
        "        return context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "l3W3JjCZ6k1a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class NewsEncoder(torch.nn.Module):\n",
        "    def __init__(self, config, word_embedding):\n",
        "        super(NewsEncoder, self).__init__()\n",
        "        self.config = config\n",
        "        self.word_embedding = word_embedding\n",
        "        self.category_embedding = nn.Embedding(config.num_categories,\n",
        "                                               config.num_filters,\n",
        "                                               padding_idx=0)\n",
        "        assert config.window_size >= 1 and config.window_size % 2 == 1\n",
        "        \n",
        "        self.title_CNN = nn.Conv2d(\n",
        "            1,\n",
        "            config.num_filters,\n",
        "            (config.window_size, config.word_embedding_dim),\n",
        "            padding=(int((config.window_size - 1) / 2), 0))\n",
        "        self.title_attention = AdditiveAttention(config.news_query_vector_dim,\n",
        "                                                 config.num_filters)\n",
        "\n",
        "    def forward(self, news):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            news:\n",
        "                {\n",
        "                    \"category\": batch_size,\n",
        "                    \"subcategory\": batch_size,\n",
        "                    \"title\": batch_size * num_words_title\n",
        "                }\n",
        "        Returns:\n",
        "            (shape) batch_size, num_filters * 3\n",
        "        \"\"\"\n",
        "        #extract title, cat, subcat\n",
        "        start = self.config.num_words_title \n",
        "        title = torch.narrow(news, -1, 0, start).long()\n",
        "        title_vector = F.dropout(self.word_embedding(title.to(device)),\n",
        "                                 p=self.config.dropout_probability,\n",
        "                                 training=self.training)\n",
        "        convoluted_title_vector = self.title_CNN(title_vector.transpose(1, 2)).transpose(1, 2)\n",
        "        activated_title_vector = F.dropout(F.relu(convoluted_title_vector),\n",
        "                                           p=self.config.dropout_probability,\n",
        "                                           training=self.training)\n",
        "        weighted_title_vector = self.title_attention(activated_title_vector)\n",
        "        \n",
        "        if self.config.use_category:\n",
        "            category = torch.narrow(news, -1, start, 1).squeeze(dim=-1).long()\n",
        "            category_vector = self.category_embedding(category.to(device))\n",
        "            start +=1\n",
        "        if self.config.use_subcategory:\n",
        "            subcategory = torch.narrow(news, -1, start, 1).squeeze(dim=-1).long()\n",
        "            subcategory_vector = self.category_embedding(subcategory.to(device))\n",
        "\n",
        "        news_vector = torch.cat(\n",
        "            [weighted_title_vector, category_vector, subcategory_vector],\n",
        "            dim=1)\n",
        "        \n",
        "        return news_vector\n",
        "    \n",
        "class UserEncoder(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(UserEncoder, self).__init__()\n",
        "        self.config = config\n",
        "        assert int(config.num_filters * 1.5) == config.num_filters * 1.5\n",
        "        self.gru = nn.GRU(\n",
        "            config.num_filters * 3,\n",
        "            config.num_filters * 3 if config.long_short_term_method == 'ini'\n",
        "            else int(config.num_filters * 1.5))\n",
        "\n",
        "    def forward(self, user, clicked_news_length, clicked_news_vector):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            user:\n",
        "                ini: batch_size, num_filters * 3\n",
        "                con: batch_size, num_filters * 1.5\n",
        "            clicked_news_length: batch_size,\n",
        "            clicked_news_vector: batch_size, num_clicked_news_a_user, num_filters * 3\n",
        "        Returns:\n",
        "            (shape) batch_size, num_filters * 3\n",
        "        \"\"\"\n",
        "\n",
        "        clicked_news_length = torch.clamp(clicked_news_length, max=clicked_news_vector.shape[1])\n",
        "\n",
        "        clicked_news_length[clicked_news_length == 0] = 1\n",
        "        # 1, batch_size, num_filters * 3\n",
        "        if self.config.long_short_term_method == 'ini':\n",
        "            packed_clicked_news_vector = pack_padded_sequence(\n",
        "                clicked_news_vector,\n",
        "                clicked_news_length,\n",
        "                batch_first=True,\n",
        "                enforce_sorted=False)\n",
        "            _, last_hidden = self.gru(packed_clicked_news_vector,\n",
        "                                      user.unsqueeze(dim=0))\n",
        "            return last_hidden.squeeze(dim=0)\n",
        "        else:\n",
        "            packed_clicked_news_vector = pack_padded_sequence(\n",
        "                clicked_news_vector,\n",
        "                clicked_news_length,\n",
        "                batch_first=True,\n",
        "                enforce_sorted=False)\n",
        "            _, last_hidden = self.gru(packed_clicked_news_vector)\n",
        "            return torch.cat((last_hidden.squeeze(dim=0), user), dim=1)\n",
        "\n",
        "class LSTUR(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    LSTUR network.\n",
        "    Input 1 + K candidate news and a list of user clicked news, produce the click probability.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, embedding_matrix):\n",
        "        \"\"\"\n",
        "        # ini\n",
        "        user embedding: num_filters * 3\n",
        "        news encoder: num_filters * 3\n",
        "        GRU:\n",
        "        input: num_filters * 3\n",
        "        hidden: num_filters * 3\n",
        "\n",
        "        # con\n",
        "        user embedding: num_filter * 1.5\n",
        "        news encoder: num_filters * 3\n",
        "        GRU:\n",
        "        input: num_fitlers * 3\n",
        "        hidden: num_filter * 1.5\n",
        "        \"\"\"\n",
        "        super(LSTUR, self).__init__()\n",
        "        self.config = config\n",
        "        word_embedding = torch.from_numpy(embedding_matrix).float()\n",
        "        pretrained_word_embedding = nn.Embedding.from_pretrained(word_embedding,\n",
        "                                                      freeze=args.freeze_embedding,\n",
        "                                                      padding_idx=0)\n",
        "        self.news_encoder = NewsEncoder(config, pretrained_word_embedding)\n",
        "        self.user_encoder = UserEncoder(config)\n",
        "        self.click_predictor = DotProductClickPredictor()\n",
        "        assert int(config.num_filters * 1.5) == config.num_filters * 1.5\n",
        "        self.user_embedding = nn.Embedding(\n",
        "            config.num_users,\n",
        "            config.num_filters * 3 if config.long_short_term_method == 'ini'\n",
        "            else int(config.num_filters * 1.5),\n",
        "            padding_idx=0)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, user, clicked_news_length, candidate_news, clicked_news, label):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            user: batch_size,\n",
        "            clicked_news_length: batch_size,\n",
        "            candidate_news:\n",
        "                [\n",
        "                    {\n",
        "                        \"category\": batch_size,\n",
        "                        \"subcategory\": batch_size,\n",
        "                        \"title\": batch_size * num_words_title\n",
        "                    } * (1 + K)\n",
        "                ]\n",
        "            clicked_news:\n",
        "                [\n",
        "                    {\n",
        "                        \"category\": batch_size,\n",
        "                        \"subcategory\": batch_size,\n",
        "                        \"title\": batch_size * num_words_title\n",
        "                    } * num_clicked_news_a_user\n",
        "                ]\n",
        "        Returns:\n",
        "            click_probability: batch_size\n",
        "        \"\"\"\n",
        "        # batch_size, 1 + K, num_filters * 3\n",
        "        candidate_news_vector = torch.stack(\n",
        "            [self.news_encoder(x) for x in candidate_news])\n",
        "        # ini: batch_size, num_filters * 3\n",
        "        # con: batch_size, num_filters * 1.5\n",
        "        # TODO what if not drop\n",
        "        user = F.dropout2d(self.user_embedding(\n",
        "            user.to(device)).unsqueeze(dim=0),\n",
        "                           p=self.config.masking_probability,\n",
        "                           training=self.training).squeeze(dim=0)\n",
        "        # batch_size, num_clicked_news_a_user, num_filters * 3\n",
        "        clicked_news_vector = torch.stack(\n",
        "            [self.news_encoder(x) for x in clicked_news])\n",
        "        # batch_size, num_filters * 3\n",
        "        user_vector = self.user_encoder(user, clicked_news_length,\n",
        "                                        clicked_news_vector)\n",
        "        # batch_size, 1 + K\n",
        "        click_probability = self.click_predictor(candidate_news_vector,\n",
        "                                                 user_vector)\n",
        "        loss = self.loss_fn(click_probability, label)\n",
        "        return loss, click_probability\n",
        "\n",
        "    def get_news_vector(self, news):\n",
        "        # batch_size, num_filters * 3\n",
        "        return self.news_encoder(news)\n",
        "\n",
        "    def get_user_vector(self, user, clicked_news_length, clicked_news_vector):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            user: batch_size\n",
        "            clicked_news_length: batch_size\n",
        "            clicked_news_vector: batch_size, num_clicked_news_a_user, num_filters * 3\n",
        "        Returns:\n",
        "            (shape) batch_size, num_filters * 3\n",
        "        \"\"\"\n",
        "        # ini: batch_size, num_filters * 3\n",
        "        # con: batch_size, num_filters * 1.5\n",
        "        user = self.user_embedding(user.to(device))\n",
        "        # batch_size, num_filters * 3\n",
        "        return self.user_encoder(user, clicked_news_length,\n",
        "                                 clicked_news_vector)\n",
        "\n",
        "    def get_prediction(self, news_vector, user_vector):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            news_vector: candidate_size, word_embedding_dim\n",
        "            user_vector: word_embedding_dim\n",
        "        Returns:\n",
        "            click_probability: candidate_size\n",
        "        \"\"\"\n",
        "        # candidate_size\n",
        "        return self.click_predictor(\n",
        "            news_vector.unsqueeze(dim=0),\n",
        "            user_vector.unsqueeze(dim=0)).squeeze(dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OZngQdLK9Xfa"
      },
      "outputs": [],
      "source": [
        "args.mode = 'train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ms-iPzD63S-",
        "outputId": "d88b5bd8-a142-4980-e8ce-f760e6068a8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2025-02-25 00:45:29,567] Preparing training data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "156965it [00:02, 68968.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2025-02-25 00:45:32,033] Writing files...\n",
            "[INFO 2025-02-25 00:45:32,280] 236344 training samples, 7385 batches in total.\n"
          ]
        }
      ],
      "source": [
        "if 'train' in args.mode:\n",
        "    if args.prepare:\n",
        "        logging.info('Preparing training data...')\n",
        "        total_sample_num = prepare_training_data(args.train_data_dir, args.nGPU, args.npratio, args.seed)\n",
        "    else:\n",
        "        total_sample_num = 0\n",
        "        for i in range(args.nGPU):\n",
        "            data_file_path = os.path.join(args.train_data_dir, f'behaviors_np{args.npratio}_{i}.tsv')\n",
        "            print(data_file_path)\n",
        "            if not os.path.exists(data_file_path):\n",
        "                logging.error(f'Splited training data {data_file_path} for GPU {i} does not exist. Please set the parameter --prepare as True and rerun the code.')\n",
        "                exit()\n",
        "            result = subprocess.getoutput(f'wc -l {data_file_path}')\n",
        "            total_sample_num += int(result.split(' ')[0])\n",
        "        logging.info('Skip training data preparation.')\n",
        "    logging.info(f'{total_sample_num} training samples, {total_sample_num // args.batch_size // args.nGPU} batches in total.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "51282it [00:02, 17362.08it/s]\n",
            "100%|| 51282/51282 [00:00<00:00, 178713.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2025-02-25 00:45:36,256] Initializing word embedding matrix...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2025-02-25 00:46:11,157] Word dict length: 12519\n",
            "[INFO 2025-02-25 00:46:11,157] Have words: 11960\n",
            "[INFO 2025-02-25 00:46:11,158] Missing rate: 0.0446521287642783\n"
          ]
        }
      ],
      "source": [
        "rank = 0\n",
        "news, news_index, category_dict, subcategory_dict, word_dict = read_news(\n",
        "\t\tos.path.join(args.train_data_dir, 'news.tsv'), args.train_abstract_dir, args, mode='train')\n",
        "\n",
        "news_title, news_category, news_subcategory, news_abstract = get_doc_input(\n",
        "    news, news_index, category_dict, subcategory_dict, word_dict, args)\n",
        "news_combined = np.concatenate([x for x in [news_title, news_category, news_subcategory] if x is not None], axis=-1)\n",
        "\n",
        "if rank == 0:\n",
        "    logging.info('Initializing word embedding matrix...')\n",
        "\n",
        "embedding_matrix, have_word = load_matrix(args.glove_embedding_path,\n",
        "                                                word_dict,\n",
        "                                                args.word_embedding_dim)\n",
        "if rank == 0:\n",
        "    logging.info(f'Word dict length: {len(word_dict)}')\n",
        "    logging.info(f'Have words: {len(have_word)}')\n",
        "    logging.info(f'Missing rate: {(len(word_dict) - len(have_word)) / len(word_dict)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qt4Qsf_r9RA_",
        "outputId": "f54e7ea2-1df5-49fc-d9af-75d422e45144"
      },
      "outputs": [
        {
          "ename": "TabError",
          "evalue": "inconsistent use of tabs and spaces in indentation (<string>, line 38)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m<string>:38\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"user_index shape: {user_index.shape}\")\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = LSTUR(args, embedding_matrix)\n",
        "is_distributed = False\n",
        "'''\n",
        "if args.load_ckpt_name is not None:\n",
        "\tckpt_path = get_checkpoint(args.model_dir, args.load_ckpt_name)\n",
        "\tcheckpoint = torch.load(ckpt_path, map_location='cpu')\n",
        "\tmodel.load_state_dict(checkpoint['model_state_dict'])\n",
        "\tlogging.info(f\"Model loaded from {ckpt_path}.\")\n",
        "'''\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "if args.enable_gpu:\n",
        "\tmodel = model.cuda(rank)\n",
        "\n",
        "if is_distributed:\n",
        "\tmodel = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n",
        "\n",
        "data_file_path = os.path.join(args.train_data_dir, f'behaviors_np{args.npratio}_{rank}.tsv')\n",
        "\n",
        "dataset = DatasetTrain(data_file_path, news_index, news_combined, \"/home/vinmike/Documents/GitHub/LLM4Rec-Dataloader/data/MINDsmall_train/user2id.json\", args)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=args.batch_size)\n",
        "\n",
        "logging.info('Training...')\n",
        "for ep in range(args.start_epoch, args.epochs):\n",
        "\tloss = 0.0\n",
        "\taccuary = 0.0\n",
        "\tfor cnt, (user_index, clicked_news_length, candidate_news_feature, clicked_news_feature, label) in enumerate(dataloader):\n",
        "\t\tif args.enable_gpu:\n",
        "\t\t\tuser_index = user_index.cuda(rank, non_blocking=True)\n",
        "\t\t\tclicked_news_length = clicked_news_length.cuda(rank, non_blocking=True)\n",
        "\t\t\tcandidate_news_feature = candidate_news_feature.cuda(rank, non_blocking=True)\n",
        "\t\t\tclicked_news_feature = clicked_news_feature.cuda(rank, non_blocking=True)\n",
        "\t\t\tlabel = label.cuda(rank, non_blocking=True)\n",
        "\t\t\n",
        "\t\tbz_loss, y_hat = model(user_index, clicked_news_length, candidate_news_feature, clicked_news_feature, label)\n",
        "\t\tloss += bz_loss.data.float()\n",
        "\t\taccuary += acc(label, y_hat)\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tbz_loss.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\tif cnt % args.log_steps == 0:\n",
        "\t\t\tlogging.info(\n",
        "\t\t\t\t'[{}] Ed: {}, train_loss: {:.5f}, acc: {:.5f}'.format(\n",
        "\t\t\t\t\trank, cnt * args.batch_size, loss.data / cnt, accuary / cnt)\n",
        "\t\t\t)\n",
        "\n",
        "\t\tif rank == 0 and cnt != 0 and cnt % args.save_steps == 0:\n",
        "\t\t\tckpt_path = os.path.join(args.model_dir, f'epoch-{ep+1}-{cnt}.pt')\n",
        "\t\t\ttorch.save(\n",
        "\t\t\t\t{\n",
        "\t\t\t\t\t'model_state_dict':\n",
        "\t\t\t\t\t\t{'.'.join(k.split('.')[1:]): v for k, v in model.state_dict().items()}\n",
        "\t\t\t\t\t\tif is_distributed else model.state_dict(),\n",
        "\t\t\t\t\t'category_dict': category_dict,\n",
        "\t\t\t\t\t'word_dict': word_dict,\n",
        "\t\t\t\t\t'subcategory_dict': subcategory_dict\n",
        "\t\t\t\t}, ckpt_path)\n",
        "\t\t\tlogging.info(f\"Model saved to {ckpt_path}.\")\n",
        "\n",
        "\tlogging.info('Training finish.')\n",
        "\n",
        "\tif rank == 0:\n",
        "\t\tckpt_path = os.path.join(args.model_dir, f'epoch-{ep+1}.pt')\n",
        "\t\ttorch.save(\n",
        "\t\t\t{\n",
        "\t\t\t\t'model_state_dict':\n",
        "\t\t\t\t\t{'.'.join(k.split('.')[1:]): v for k, v in model.state_dict().items()}\n",
        "\t\t\t\t\tif is_distributed else model.state_dict(),\n",
        "\t\t\t\t'category_dict': category_dict,\n",
        "\t\t\t\t'subcategory_dict': subcategory_dict,\n",
        "\t\t\t\t'word_dict': word_dict,\n",
        "\t\t\t}, ckpt_path)\n",
        "\t\tlogging.info(f\"Model saved to {ckpt_path}.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "um0H4NwW69yY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2025-02-19 15:58:35,169] Preparing testing data...\n",
            "[INFO 2025-02-19 15:58:35,169] Preparing testing data...\n",
            "[INFO 2025-02-19 15:58:35,169] Preparing testing data...\n",
            "[INFO 2025-02-19 15:58:35,169] Preparing testing data...\n",
            "[INFO 2025-02-19 15:58:35,169] Preparing testing data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "73152it [00:00, 489730.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2025-02-19 15:58:35,326] Writing files...\n",
            "[INFO 2025-02-19 15:58:35,326] Writing files...\n",
            "[INFO 2025-02-19 15:58:35,326] Writing files...\n",
            "[INFO 2025-02-19 15:58:35,326] Writing files...\n",
            "[INFO 2025-02-19 15:58:35,326] Writing files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2025-02-19 15:58:35,483] 73152 testing samples in total.\n",
            "[INFO 2025-02-19 15:58:35,483] 73152 testing samples in total.\n",
            "[INFO 2025-02-19 15:58:35,483] 73152 testing samples in total.\n",
            "[INFO 2025-02-19 15:58:35,483] 73152 testing samples in total.\n",
            "[INFO 2025-02-19 15:58:35,483] 73152 testing samples in total.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'test' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[43], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSkip testing data preparation.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_sample_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m testing samples in total.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtest\u001b[49m(\u001b[38;5;241m0\u001b[39m, args)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
          ]
        }
      ],
      "source": [
        "args.mode = 'test'\n",
        "args.user_log_mask=True\n",
        "args.batch_size=128\n",
        "args.load_ckpt_name= 'epoch-5.pt'\n",
        "args.prepare=True\n",
        "if 'test' in args.mode:\n",
        "        if args.prepare:\n",
        "            logging.info('Preparing testing data...')\n",
        "            total_sample_num = prepare_testing_data(args.test_data_dir, args.nGPU)\n",
        "        else:\n",
        "            total_sample_num = 0\n",
        "            for i in range(args.nGPU):\n",
        "                data_file_path = os.path.join(args.test_data_dir, f'behaviors_{i}.tsv')\n",
        "                if not os.path.exists(data_file_path):\n",
        "                    logging.error(f'Splited testing data {data_file_path} for GPU {i} does not exist. Please set the parameter --prepare as True and rerun the code.')\n",
        "                    exit()\n",
        "                result = subprocess.getoutput(f'wc -l {data_file_path}')\n",
        "                total_sample_num += int(result.split(' ')[0])\n",
        "            logging.info('Skip testing data preparation.')\n",
        "        logging.info(f'{total_sample_num} testing samples in total.')\n",
        "\n",
        "        test(0, args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soI3jlXBCfD9"
      },
      "source": [
        "# Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abHIlnIvCgsI",
        "outputId": "3803e4e4-1391-4da6-d1c6-3d2d301823f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label = random.randint(0, 4)\n",
        "label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrDpWw36CiCe",
        "outputId": "69073a5d-0155-4f19-c263-b088af183c82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-02-16 04:36:04--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
            "--2025-02-16 04:36:04--  https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: glove.840B.300d.zip\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  4.99MB/s    in 6m 49s  \n",
            "\n",
            "2025-02-16 04:42:54 (5.07 MB/s) - glove.840B.300d.zip saved [2176768927/2176768927]\n",
            "\n",
            "Archive:  glove.840B.300d.zip\n",
            "  inflating: glove.840B.300d.txt     \n",
            "--2025-02-16 04:43:48--  https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip\n",
            "Resolving mind201910small.blob.core.windows.net (mind201910small.blob.core.windows.net)... 20.150.34.36\n",
            "Connecting to mind201910small.blob.core.windows.net (mind201910small.blob.core.windows.net)|20.150.34.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 409 Public access is not permitted on this storage account.\n",
            "2025-02-16 04:43:48 ERROR 409: Public access is not permitted on this storage account..\n",
            "\n",
            "--2025-02-16 04:43:48--  https://mind201910small.blob.core.windows.net/release/MINDsmall_dev.zip\n",
            "Resolving mind201910small.blob.core.windows.net (mind201910small.blob.core.windows.net)... 20.150.34.36\n",
            "Connecting to mind201910small.blob.core.windows.net (mind201910small.blob.core.windows.net)|20.150.34.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 409 Public access is not permitted on this storage account.\n",
            "2025-02-16 04:43:48 ERROR 409: Public access is not permitted on this storage account..\n",
            "\n",
            "unzip:  cannot find or open MINDsmall_train.zip, MINDsmall_train.zip.zip or MINDsmall_train.zip.ZIP.\n",
            "unzip:  cannot find or open MINDsmall_dev.zip, MINDsmall_dev.zip.zip or MINDsmall_dev.zip.ZIP.\n",
            "rm: cannot remove 'MINDsmall_train.zip': No such file or directory\n",
            "rm: cannot remove 'MINDsmall_dev.zip': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!cd data\n",
        "\n",
        "# Dowload GloVe pre-trained word embedding and unzip\n",
        "!wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "!unzip glove.840B.300d.zip\n",
        "\n",
        "# Download MIND-small dataset and unzip\n",
        "!wget https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip\n",
        "!wget https://mind201910small.blob.core.windows.net/release/MINDsmall_dev.zip\n",
        "!unzip MINDsmall_train.zip -d MINDsmall_train\n",
        "!unzip MINDsmall_dev.zip -d MINDsmall_dev\n",
        "\n",
        "!rm glove.840B.300d.zip\n",
        "!rm MINDsmall_train.zip\n",
        "!rm MINDsmall_dev.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU3UxmipCje4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
